{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Shakudo Platform \u00b6 Shakudo Platform is an end-to-end platform for model development, inference, and serving with build-in infrastructure support. On Shakudo Platform, scientists can focus on iterating models and algorithms with the seamless workflow from ideation to production and work with data at scale with fully integrated distributed frameworkes such as DASK , Ray and Spark. Shakudo Platform Core Components \u00b6 Shakudo Platform enables scientists to scale and take code to production without engineering overhead with the following core components. Hyperplane : Simple APIs for distributed DASK, job management, and alerting Sessions : A Jupyter environment with multi-CPU/GPU support and VSCode integration Shakudo Platform Jobs : Turn Jupyter notebooks and scripts into pipeline jobs Shakudo Platform Services : Serve Jupyter notebooks and scripts as a pipeline job Shakudo Platform Triton serving : Model serving at scale with NVIDIA Triton Shakudo Platform Integrations \u00b6 In addition to the core components, Shakudo Platform has a growing list of intergations of popular tools. To request an integration, please create an issue to the github repo . MLFLow : model tracking and monitoring GraphQL Playground : Job management with GraphQL RAPIDS : Accelerated data science with GPU Dask : Parallel computing in python RAY : Scale any python work load with full support of deep learning Spark : A unified analytics engine for large-scale data processing.","title":"Overview"},{"location":"index.html#shakudo-platform","text":"Shakudo Platform is an end-to-end platform for model development, inference, and serving with build-in infrastructure support. On Shakudo Platform, scientists can focus on iterating models and algorithms with the seamless workflow from ideation to production and work with data at scale with fully integrated distributed frameworkes such as DASK , Ray and Spark.","title":"Shakudo Platform"},{"location":"index.html#shakudo-platform-core-components","text":"Shakudo Platform enables scientists to scale and take code to production without engineering overhead with the following core components. Hyperplane : Simple APIs for distributed DASK, job management, and alerting Sessions : A Jupyter environment with multi-CPU/GPU support and VSCode integration Shakudo Platform Jobs : Turn Jupyter notebooks and scripts into pipeline jobs Shakudo Platform Services : Serve Jupyter notebooks and scripts as a pipeline job Shakudo Platform Triton serving : Model serving at scale with NVIDIA Triton","title":"Shakudo Platform Core Components"},{"location":"index.html#shakudo-platform-integrations","text":"In addition to the core components, Shakudo Platform has a growing list of intergations of popular tools. To request an integration, please create an issue to the github repo . MLFLow : model tracking and monitoring GraphQL Playground : Job management with GraphQL RAPIDS : Accelerated data science with GPU Dask : Parallel computing in python RAY : Scale any python work load with full support of deep learning Spark : A unified analytics engine for large-scale data processing.","title":"Shakudo Platform Integrations"},{"location":"about.html","text":"About Shakudo \u00b6 Release Notes \u00b6","title":"About Shakudo"},{"location":"about.html#about-shakudo","text":"","title":"About Shakudo"},{"location":"about.html#release-notes","text":"","title":"Release Notes"},{"location":"api.html","text":"API \u00b6 Let\u2019s take a quick look at the \u2708\ufe0f Hyperplane library features. The library helps you spin up a distributed cluster in Dask, to create, run, and check your pipeline jobs. notebook_common Function Description notebook_common.initialize_cluster(num_workers, **kwargs) Initialize a distributed Dask cluster notebook_common.slack_helper(msg, channel) initialize a slack helper with slack token notebook_common.slack_helper.post_message(msg, channel) post a slack message with slack token notebook_common.slack_helper.post_table(msg, channel) post a slack message in table format with slack token notebook_common.post_message(msg, channel) post a slack message with a webhook URL notebook_common.post_table(msg, channel) post a slack message in table format with a webhook URL notebook_common.graphql_operation(**kwargs) Submit a GraphQL operation notebook_common.submit_pipeline_job(pipelineYamlPath, **kwargs) Submit a pipeline job notebook_common.checkjobs(loop, **args) Check on the job status of one or multiple jobs notebook_common.cancel_pipeline_job(**args) Submit a pipeline job notebook_common.set_current_job_output(**args) Set current job output to a value in scheduled jobs notebook_common.get_previous_job_runs(**args) Get previous job output to a value in scheduled jobs Initializing a DASK cluster \u00b6 initialize_cluster(num_workers:int=2, local_mode:bool=False, worker_spec_yaml:str=WORKER_SPEC_TEMPLATE_1_1, timeout:int=1200, nthreads:int=1, nprocs:int=15, ram_gb_per_proc:float=0.7, cores_per_worker:int=15, ngpus=0, scheduler_deploy_mode:str=\"remote\", dashboard_port:str=\"random\", logging:str=\"quiet\", node_selector: str = None, extra_workers_if_scaleup_required=0, pip_packages: List[str] = [] ): Parameters \u00b6 num_workers ( int , optional , defaults to 2) - Number of remote worker pods for you dask cluster local_mode ( bool , optional , defaults to False) - Whether to use local cluster or distributed KubeCluster worker_spec_yaml ( str , optional , defaults to nc.WORKER_SPEC_TEMPLATE_1_1 ) - a string yaml for cluster configs timeout ( int , optional , defaults to 1200) - Time limit (seconds) for a scheduler to wait to connect before returning a timeout error nthreads ( int , optional , defaults to 1) - Number of threads per worker in your cluster nprocs ( int , optional , defaults to 15) - Number of processes per worker in your cluster ram_gb_per_proc ( float , optional , defaults to 0.7) - GB of Ram per process, per worker cores_per_worker ( int , optional , defaults to 15) - Number of cores per worker scheduler_deploy_mode ( str , optional , defaults to \"remote\") - Where to deploy the scheduler (remote in its own worker, or locally in jhub). Choose remote when the Dask graph dashboard_port ( str , optional , defaults to \"random\") - Choose a port number for your dashboard, or leave as \"random\" to have a random port, which will not conflict logging ( str , optional , defaults to \"quiet\") - Logging level for printouts when initializing. Available options are verbose or quiet . Returns \u00b6 Tuple[Client, KubeCluster] Example usage The easiest way to get started is by using the notebook_common to spin up a pre-configured Dask cluster. You can specify the number of dask worker nodes with num_workers. Note that the number of dask workers in the cluster will be the num_workers x num_procs. Shakudo platform will automatically choose the closest node type based on the combination of parameters. from hyperplane import notebook_common as nc client , cluster = nc . initialize_cluster ( num_workers = 2 ) For more information on choosing an appropriate cluster config, see How to Choose a Cluster Config . You may use the client and cluster like any other dask cluster. See Shakudo Platform Dask Examples for examples of usage or Create a Pipeline to create a pipeline with existing code. Slack helper Class \u00b6 class nc.SlackHelper() A common way for alerting on jobs is to send a Slack message. On Shakudo there are two ways to send slack messages: Use a slack token or use a webhook Send messages using a slack token \u00b6 A slack token can be set up following this tutorial on Slack by creating a quick pre-configured App. Once the App is created and installed to the workspace, the token can be found at the Install App tab in the App's homepage. The token looks like this xoxb-694301530724-2549825675367-Zn4NNP34r3c7aN3EkPDLMiNX Now add this slack token to your environment variable. One simple way to do it is as the following os . environ [ 'SLACK_TOKEN' ] = 'your_slack_token' You can also ask the Shakudo Platform admin to add or change the WEBHOOK_URL variable in your environment permanently. Now we can now initialize a SlackHelper Object like this: from hyperplane import notebook_common as nc sh = nc . SlackHelper () Methods \u00b6 post_message(msg: str, channel: str = \"\") Parameters \u00b6 msg ( str , required ) - String of message to send (can include formatting) channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel Example Usage sh . post_message ( \"This is a test message!\" , channel = \"alert-channel\" ) post_table(dict_of_values: Dict, channel: str = \"\") Parameters \u00b6 dict_of_values ( Dict , required ) - Dictionary of values to send channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel Example Usage message_dict = { \"key1\" : \"value1\" , \"key2\" : \"value2\" } sh . post_table ( message_dict , channel = \"alert-channel\" ) Post a Slack message with webhook URL \u00b6 To send messages to Slack with webhooks, first follow this tutorial on Slack to create a new Slack App and obtain the webhook URL like this https://hooks.slack.com/services/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 Now add this webhook URL as an environment variable. One simple way to do it is as the following: import os os . environ [ 'WEBHOOK_URL' ] = 'your_webhook_url' You can also ask the Shakudo Platform admin to add or change the WEBHOOK_URL variable in your environment permanently. post_message(msg: str, channel: str = \"\") Parameters \u00b6 msg ( str , required ) - String of message to send (can include formatting) channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel Returns \u00b6 requests.models.Response Example Usage from hyperplane import notebook_common as nc nc . post_message ( \"This is a test message!\" , channel = \"alert-channel\" ) Post a Slack message in table format \u00b6 post_table(dict_of_values: Dict, channel: str = \"\") Parameters \u00b6 dict_of_values ( Dict , required ) - Dictionary of values to send channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel Returns \u00b6 requests.models.Response Example Usage from hyperplane import notebook_common as nc message_dict = { \"key1\" : \"value1\" , \"key2\" : \"value2\" } nc . post_table ( message_dict , channel = \"alert-channel\" ) Submit a GraphQL query \u00b6 graphql_operation(gql_statements: Union[str, List[str]]): Example usage gql_query = ''' mutation submitModel { createPipelineJob (data: { jobType: \"basic\", timeout: %d , activeTimeout: %d , maxRetries: %d , pipelineYamlPath: \"model_pipeline.yaml\", parentJob: {connect: {id: \"notebook\"}}, parameters: { create: [ {key: \"model_of_interest\", value: \" %s \"}, ] } }) { id runId } } ''' % ( timeout , a_timeout , \\ max_retries , row . model_of_interest ) gql_queries = [ gql_querie ] results = await nc . graphql_operation ( gql_queries ) Submit a pipeline job \u00b6 There are various ways to submit and check your jobs: through the graphql playground, in your notebook, or through the dashboard (coming soon!). Details on submission query fields can be found in Create a Pipeline . def submit_pipeline_job(pipelineYamlPath: str, jobType: str = \"basic\", timeout: int = 1800,active_timeout: int = 1800, max_retries: int = 3, parameters: Dict[str, str] = {}) Example usage newjob = await nc . submit_pipeline_job ( 'hello.yaml' , parameters = { \"a\" : 1 , \"b\" : 1 }) newjob returns {'id': 'd6f2f4a3-aef7-477e-bc0b-f3a27b0ac0cb', 'runId': None} Check on a job status \u00b6 checkjobs(results: List[str], loop: bool = False, interval: int = 15) Example usage results = [ \"a-job-id\" , \"another-job-id\" ] res = await nc . checkjobs ( results , loop = True ) print ( res ) loop=True will refresh the output every 5 seconds. #### Jobs summary 0 / 2 in progress 0 / 2 pending 2 / 2 processed 2 done | 0 timed out | 0 cancelled | 0 failed Progress: 100.0% #### Dask dashboards a-job-id done None another-job-id done None Cancel a pipeline job \u00b6 cancel_pipeline_job(jobId: str) Example usage await nc . cancel_pipeline_job ( 'some-id' ) returns {'id': 'some-id', 'status': 'cancelled} Set current job output \u00b6 For scheduled jobs, you may want to store values from previous or get the output from the current run. set_current_job_output(s: str, verbose: bool = False) Example usage nc . set_current_job_output ( \"message in str\" ) Get previous job output \u00b6 get_previous_job_runs(n_jobs: Union[int, None] = None, job_status: Union[List[str], str, None] = None, filter_nones: bool = False, verbose: bool = False) Example usage nc . get_previous_job_runs ( n = time_segments )","title":"Hyperplane API"},{"location":"api.html#api","text":"Let\u2019s take a quick look at the \u2708\ufe0f Hyperplane library features. The library helps you spin up a distributed cluster in Dask, to create, run, and check your pipeline jobs. notebook_common Function Description notebook_common.initialize_cluster(num_workers, **kwargs) Initialize a distributed Dask cluster notebook_common.slack_helper(msg, channel) initialize a slack helper with slack token notebook_common.slack_helper.post_message(msg, channel) post a slack message with slack token notebook_common.slack_helper.post_table(msg, channel) post a slack message in table format with slack token notebook_common.post_message(msg, channel) post a slack message with a webhook URL notebook_common.post_table(msg, channel) post a slack message in table format with a webhook URL notebook_common.graphql_operation(**kwargs) Submit a GraphQL operation notebook_common.submit_pipeline_job(pipelineYamlPath, **kwargs) Submit a pipeline job notebook_common.checkjobs(loop, **args) Check on the job status of one or multiple jobs notebook_common.cancel_pipeline_job(**args) Submit a pipeline job notebook_common.set_current_job_output(**args) Set current job output to a value in scheduled jobs notebook_common.get_previous_job_runs(**args) Get previous job output to a value in scheduled jobs","title":"API"},{"location":"api.html#initializing-a-dask-cluster","text":"initialize_cluster(num_workers:int=2, local_mode:bool=False, worker_spec_yaml:str=WORKER_SPEC_TEMPLATE_1_1, timeout:int=1200, nthreads:int=1, nprocs:int=15, ram_gb_per_proc:float=0.7, cores_per_worker:int=15, ngpus=0, scheduler_deploy_mode:str=\"remote\", dashboard_port:str=\"random\", logging:str=\"quiet\", node_selector: str = None, extra_workers_if_scaleup_required=0, pip_packages: List[str] = [] ):","title":"Initializing a DASK cluster"},{"location":"api.html#parameters","text":"num_workers ( int , optional , defaults to 2) - Number of remote worker pods for you dask cluster local_mode ( bool , optional , defaults to False) - Whether to use local cluster or distributed KubeCluster worker_spec_yaml ( str , optional , defaults to nc.WORKER_SPEC_TEMPLATE_1_1 ) - a string yaml for cluster configs timeout ( int , optional , defaults to 1200) - Time limit (seconds) for a scheduler to wait to connect before returning a timeout error nthreads ( int , optional , defaults to 1) - Number of threads per worker in your cluster nprocs ( int , optional , defaults to 15) - Number of processes per worker in your cluster ram_gb_per_proc ( float , optional , defaults to 0.7) - GB of Ram per process, per worker cores_per_worker ( int , optional , defaults to 15) - Number of cores per worker scheduler_deploy_mode ( str , optional , defaults to \"remote\") - Where to deploy the scheduler (remote in its own worker, or locally in jhub). Choose remote when the Dask graph dashboard_port ( str , optional , defaults to \"random\") - Choose a port number for your dashboard, or leave as \"random\" to have a random port, which will not conflict logging ( str , optional , defaults to \"quiet\") - Logging level for printouts when initializing. Available options are verbose or quiet .","title":"Parameters"},{"location":"api.html#returns","text":"Tuple[Client, KubeCluster] Example usage The easiest way to get started is by using the notebook_common to spin up a pre-configured Dask cluster. You can specify the number of dask worker nodes with num_workers. Note that the number of dask workers in the cluster will be the num_workers x num_procs. Shakudo platform will automatically choose the closest node type based on the combination of parameters. from hyperplane import notebook_common as nc client , cluster = nc . initialize_cluster ( num_workers = 2 ) For more information on choosing an appropriate cluster config, see How to Choose a Cluster Config . You may use the client and cluster like any other dask cluster. See Shakudo Platform Dask Examples for examples of usage or Create a Pipeline to create a pipeline with existing code.","title":"Returns"},{"location":"api.html#slack-helper-class","text":"class nc.SlackHelper() A common way for alerting on jobs is to send a Slack message. On Shakudo there are two ways to send slack messages: Use a slack token or use a webhook","title":"Slack helper Class"},{"location":"api.html#send-messages-using-a-slack-token","text":"A slack token can be set up following this tutorial on Slack by creating a quick pre-configured App. Once the App is created and installed to the workspace, the token can be found at the Install App tab in the App's homepage. The token looks like this xoxb-694301530724-2549825675367-Zn4NNP34r3c7aN3EkPDLMiNX Now add this slack token to your environment variable. One simple way to do it is as the following os . environ [ 'SLACK_TOKEN' ] = 'your_slack_token' You can also ask the Shakudo Platform admin to add or change the WEBHOOK_URL variable in your environment permanently. Now we can now initialize a SlackHelper Object like this: from hyperplane import notebook_common as nc sh = nc . SlackHelper ()","title":"Send messages using a slack token"},{"location":"api.html#methods","text":"post_message(msg: str, channel: str = \"\")","title":"Methods"},{"location":"api.html#parameters_1","text":"msg ( str , required ) - String of message to send (can include formatting) channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel Example Usage sh . post_message ( \"This is a test message!\" , channel = \"alert-channel\" ) post_table(dict_of_values: Dict, channel: str = \"\")","title":"Parameters"},{"location":"api.html#parameters_2","text":"dict_of_values ( Dict , required ) - Dictionary of values to send channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel Example Usage message_dict = { \"key1\" : \"value1\" , \"key2\" : \"value2\" } sh . post_table ( message_dict , channel = \"alert-channel\" )","title":"Parameters"},{"location":"api.html#post-a-slack-message-with-webhook-url","text":"To send messages to Slack with webhooks, first follow this tutorial on Slack to create a new Slack App and obtain the webhook URL like this https://hooks.slack.com/services/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 Now add this webhook URL as an environment variable. One simple way to do it is as the following: import os os . environ [ 'WEBHOOK_URL' ] = 'your_webhook_url' You can also ask the Shakudo Platform admin to add or change the WEBHOOK_URL variable in your environment permanently. post_message(msg: str, channel: str = \"\")","title":"Post a Slack message with webhook URL"},{"location":"api.html#parameters_3","text":"msg ( str , required ) - String of message to send (can include formatting) channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel","title":"Parameters"},{"location":"api.html#returns_1","text":"requests.models.Response Example Usage from hyperplane import notebook_common as nc nc . post_message ( \"This is a test message!\" , channel = \"alert-channel\" )","title":"Returns"},{"location":"api.html#post-a-slack-message-in-table-format","text":"post_table(dict_of_values: Dict, channel: str = \"\")","title":"Post a Slack message in table format"},{"location":"api.html#parameters_4","text":"dict_of_values ( Dict , required ) - Dictionary of values to send channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel","title":"Parameters"},{"location":"api.html#returns_2","text":"requests.models.Response Example Usage from hyperplane import notebook_common as nc message_dict = { \"key1\" : \"value1\" , \"key2\" : \"value2\" } nc . post_table ( message_dict , channel = \"alert-channel\" )","title":"Returns"},{"location":"api.html#submit-a-graphql-query","text":"graphql_operation(gql_statements: Union[str, List[str]]): Example usage gql_query = ''' mutation submitModel { createPipelineJob (data: { jobType: \"basic\", timeout: %d , activeTimeout: %d , maxRetries: %d , pipelineYamlPath: \"model_pipeline.yaml\", parentJob: {connect: {id: \"notebook\"}}, parameters: { create: [ {key: \"model_of_interest\", value: \" %s \"}, ] } }) { id runId } } ''' % ( timeout , a_timeout , \\ max_retries , row . model_of_interest ) gql_queries = [ gql_querie ] results = await nc . graphql_operation ( gql_queries )","title":"Submit a GraphQL query"},{"location":"api.html#submit-a-pipeline-job","text":"There are various ways to submit and check your jobs: through the graphql playground, in your notebook, or through the dashboard (coming soon!). Details on submission query fields can be found in Create a Pipeline . def submit_pipeline_job(pipelineYamlPath: str, jobType: str = \"basic\", timeout: int = 1800,active_timeout: int = 1800, max_retries: int = 3, parameters: Dict[str, str] = {}) Example usage newjob = await nc . submit_pipeline_job ( 'hello.yaml' , parameters = { \"a\" : 1 , \"b\" : 1 }) newjob returns {'id': 'd6f2f4a3-aef7-477e-bc0b-f3a27b0ac0cb', 'runId': None}","title":"Submit a pipeline job"},{"location":"api.html#check-on-a-job-status","text":"checkjobs(results: List[str], loop: bool = False, interval: int = 15) Example usage results = [ \"a-job-id\" , \"another-job-id\" ] res = await nc . checkjobs ( results , loop = True ) print ( res ) loop=True will refresh the output every 5 seconds. #### Jobs summary 0 / 2 in progress 0 / 2 pending 2 / 2 processed 2 done | 0 timed out | 0 cancelled | 0 failed Progress: 100.0% #### Dask dashboards a-job-id done None another-job-id done None","title":"Check on a job status"},{"location":"api.html#cancel-a-pipeline-job","text":"cancel_pipeline_job(jobId: str) Example usage await nc . cancel_pipeline_job ( 'some-id' ) returns {'id': 'some-id', 'status': 'cancelled}","title":"Cancel a pipeline job"},{"location":"api.html#set-current-job-output","text":"For scheduled jobs, you may want to store values from previous or get the output from the current run. set_current_job_output(s: str, verbose: bool = False) Example usage nc . set_current_job_output ( \"message in str\" )","title":"Set current job output"},{"location":"api.html#get-previous-job-output","text":"get_previous_job_runs(n_jobs: Union[int, None] = None, job_status: Union[List[str], str, None] = None, filter_nones: bool = False, verbose: bool = False) Example usage nc . get_previous_job_runs ( n = time_segments )","title":"Get previous job output"},{"location":"common_issues.html","text":"Common Issues \u00b6 Out of memory error module not found error timeouts when spinning up premption Out of memory \u00b6 Out of memory errors may occur if one of your chunks/partitions or persisted data is too large to fit into RAM. OOM errors can also occur if one of your operations requires more RAM than what is available. Increasing the available memory on your workers can usually solve this issue. OOMs show up as any of the following: killed worker , key error , canceled , http error . Module not found \u00b6 When add code to your pipeline, some references to .py modules may not be found. Ensure you are importing from the correct directory (relative to the top level of your repository). Timeouts when spinning up Dask workers \u00b6 You may receive an error if the Dask nodes are taking too long to scale up (due to resource availability, resource limits for your project, etc.). You can retry by re-running your cell or script when you see this error. Prempted nodes \u00b6 If a node is preempted, you will see a canceled error . For this reason, we do not recommend long jobs, but rather split your long-running functions into multiple jobs to avoid http error or canceled error .","title":"Common issues"},{"location":"common_issues.html#common-issues","text":"Out of memory error module not found error timeouts when spinning up premption","title":"Common Issues"},{"location":"common_issues.html#out-of-memory","text":"Out of memory errors may occur if one of your chunks/partitions or persisted data is too large to fit into RAM. OOM errors can also occur if one of your operations requires more RAM than what is available. Increasing the available memory on your workers can usually solve this issue. OOMs show up as any of the following: killed worker , key error , canceled , http error .","title":"Out of memory"},{"location":"common_issues.html#module-not-found","text":"When add code to your pipeline, some references to .py modules may not be found. Ensure you are importing from the correct directory (relative to the top level of your repository).","title":"Module not found"},{"location":"common_issues.html#timeouts-when-spinning-up-dask-workers","text":"You may receive an error if the Dask nodes are taking too long to scale up (due to resource availability, resource limits for your project, etc.). You can retry by re-running your cell or script when you see this error.","title":"Timeouts when spinning up Dask workers"},{"location":"common_issues.html#prempted-nodes","text":"If a node is preempted, you will see a canceled error . For this reason, we do not recommend long jobs, but rather split your long-running functions into multiple jobs to avoid http error or canceled error .","title":"Prempted nodes"},{"location":"create_pipeline.html","text":"Create a pipeline \u00b6 Pipelines are used to run a sequence of notebooks or python files from beginning to end. Hyperplane enables you to build pipelines from VSCode notebooks, python files, or Jupyter notebooks. The main components of each pipeline are the steps, parameters, pipeline YAML, scheduling, saving files, timeouts, submitting and checking jobs with GraphQL, and the outputs. 1. Quick start \u00b6 To start creating your pipeline with a pre-existing notebook, ensure your notebook can run from beginning to end without errors. Alternatively, if you are using the pipeline to debug your runs, you can skip this step. 2. Organize code into steps \u00b6 Hyperplane pipelines enable you to run multiple python scripts and Jupyter notebooks in succession. We recommend separate long-running code into separate files and add them as separate steps in the YAML. 3. Pipeline yaml \u00b6 Create a .yaml file to list your steps. You can start with adding the following template to example_pipeline.yaml : pipeline: name: \"Example pipeline\" tasks: - name: \"First step\" type: \"jupyter notebook\" notebook_path: \"notebook_1.ipynb\" notebook_output_path: \"notebook_1_output.ipynb\" - ... You can replace the first step under tasks and add additional tasks following the same notation. Add a Jupyter notebook step by adding the following block to your YAML file: - name: \"[your_step_name]\" type: \"jupyter notebook\" notebook_path: \"[notebook/path/relative/to/top/level/of/repo.ipynb]\" notebook_output_path: \"[some/notebook_output_name.ipynb]\" Add a Python/VScode step by adding the following block to your YAML. The script should be runnable with python [py_path] : - name: \"[another_step_name]\" type: \"vscode notebook\" py_path: \"[py/file/relative/to/top/level/of/repo.py]\" Add a Javascript step by adding the following block to your YAML. The script should be runnable with node [js_path] : - name: \"[another_step_name]\" type: \"js script\" js_path: \"[js/file/relative/to/top/level/of/repo.js]\" Add a bash script step by adding the following block to your YAML. The bash script should be runnable with bash [bash_script_path] : - name: \"[another_step_name]\" type: \"bash script\" bash_script_path: \"[sh/file/relative/to/top/level/of/repo.sh]\" 4. Parameters \u00b6 Parameters are useful for experimentation, or when you want to run the same pipeline with different variables. For example, you may want to submit a query multiple times, with different values for your parameters each time. mu tat io n submi t Model { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , pipeli ne YamlPa t h : \"example_pipeline.yaml\" , parame ters : { crea te : [ { key : \"param1\" , value : \"value1\" }, ## cha n ge t hese f or di fferent experime nts ] } } ) { id ru n Id } } jupyter notebook step \u00b6 To prepare your notebooks to receive parameters, tag the notebook cell containing replaceable variables with \"parameters\" using the cogs on the right. Once you submit a query with the parameters field, the pipeline will insert an extra cell containing the parameter values from your job submission query. For example, the job submission query above would insert a cell containing param1 = \"value1\" below the cell tagged with \"parameters\" in each notebook step in the example_pipeline. other types of steps \u00b6 In Python, Javascript, Bash, or other types of steps (including Jupyter notebooks), parameters can be accessed through environment variables. For example, using the job submission query above, one could access the new frequency parameter with the following line: freq = os . environ . get ( 'HYPERPLANE_JOB_PARAMETER_PARAM1' ) if os . environ . get ( 'HYPERPLANE_JOB_PARAMETER_PARAM1' ) else 'value_x' 5. Scheduling chron jobs \u00b6 You may schedule your pipeline to run on a schedule, using the schedule field in your query. The scheduling values can be set following standard chron expressions. See https://crontab.guru/ for examples. The following GraphQL query will run the example_pipeline every 30 minutes. mu tat io n submi t Model { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , schedule : \"*/30 * * * *\" pipeli ne YamlPa t h : \"example_pipeline.yaml\" , } ) { id ru n Id } } Note that jobType indicates the image for the job to run. For the complete list please visit the Image and jobTypes page . 6. Specifying timeouts \u00b6 Shakudo Platform uses three types of timeouts to ensure your pipelines do not stall or run indefinitely. mu tat io n submi t Model { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , t imeou t : %d , ## global t imeou t ac t iveTimeou t : %d , ## ac t ive t imeou t maxRe tr ies : %d , pipeli ne YamlPa t h : \"example_pipeline.yaml\" , } ) { id ru n Id } } Global timeout \u00b6 The global timeout is the maximum time that the pipeline may run, starting from the moment of job submission. Even if the job is never picked up (due to resource limits or task limits set by your admin team), it will throw a timeout error after this time is reached. Global timeout is set as part of the submission query using the timeout field. Active timeout \u00b6 The active timeout is maximum time that the pipeline may run once it is picked up. Regardless of whether steps remain to be completed, the pipeline will stop when this timeout is reached. Active timeout is set as part of the submission query using the activeTimeout field. Step timeout \u00b6 The step timeout is the amount of time each step can run before being stopped and restarted (for maxRetries number of times, set in the submission query). This value can be set for each step using the timeout value in each step of the pipeline YAML file. For example, the following ensures that the step will not run for more than 120 seconds. - name: \"[your_step_name]\" type: \"jupyter notebook\" notebook_path: \"[notebook/path/relative/to/top/level/of/repo.ipynb]\" notebook_output_path: \"[notebook_output_name].ipynb\" timeout: 120 maxRetries \u00b6 maxRetries specifies the maximum number of attempts to run your pipeline job before returning an error, even if the timeouts are not reached. 7. Submit a job \u00b6 Using GraphQL playground \u00b6 Queries and mutations will autocomplete in the GraphQL playground . The query below can be copied and pasted into the GraphQL playground to create a job, provided that your pipelineYamlPath (relative to the top directory of your repo) exists and committed to the branch that's connected to Hyperplane. You can find which branch of the repository is connected to Shakudo Platform on the header of your Shakudo Platform dashboard it indicates which branch of your repository is connected to your Shakudo Platform mu tat io n { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , ## correspo n ds t o job_ t ype fr om sec t io n 3 t imeou t : 1800 , ## global t imeou t pipeli ne YamlPa t h : \"example_pipeline.yaml\" , ## pa t h t o your yaml parame ters : { crea te : [ { key : \"param1\" , value : \"value1\" }, ## cha n ge t hese f or di fferent experime nts ] } } ) { id ru n Id } } In a jhub notebook \u00b6 Use the following inside a notebook cell or Python script to submit a job gql_query = ''' mutation submitModel { createPipelineJob (data: { jobType: \"basic\", timeout: %d , activeTimeout: %d , maxRetries: %d , pipelineYamlPath: \"example_pipeline.yaml\", parentJob: {connect: {id: \"notebook\"}}, parameters: { create: [ {key: \"model_of_interest\", value: \" %s \"}, ] } }) { id runId } } ''' % ( timeout , a_timeout , \\ max_retries , row . model_of_interest ) gql_queries = [ gql_querie ] results = await nc . graphql_operation ( gql_queries ) Using the dashboard \u00b6 Go to the Jobs tab Add your YAML file, parameters, timeouts, and retries as normal. You can see how to create a pipeline in the this short video 8. Check on pipeline jobs \u00b6 You can check your jobs using the Shakudo Platform dashboard, or with the following simple query in GraphQL playground: query checkJob { pipeli ne Job(where : { id : \"your-job-id\" } ) { id s tatus s tart Time ou t pu t No te booksPa t h s tatus Reaso n } } Pending : the job is waiting to be started In progress : the job is running Done : the job is finished Any other status means the run was not successful; refer to the statusReason field to see why the pipeline job failed. 9. Outputs and debugging \u00b6 You will notice that the status query above also returns an output path. Each Jupyter notebook step (including outputs from running the cells) will be stored at the value of outputNotebooksPath. You can download the notebook and run it on your Jupyter instance to debug if necessary. For Python/VSCode notebooks, all stdouts will also be saved in an output .txt file. 10. Other output options \u00b6 Github image upload \u00b6 If it is more convenient for your workflow, Shakudo Platform provides an example BASH script to upload graphs and other visual output directly to GitHub. It may be used as is or customized to your liking. See here for the relevant documentation. Slack notification \u00b6 Shakudo's Hyperplane Python package includes a function allowing jobs to post messages to your Slack channel. You can use this to notify you of finished jobs, or even to post job results. See here for the relevant documentation. EventBridge \u00b6 If you prefer to use Amazon EventBridge, we provide convenience functions at multiple levels of abstraction allowing you to easily forward data into that service ecosystem. See here for more information.","title":"Create a pipeline job"},{"location":"create_pipeline.html#create-a-pipeline","text":"Pipelines are used to run a sequence of notebooks or python files from beginning to end. Hyperplane enables you to build pipelines from VSCode notebooks, python files, or Jupyter notebooks. The main components of each pipeline are the steps, parameters, pipeline YAML, scheduling, saving files, timeouts, submitting and checking jobs with GraphQL, and the outputs.","title":"Create a pipeline"},{"location":"create_pipeline.html#1-quick-start","text":"To start creating your pipeline with a pre-existing notebook, ensure your notebook can run from beginning to end without errors. Alternatively, if you are using the pipeline to debug your runs, you can skip this step.","title":"1. Quick start"},{"location":"create_pipeline.html#2-organize-code-into-steps","text":"Hyperplane pipelines enable you to run multiple python scripts and Jupyter notebooks in succession. We recommend separate long-running code into separate files and add them as separate steps in the YAML.","title":"2. Organize code into steps"},{"location":"create_pipeline.html#3-pipeline-yaml","text":"Create a .yaml file to list your steps. You can start with adding the following template to example_pipeline.yaml : pipeline: name: \"Example pipeline\" tasks: - name: \"First step\" type: \"jupyter notebook\" notebook_path: \"notebook_1.ipynb\" notebook_output_path: \"notebook_1_output.ipynb\" - ... You can replace the first step under tasks and add additional tasks following the same notation. Add a Jupyter notebook step by adding the following block to your YAML file: - name: \"[your_step_name]\" type: \"jupyter notebook\" notebook_path: \"[notebook/path/relative/to/top/level/of/repo.ipynb]\" notebook_output_path: \"[some/notebook_output_name.ipynb]\" Add a Python/VScode step by adding the following block to your YAML. The script should be runnable with python [py_path] : - name: \"[another_step_name]\" type: \"vscode notebook\" py_path: \"[py/file/relative/to/top/level/of/repo.py]\" Add a Javascript step by adding the following block to your YAML. The script should be runnable with node [js_path] : - name: \"[another_step_name]\" type: \"js script\" js_path: \"[js/file/relative/to/top/level/of/repo.js]\" Add a bash script step by adding the following block to your YAML. The bash script should be runnable with bash [bash_script_path] : - name: \"[another_step_name]\" type: \"bash script\" bash_script_path: \"[sh/file/relative/to/top/level/of/repo.sh]\"","title":"3. Pipeline yaml"},{"location":"create_pipeline.html#4-parameters","text":"Parameters are useful for experimentation, or when you want to run the same pipeline with different variables. For example, you may want to submit a query multiple times, with different values for your parameters each time. mu tat io n submi t Model { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , pipeli ne YamlPa t h : \"example_pipeline.yaml\" , parame ters : { crea te : [ { key : \"param1\" , value : \"value1\" }, ## cha n ge t hese f or di fferent experime nts ] } } ) { id ru n Id } }","title":"4. Parameters"},{"location":"create_pipeline.html#jupyter-notebook-step","text":"To prepare your notebooks to receive parameters, tag the notebook cell containing replaceable variables with \"parameters\" using the cogs on the right. Once you submit a query with the parameters field, the pipeline will insert an extra cell containing the parameter values from your job submission query. For example, the job submission query above would insert a cell containing param1 = \"value1\" below the cell tagged with \"parameters\" in each notebook step in the example_pipeline.","title":"jupyter notebook step"},{"location":"create_pipeline.html#other-types-of-steps","text":"In Python, Javascript, Bash, or other types of steps (including Jupyter notebooks), parameters can be accessed through environment variables. For example, using the job submission query above, one could access the new frequency parameter with the following line: freq = os . environ . get ( 'HYPERPLANE_JOB_PARAMETER_PARAM1' ) if os . environ . get ( 'HYPERPLANE_JOB_PARAMETER_PARAM1' ) else 'value_x'","title":"other types of steps"},{"location":"create_pipeline.html#5-scheduling-chron-jobs","text":"You may schedule your pipeline to run on a schedule, using the schedule field in your query. The scheduling values can be set following standard chron expressions. See https://crontab.guru/ for examples. The following GraphQL query will run the example_pipeline every 30 minutes. mu tat io n submi t Model { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , schedule : \"*/30 * * * *\" pipeli ne YamlPa t h : \"example_pipeline.yaml\" , } ) { id ru n Id } } Note that jobType indicates the image for the job to run. For the complete list please visit the Image and jobTypes page .","title":"5. Scheduling chron jobs"},{"location":"create_pipeline.html#6-specifying-timeouts","text":"Shakudo Platform uses three types of timeouts to ensure your pipelines do not stall or run indefinitely. mu tat io n submi t Model { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , t imeou t : %d , ## global t imeou t ac t iveTimeou t : %d , ## ac t ive t imeou t maxRe tr ies : %d , pipeli ne YamlPa t h : \"example_pipeline.yaml\" , } ) { id ru n Id } }","title":"6. Specifying timeouts"},{"location":"create_pipeline.html#global-timeout","text":"The global timeout is the maximum time that the pipeline may run, starting from the moment of job submission. Even if the job is never picked up (due to resource limits or task limits set by your admin team), it will throw a timeout error after this time is reached. Global timeout is set as part of the submission query using the timeout field.","title":"Global timeout"},{"location":"create_pipeline.html#active-timeout","text":"The active timeout is maximum time that the pipeline may run once it is picked up. Regardless of whether steps remain to be completed, the pipeline will stop when this timeout is reached. Active timeout is set as part of the submission query using the activeTimeout field.","title":"Active timeout"},{"location":"create_pipeline.html#step-timeout","text":"The step timeout is the amount of time each step can run before being stopped and restarted (for maxRetries number of times, set in the submission query). This value can be set for each step using the timeout value in each step of the pipeline YAML file. For example, the following ensures that the step will not run for more than 120 seconds. - name: \"[your_step_name]\" type: \"jupyter notebook\" notebook_path: \"[notebook/path/relative/to/top/level/of/repo.ipynb]\" notebook_output_path: \"[notebook_output_name].ipynb\" timeout: 120","title":"Step timeout"},{"location":"create_pipeline.html#maxretries","text":"maxRetries specifies the maximum number of attempts to run your pipeline job before returning an error, even if the timeouts are not reached.","title":"maxRetries"},{"location":"create_pipeline.html#7-submit-a-job","text":"","title":"7. Submit a job"},{"location":"create_pipeline.html#using-graphql-playground","text":"Queries and mutations will autocomplete in the GraphQL playground . The query below can be copied and pasted into the GraphQL playground to create a job, provided that your pipelineYamlPath (relative to the top directory of your repo) exists and committed to the branch that's connected to Hyperplane. You can find which branch of the repository is connected to Shakudo Platform on the header of your Shakudo Platform dashboard it indicates which branch of your repository is connected to your Shakudo Platform mu tat io n { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , ## correspo n ds t o job_ t ype fr om sec t io n 3 t imeou t : 1800 , ## global t imeou t pipeli ne YamlPa t h : \"example_pipeline.yaml\" , ## pa t h t o your yaml parame ters : { crea te : [ { key : \"param1\" , value : \"value1\" }, ## cha n ge t hese f or di fferent experime nts ] } } ) { id ru n Id } }","title":"Using GraphQL playground"},{"location":"create_pipeline.html#in-a-jhub-notebook","text":"Use the following inside a notebook cell or Python script to submit a job gql_query = ''' mutation submitModel { createPipelineJob (data: { jobType: \"basic\", timeout: %d , activeTimeout: %d , maxRetries: %d , pipelineYamlPath: \"example_pipeline.yaml\", parentJob: {connect: {id: \"notebook\"}}, parameters: { create: [ {key: \"model_of_interest\", value: \" %s \"}, ] } }) { id runId } } ''' % ( timeout , a_timeout , \\ max_retries , row . model_of_interest ) gql_queries = [ gql_querie ] results = await nc . graphql_operation ( gql_queries )","title":"In a jhub notebook"},{"location":"create_pipeline.html#using-the-dashboard","text":"Go to the Jobs tab Add your YAML file, parameters, timeouts, and retries as normal. You can see how to create a pipeline in the this short video","title":"Using the dashboard"},{"location":"create_pipeline.html#8-check-on-pipeline-jobs","text":"You can check your jobs using the Shakudo Platform dashboard, or with the following simple query in GraphQL playground: query checkJob { pipeli ne Job(where : { id : \"your-job-id\" } ) { id s tatus s tart Time ou t pu t No te booksPa t h s tatus Reaso n } } Pending : the job is waiting to be started In progress : the job is running Done : the job is finished Any other status means the run was not successful; refer to the statusReason field to see why the pipeline job failed.","title":"8. Check on pipeline jobs"},{"location":"create_pipeline.html#9-outputs-and-debugging","text":"You will notice that the status query above also returns an output path. Each Jupyter notebook step (including outputs from running the cells) will be stored at the value of outputNotebooksPath. You can download the notebook and run it on your Jupyter instance to debug if necessary. For Python/VSCode notebooks, all stdouts will also be saved in an output .txt file.","title":"9. Outputs and debugging"},{"location":"create_pipeline.html#10-other-output-options","text":"","title":"10. Other output options"},{"location":"create_pipeline.html#github-image-upload","text":"If it is more convenient for your workflow, Shakudo Platform provides an example BASH script to upload graphs and other visual output directly to GitHub. It may be used as is or customized to your liking. See here for the relevant documentation.","title":"Github image upload"},{"location":"create_pipeline.html#slack-notification","text":"Shakudo's Hyperplane Python package includes a function allowing jobs to post messages to your Slack channel. You can use this to notify you of finished jobs, or even to post job results. See here for the relevant documentation.","title":"Slack notification"},{"location":"create_pipeline.html#eventbridge","text":"If you prefer to use Amazon EventBridge, we provide convenience functions at multiple levels of abstraction allowing you to easily forward data into that service ecosystem. See here for more information.","title":"EventBridge"},{"location":"create_service.html","text":"Create a service \u00b6 In contrast to regular pipeline jobs, which are batch jobs or a series of steps that run end-to-end, you can create jobs from never-ending notebooks or scripts as services. Rather than running in your cluster, you can trigger this to run similar to a pipeline job on Shakudo Platform to run a continuous loop without having to keep your cluster up or your notebook running. 1. Prepare your code \u00b6 Ensure your notebook or scripts can run without errors on your Jupyter instance. Alternatively, if you are using the pipeline to debug your runs, you can skip this step. 2. Create a pipeline yaml \u00b6 Create a .yaml file to list your steps. You can start with adding the following template to example_pipeline.yaml : pipeline: name: \"Example pipeline\" tasks: - name: \"First step\" type: \"jupyter notebook\" notebook_path: \"neverending_notebook.ipynb\" notebook_output_path: \"neverending_notebook_output.ipynb\" - ... You can replace the first step under tasks and add additional tasks following the same notation. Add a Jupyter notebook step by adding the following block to your YAML: - name: \"[your_step_name]\" type: \"jupyter notebook\" notebook_path: \"[notebook/path/relative/to/top/level/of/repo.ipynb]\" notebook_output_path: \"[some/notebook_output_name.ipynb]\" Add a Python/VSCode step by adding the following block to your yaml. The script should be runnable with python [py_path] : - name: \"[another_step_name]\" type: \"vscode notebook\" py_path: \"[py/file/relative/to/top/level/of/repo.py]\" Add a Javascript step by adding the following block to your YAML. The script should be runnable with node [js_path] : - name: \"[another_step_name]\" type: \"js script\" js_path: \"[js/file/relative/to/top/level/of/repo.js]\" Add a Bash script step by adding the following block to your YAML. The Bash script should be runnable with bash [bash_script_path] : - name: \"[another_step_name]\" type: \"bash script\" bash_script_path: \"[sh/file/relative/to/top/level/of/repo.sh]\" 3. Set your timeouts to -1 \u00b6 Specify activeTimeout and timeout as -1 to indicate to Shakudo Platform that your job should not time out. mu tat io n submi t Model { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , ac t iveTimeou t : -1 , t imeou t : -1 , pipeli ne YamlPa t h : \"example_pipeline.yaml\" , parame ters : { crea te : [ { key : \"param1\" , value : \"value1\" }, ## cha n ge t hese f or di fferent experime nts ] } } ) { id ru n Id } } Alternatively, use the Services tab on the dashboard. 4. (optional) Expose a port \u00b6 If your neverending script requires an exposed port, be sure to expose it on host 0.0.0.0, port 8787 . Then, include an exposed port in the job submission. 5. (optional) Connecting to a service \u00b6 Once your service is up and running you can connect to it from other jobs. To make this easier, Shakudo Platform's python package includes a helper function to find the current URL of any running service. Note that this is an internal URL and is only accessible from your other Shakudo Platform jobs - information regarding making a publicly accessible service is available here . Importing and invoking the function: \u00b6 from hyperplane import utils URL_string = utils.get_service_url(name_or_id, input_type, check_exists, check_listening) Argument Type Description name_ or_id string, required The name or ID of the service in question. Service names are assigned by the user at job creation. IDs are Shakudo Platform's internal job labels, and take the form of a string of hexadecimal characters and dashes. input_ type string, optional, defaults to \"auto\" Specifies whether a name or ID has been supplied. Possible values: \"auto\" - function will automatically determine how to treat the input. \"name\" - assume the input is a service name (useful if you have named your service with the ID of another job or service for some reason). \"id\" - assume the input is a job ID. Any other string will fall back to auto-recognition. check_ exists boolean, optional, defaults to True If True the function will query Shakudo Platform's job database to ensure the service is running. Will raise an error if the service is not found. This test is necessary if name_or_id is a service name, and in that case values of False will be ignored. check_ listening boolean, optional, defaults to False If True the function will ensure that the service is listening on the expected port and URL. Will raise an error if it cannot connect to the service. May introduce a slight execution delay, depending on network environment. Return value: \u00b6 The function returns a string containing the service URL, including port number","title":"Create a pipeline service"},{"location":"create_service.html#create-a-service","text":"In contrast to regular pipeline jobs, which are batch jobs or a series of steps that run end-to-end, you can create jobs from never-ending notebooks or scripts as services. Rather than running in your cluster, you can trigger this to run similar to a pipeline job on Shakudo Platform to run a continuous loop without having to keep your cluster up or your notebook running.","title":"Create a service"},{"location":"create_service.html#1-prepare-your-code","text":"Ensure your notebook or scripts can run without errors on your Jupyter instance. Alternatively, if you are using the pipeline to debug your runs, you can skip this step.","title":"1. Prepare your code"},{"location":"create_service.html#2-create-a-pipeline-yaml","text":"Create a .yaml file to list your steps. You can start with adding the following template to example_pipeline.yaml : pipeline: name: \"Example pipeline\" tasks: - name: \"First step\" type: \"jupyter notebook\" notebook_path: \"neverending_notebook.ipynb\" notebook_output_path: \"neverending_notebook_output.ipynb\" - ... You can replace the first step under tasks and add additional tasks following the same notation. Add a Jupyter notebook step by adding the following block to your YAML: - name: \"[your_step_name]\" type: \"jupyter notebook\" notebook_path: \"[notebook/path/relative/to/top/level/of/repo.ipynb]\" notebook_output_path: \"[some/notebook_output_name.ipynb]\" Add a Python/VSCode step by adding the following block to your yaml. The script should be runnable with python [py_path] : - name: \"[another_step_name]\" type: \"vscode notebook\" py_path: \"[py/file/relative/to/top/level/of/repo.py]\" Add a Javascript step by adding the following block to your YAML. The script should be runnable with node [js_path] : - name: \"[another_step_name]\" type: \"js script\" js_path: \"[js/file/relative/to/top/level/of/repo.js]\" Add a Bash script step by adding the following block to your YAML. The Bash script should be runnable with bash [bash_script_path] : - name: \"[another_step_name]\" type: \"bash script\" bash_script_path: \"[sh/file/relative/to/top/level/of/repo.sh]\"","title":"2. Create a pipeline yaml"},{"location":"create_service.html#3-set-your-timeouts-to-1","text":"Specify activeTimeout and timeout as -1 to indicate to Shakudo Platform that your job should not time out. mu tat io n submi t Model { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , ac t iveTimeou t : -1 , t imeou t : -1 , pipeli ne YamlPa t h : \"example_pipeline.yaml\" , parame ters : { crea te : [ { key : \"param1\" , value : \"value1\" }, ## cha n ge t hese f or di fferent experime nts ] } } ) { id ru n Id } } Alternatively, use the Services tab on the dashboard.","title":"3. Set your timeouts to -1"},{"location":"create_service.html#4-optional-expose-a-port","text":"If your neverending script requires an exposed port, be sure to expose it on host 0.0.0.0, port 8787 . Then, include an exposed port in the job submission.","title":"4. (optional) Expose a port"},{"location":"create_service.html#5-optional-connecting-to-a-service","text":"Once your service is up and running you can connect to it from other jobs. To make this easier, Shakudo Platform's python package includes a helper function to find the current URL of any running service. Note that this is an internal URL and is only accessible from your other Shakudo Platform jobs - information regarding making a publicly accessible service is available here .","title":"5. (optional) Connecting to a service"},{"location":"create_service.html#importing-and-invoking-the-function","text":"from hyperplane import utils URL_string = utils.get_service_url(name_or_id, input_type, check_exists, check_listening) Argument Type Description name_ or_id string, required The name or ID of the service in question. Service names are assigned by the user at job creation. IDs are Shakudo Platform's internal job labels, and take the form of a string of hexadecimal characters and dashes. input_ type string, optional, defaults to \"auto\" Specifies whether a name or ID has been supplied. Possible values: \"auto\" - function will automatically determine how to treat the input. \"name\" - assume the input is a service name (useful if you have named your service with the ID of another job or service for some reason). \"id\" - assume the input is a job ID. Any other string will fall back to auto-recognition. check_ exists boolean, optional, defaults to True If True the function will query Shakudo Platform's job database to ensure the service is running. Will raise an error if the service is not found. This test is necessary if name_or_id is a service name, and in that case values of False will be ignored. check_ listening boolean, optional, defaults to False If True the function will ensure that the service is listening on the expected port and URL. Will raise an error if it cannot connect to the service. May introduce a slight execution delay, depending on network environment.","title":"Importing and invoking the function:"},{"location":"create_service.html#return-value","text":"The function returns a string containing the service URL, including port number","title":"Return value:"},{"location":"crypto.html","text":"Shakudo Platform as a trading bot platform \u00b6 CCXT - CryptoCurrency eXchange Trading Library is a JavaScript / Python / PHP library for cryptocurrency trading and e-commerce with support for many bitcoin/ether/altcoin exchange markets and merchant APIs. CCXT Supports unified APIs for access marketing data and OrderBook data on over 100 Crypto exchanges including the most popular Binance, FTX, Coinbase, Kraken, Kucoin and etc. CCXT.Pro is the paid professional version of CCXT and it's offered by Shakudo to Platform Users for free and fully integrated with the Shakudo Platform. Compared to the basic CCXT, CCXT.Pro provides WebSocket APIs to enable real-time high frequency trading and ensure efficiency, higher speeds, and lower latencies. Typical work flow \u00b6 Shakudo Platform is fully integrated with the pro version of CCXT , making it an ideal platform to build and deploy trading bots. Below is a typical workflow of building a Crypto trading App: Start a Shakudo Platform Sessions with the Crypto image to write a trading script using web3 to access blockchain data and ccxt.pro to exchange markets and APIs. Use distributed data processing such as Dask , Ray to process Order Book data. Build a frontend app to visualize the trades with built-in tools such as Streamlit . Deploy a Shakudo Platform service with the trading script and the frontend app, read more about deploying a Shakudo Platform service .","title":"CCXT"},{"location":"crypto.html#shakudo-platform-as-a-trading-bot-platform","text":"CCXT - CryptoCurrency eXchange Trading Library is a JavaScript / Python / PHP library for cryptocurrency trading and e-commerce with support for many bitcoin/ether/altcoin exchange markets and merchant APIs. CCXT Supports unified APIs for access marketing data and OrderBook data on over 100 Crypto exchanges including the most popular Binance, FTX, Coinbase, Kraken, Kucoin and etc. CCXT.Pro is the paid professional version of CCXT and it's offered by Shakudo to Platform Users for free and fully integrated with the Shakudo Platform. Compared to the basic CCXT, CCXT.Pro provides WebSocket APIs to enable real-time high frequency trading and ensure efficiency, higher speeds, and lower latencies.","title":"Shakudo Platform as a trading bot platform"},{"location":"crypto.html#typical-work-flow","text":"Shakudo Platform is fully integrated with the pro version of CCXT , making it an ideal platform to build and deploy trading bots. Below is a typical workflow of building a Crypto trading App: Start a Shakudo Platform Sessions with the Crypto image to write a trading script using web3 to access blockchain data and ccxt.pro to exchange markets and APIs. Use distributed data processing such as Dask , Ray to process Order Book data. Build a frontend app to visualize the trades with built-in tools such as Streamlit . Deploy a Shakudo Platform service with the trading script and the frontend app, read more about deploying a Shakudo Platform service .","title":"Typical work flow"},{"location":"custom_images.html","text":"Create and use custom images in Shakudo Platform \u00b6 Prerequisites \u00b6 The following tools are needed to create a custom Shakudo Platform image: kubectl + kubeconfig access to Shakudo Platform gcloud cli docker Retrieve and use image pull credentials \u00b6 Get and store the credentials from your Shakudo Platform cluster secrets: kubectl get secrets/image-pull-secret --template=\"{{index .data \\\"mothership-key-content.json\\\" | base64decode}}\" > keyfile_mothership.json Authenticate using Shakudo Platform image pull credentials: gcloud auth activate-service-account --key-file=keyfile_mothership.json gcloud config set account mothership@hyperplane-test.iam.gserviceaccount.com Pulling the basic Shakudo Platform image: docker pull gcr.io/devsentient-infra/dev/jhub-basic@sha256:65873aad4648282b0f74669cca623205acaca25e9be0180071b620552b77c42b Create your custom image \u00b6 Now that you have the basic Shakudo Platform image, you can follow the normal steps to create a Docker container. For example, you can create a Dockerfile that looks like this following: FROM gcr.io/devsentient-infra/dev/jhub-basic:e520b7d74c7c04abc764dd43d60229ffefefb362 RUN pip install [some-packages-here] You can now build your image (e.g. docker build -t image_name . in the directory of your Dockerfile) and push to your registry (e.g. docker image tag image_name:image_tag registry-host:my-org/image_name:image_tag and docker image push registry-host:my-org/image_name:image_tag ) as normal. Ensure you grant the Shakudo Platform service account permissions to pull your custom image. Use your custom image in Shakudo Platform \u00b6 Select \u201ccustom\u201d Session Type (when starting Sessions) or Job Type (when starting Pipeline jobs) and paste your image URL in the text field.","title":"Custom Images"},{"location":"custom_images.html#create-and-use-custom-images-in-shakudo-platform","text":"","title":"Create and use custom images in Shakudo Platform"},{"location":"custom_images.html#prerequisites","text":"The following tools are needed to create a custom Shakudo Platform image: kubectl + kubeconfig access to Shakudo Platform gcloud cli docker","title":"Prerequisites"},{"location":"custom_images.html#retrieve-and-use-image-pull-credentials","text":"Get and store the credentials from your Shakudo Platform cluster secrets: kubectl get secrets/image-pull-secret --template=\"{{index .data \\\"mothership-key-content.json\\\" | base64decode}}\" > keyfile_mothership.json Authenticate using Shakudo Platform image pull credentials: gcloud auth activate-service-account --key-file=keyfile_mothership.json gcloud config set account mothership@hyperplane-test.iam.gserviceaccount.com Pulling the basic Shakudo Platform image: docker pull gcr.io/devsentient-infra/dev/jhub-basic@sha256:65873aad4648282b0f74669cca623205acaca25e9be0180071b620552b77c42b","title":"Retrieve and use image pull credentials"},{"location":"custom_images.html#create-your-custom-image","text":"Now that you have the basic Shakudo Platform image, you can follow the normal steps to create a Docker container. For example, you can create a Dockerfile that looks like this following: FROM gcr.io/devsentient-infra/dev/jhub-basic:e520b7d74c7c04abc764dd43d60229ffefefb362 RUN pip install [some-packages-here] You can now build your image (e.g. docker build -t image_name . in the directory of your Dockerfile) and push to your registry (e.g. docker image tag image_name:image_tag registry-host:my-org/image_name:image_tag and docker image push registry-host:my-org/image_name:image_tag ) as normal. Ensure you grant the Shakudo Platform service account permissions to pull your custom image.","title":"Create your custom image"},{"location":"custom_images.html#use-your-custom-image-in-shakudo-platform","text":"Select \u201ccustom\u201d Session Type (when starting Sessions) or Job Type (when starting Pipeline jobs) and paste your image URL in the text field.","title":"Use your custom image in Shakudo Platform"},{"location":"custom_secret.html","text":"Configuring a Custom Secret to Attach to Pipelines \u00b6 The Steps To Deploy A Secret \u00b6 Install kubectl : curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Run the following command in your Shakudo Platform session to get the secret name. echo $HYPERPLANE_CUSTOM_DEPLOY_SECRET Run the following command in your Shakudo Platform session to get the cluster's pipeline namespace. echo $PIPELINES_NAMESPACE Base64 encode your secret my_secret_key.json . We'll refer to the output of the following command as ENCODED_SECRET cat my_secret_key.json | base64 -w 0 Create the kubernetes secret YAML file my_secret_key_k8s.yaml as follows: apiVersion: v1 data: my_key_file.json: ENCODED_SECRET kind: Secret metadata: name: HYPERPLANE_CUSTOM_DEPLOY_SECRET namespace: PIPELINES_NAMESPACE type: Opaque Deploy the kubernetes secret: kubectl apply -f my_secret_key_k8s.yaml You should see an output like this: \u276f kubectl apply -f my_secret_key_k8s.yaml secret/custom-deploy-secrets-vpqqv7ba configured Your custom secret is now configured. Configuring GCP SDK / CLI to use the new secret \u00b6 The app key is only deployed in the pipelines and services; in jupyter each user can use their own credentials. The follow example shows how to configure GCP to use the app key in a PipelineJob; you can use the following function in your jupyter notebooks: import os def is_jhub(): if (\"HYPERPLANE_JOB_ID\" not in os.environ or os.environ[\"HYPERPLANE_JOB_ID\"] == \"__local_test\"): print(\"From jupyter. Not modifying GCP credentials path.\") return True else: print(\"From pipeline. Updating GCP credentials path to app key.\") os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/etc/hyperplane/secrets/my_key_file.json\" return False","title":"Custom Secrets"},{"location":"custom_secret.html#configuring-a-custom-secret-to-attach-to-pipelines","text":"","title":"Configuring a Custom Secret to Attach to Pipelines"},{"location":"custom_secret.html#the-steps-to-deploy-a-secret","text":"Install kubectl : curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Run the following command in your Shakudo Platform session to get the secret name. echo $HYPERPLANE_CUSTOM_DEPLOY_SECRET Run the following command in your Shakudo Platform session to get the cluster's pipeline namespace. echo $PIPELINES_NAMESPACE Base64 encode your secret my_secret_key.json . We'll refer to the output of the following command as ENCODED_SECRET cat my_secret_key.json | base64 -w 0 Create the kubernetes secret YAML file my_secret_key_k8s.yaml as follows: apiVersion: v1 data: my_key_file.json: ENCODED_SECRET kind: Secret metadata: name: HYPERPLANE_CUSTOM_DEPLOY_SECRET namespace: PIPELINES_NAMESPACE type: Opaque Deploy the kubernetes secret: kubectl apply -f my_secret_key_k8s.yaml You should see an output like this: \u276f kubectl apply -f my_secret_key_k8s.yaml secret/custom-deploy-secrets-vpqqv7ba configured Your custom secret is now configured.","title":"The Steps To Deploy A Secret"},{"location":"custom_secret.html#configuring-gcp-sdk-cli-to-use-the-new-secret","text":"The app key is only deployed in the pipelines and services; in jupyter each user can use their own credentials. The follow example shows how to configure GCP to use the app key in a PipelineJob; you can use the following function in your jupyter notebooks: import os def is_jhub(): if (\"HYPERPLANE_JOB_ID\" not in os.environ or os.environ[\"HYPERPLANE_JOB_ID\"] == \"__local_test\"): print(\"From jupyter. Not modifying GCP credentials path.\") return True else: print(\"From pipeline. Updating GCP credentials path to app key.\") os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/etc/hyperplane/secrets/my_key_file.json\" return False","title":"Configuring GCP SDK / CLI to use the new secret"},{"location":"dask_concepts.html","text":"Working with Dask \u00b6 Let's explore some Dask concepts. In this section, we introduce dask bags and dataframes, as well as common file formats. For full Dask documentation, check out https://docs.dask.org/en/ Collections \u00b6 Dask collections are useful for large datasets because they support delayed tasks. We will explore three types\u2014 Dask bags, Dask dataframes, and Dask arrays. dask bags \u00b6 Dask bags (synonymous with multisets) are unordered collections of immutable objects. Below are some common operations: Select records where: b . filter ( lambda record : record [ 'num_clicks' ] > 2 ) . take ( 2 ) ({'id': '01mz489cnkd', 'area': 'Aerial Alaska', 'num_clicks': 3, 'info': {'a_field': 0}}, {'id': '25z48t9cfaf', 'area': 'Bustling Birktown', 'num_clicks': 5, 'info': {'a_field': 1}}) Select one field: b . map ( lambda record : record [ 'area' ]) . take ( 2 ) ('Aerial Alaska', 'Bustling Birktown') Aggregate the number of records in your bag: b . count () . compute () 100000 Note that the .take(n) function will return the first n records from the bag, only in the first partition. For more info, see https://examples.dask.org/bag.html Dask dataframes \u00b6 Dask dataframes are collections of pandas dataframes. It can be used in cases where one pandas dataframe is too large to fit in memory and to speed up expensive computations by using multiple cores. To read multiple csvs, use the * or a list of files. Each file will be read into a separate partition. import dask.dataframe as dd df = dd.read_csv('2014-*.csv') A common workflow is the following: Load large datasets from files Filter to a subset of data Shuffle data to get an intelligent index Perform queries or aggregations using the indices For more information on Dask dataframes, see https://docs.dask.org/en/latest/dataframe.html . Operations \u00b6 Lazy calculations \u00b6 Dask operates with lazy collections, meaning operations on a collection are simply scheduled by the scheduler, but the actual calculation will not be triggered until explicitly called. At first, you will notice that dataframe operations or functions seem to happen almost instantaneously, but nothing will be calculated until one of the following is used: .persist() .compute() .load() .compute() will trigger a computation on the Dask cluster without returning anything. You can use this if some of your functions include saving to a location. .persist() will trigger a computation on the Dask cluster and store the results in ram. Use this sparingly, only if you need to use an intermediate collection, or after a computationally expensive operation such as index, groupby, etc. .load() will trigger a computation on the Dask cluster when you are working with Dask xarrays. For example, you can trigger all computations on your dataframe like the following: df = dask . df . read_parquet ( 'file*.csv' ) df = df [[ 'col_a' , 'col_b' ]] df = df . drop_duplicates () df = client . persist ( df ) Repartitioning \u00b6 After a few computations, your Dask df may need to be repartitioned, due to the partition size-number tradeoff. Partitions that are too large will cause out of memory errors, while too many partitions will incure a larger overhead time for the schedule to process. See more on best practices at https://docs.dask.org/en/latest/dataframe-best-practices.html . Split out \u00b6 Dataframe aggregation operations can get wuite slow. Try to use split_out in aggregation operationg like groupbys to spread the aggregation work. Cheap vs. expensive computations \u00b6 Examples of fast and cheap computations: Element-wise ops (addition, multiplication) Row-wise operations and filtering: df[df.x > 0] Joining Dask dfs along indexed fields, or joining with a one-partition Dask df Max, min, count, common aggregations ( df.groupby(df.x).y.max() ) isin: df[df.x.isin([1, 2, 3])] drop_duplicates groupby-apply on an index: df.groupby(['idx', 'x']).apply(myfunc) Examples of slow and expensive computations (for this reason, it is often recommended to use persist your data after these steps for stability): setting an index: df.set_index(df.x) groupby-apply on non-index fields: df.groupby(df.x).apply(myfunc) joining two dataframes along non-index columns File types \u00b6 parquet files \u00b6 Parquet is a columnar storage format for Hadoop, which enables parallel reading and writing, and is most useful for efficiently filtering a subset of fields in a Dask df. avro files \u00b6 Avro is a row-based storage format for Hadoop, which is most efficient if you intend to retrieve and use all fields or columns in the dataset. Saving to cloud storage \u00b6 Remember that new workers are spun up when use .initialize_cluster() , and they are destroyed on cluster.close() . This means you should ensure your intermediate and output files are saved in a cloud storage location that can be accessed outside of each node. This can be achieved through the following code example: gcp_project = YOUR_GCP_PROJECT gcs_client = storage . Client ( project = gcp_project ) bucket = gcs_client . get_bucket ( bucket ) blob = bucket . blob ( yourfile ) blob . upload_from_string ( filename , content_type = 'application/x-www-form-urlencoded;charset=UTF-8' )","title":"Working with Dask"},{"location":"dask_concepts.html#working-with-dask","text":"Let's explore some Dask concepts. In this section, we introduce dask bags and dataframes, as well as common file formats. For full Dask documentation, check out https://docs.dask.org/en/","title":"Working with Dask"},{"location":"dask_concepts.html#collections","text":"Dask collections are useful for large datasets because they support delayed tasks. We will explore three types\u2014 Dask bags, Dask dataframes, and Dask arrays.","title":"Collections"},{"location":"dask_concepts.html#dask-bags","text":"Dask bags (synonymous with multisets) are unordered collections of immutable objects. Below are some common operations: Select records where: b . filter ( lambda record : record [ 'num_clicks' ] > 2 ) . take ( 2 ) ({'id': '01mz489cnkd', 'area': 'Aerial Alaska', 'num_clicks': 3, 'info': {'a_field': 0}}, {'id': '25z48t9cfaf', 'area': 'Bustling Birktown', 'num_clicks': 5, 'info': {'a_field': 1}}) Select one field: b . map ( lambda record : record [ 'area' ]) . take ( 2 ) ('Aerial Alaska', 'Bustling Birktown') Aggregate the number of records in your bag: b . count () . compute () 100000 Note that the .take(n) function will return the first n records from the bag, only in the first partition. For more info, see https://examples.dask.org/bag.html","title":"dask bags"},{"location":"dask_concepts.html#dask-dataframes","text":"Dask dataframes are collections of pandas dataframes. It can be used in cases where one pandas dataframe is too large to fit in memory and to speed up expensive computations by using multiple cores. To read multiple csvs, use the * or a list of files. Each file will be read into a separate partition. import dask.dataframe as dd df = dd.read_csv('2014-*.csv') A common workflow is the following: Load large datasets from files Filter to a subset of data Shuffle data to get an intelligent index Perform queries or aggregations using the indices For more information on Dask dataframes, see https://docs.dask.org/en/latest/dataframe.html .","title":"Dask dataframes"},{"location":"dask_concepts.html#operations","text":"","title":"Operations"},{"location":"dask_concepts.html#lazy-calculations","text":"Dask operates with lazy collections, meaning operations on a collection are simply scheduled by the scheduler, but the actual calculation will not be triggered until explicitly called. At first, you will notice that dataframe operations or functions seem to happen almost instantaneously, but nothing will be calculated until one of the following is used: .persist() .compute() .load() .compute() will trigger a computation on the Dask cluster without returning anything. You can use this if some of your functions include saving to a location. .persist() will trigger a computation on the Dask cluster and store the results in ram. Use this sparingly, only if you need to use an intermediate collection, or after a computationally expensive operation such as index, groupby, etc. .load() will trigger a computation on the Dask cluster when you are working with Dask xarrays. For example, you can trigger all computations on your dataframe like the following: df = dask . df . read_parquet ( 'file*.csv' ) df = df [[ 'col_a' , 'col_b' ]] df = df . drop_duplicates () df = client . persist ( df )","title":"Lazy calculations"},{"location":"dask_concepts.html#repartitioning","text":"After a few computations, your Dask df may need to be repartitioned, due to the partition size-number tradeoff. Partitions that are too large will cause out of memory errors, while too many partitions will incure a larger overhead time for the schedule to process. See more on best practices at https://docs.dask.org/en/latest/dataframe-best-practices.html .","title":"Repartitioning"},{"location":"dask_concepts.html#split-out","text":"Dataframe aggregation operations can get wuite slow. Try to use split_out in aggregation operationg like groupbys to spread the aggregation work.","title":"Split out"},{"location":"dask_concepts.html#cheap-vs-expensive-computations","text":"Examples of fast and cheap computations: Element-wise ops (addition, multiplication) Row-wise operations and filtering: df[df.x > 0] Joining Dask dfs along indexed fields, or joining with a one-partition Dask df Max, min, count, common aggregations ( df.groupby(df.x).y.max() ) isin: df[df.x.isin([1, 2, 3])] drop_duplicates groupby-apply on an index: df.groupby(['idx', 'x']).apply(myfunc) Examples of slow and expensive computations (for this reason, it is often recommended to use persist your data after these steps for stability): setting an index: df.set_index(df.x) groupby-apply on non-index fields: df.groupby(df.x).apply(myfunc) joining two dataframes along non-index columns","title":"Cheap vs. expensive computations"},{"location":"dask_concepts.html#file-types","text":"","title":"File types"},{"location":"dask_concepts.html#parquet-files","text":"Parquet is a columnar storage format for Hadoop, which enables parallel reading and writing, and is most useful for efficiently filtering a subset of fields in a Dask df.","title":"parquet files"},{"location":"dask_concepts.html#avro-files","text":"Avro is a row-based storage format for Hadoop, which is most efficient if you intend to retrieve and use all fields or columns in the dataset.","title":"avro files"},{"location":"dask_concepts.html#saving-to-cloud-storage","text":"Remember that new workers are spun up when use .initialize_cluster() , and they are destroyed on cluster.close() . This means you should ensure your intermediate and output files are saved in a cloud storage location that can be accessed outside of each node. This can be achieved through the following code example: gcp_project = YOUR_GCP_PROJECT gcs_client = storage . Client ( project = gcp_project ) bucket = gcs_client . get_bucket ( bucket ) blob = bucket . blob ( yourfile ) blob . upload_from_string ( filename , content_type = 'application/x-www-form-urlencoded;charset=UTF-8' )","title":"Saving to cloud storage"},{"location":"dask_hyperplane.html","text":"Working with Dask \u00b6 Initializing a Dask cluster \u00b6 Quickstart a distributed Dask cluster using the following. from hyperplane.notebook_common import quickstart_dask ray_cluster = quickstart_dask ( num_workers = 4 , size = 'hyperplane-med-high-mem' ) Initialize a distributed Dask cluster with ease using the following: from hyperplane import notebook_common as nc client , cluster = nc . initialize_cluster ( num_workers = 2 , nthreads = 1 , nprocs = 15 , ram_gb_per_proc = 0.7 , cores_per_worker = 15 ) Initialize a distributed Dask cluster with Dask defaults using the following: from hyperplane import notebook_common as nc client , cluster = nc . initialize_dask_cluster ( num_workers = 2 , total_cores_per_worker = 15 , total_ram_per_worker = 12 ) Choosing a cluster config \u00b6 Shakudo Platform currently comes with nine pre-configured worker pools: 'hyperplane-xs-high-mem' (POOL_4_32) with 3.5 allocatable cores, and 27.0 allocatable ram 'hyperplane-small' (POOL_8_8) with 7.0 allocatable cores, and 5.0 allocatable ram 'hyperplane-small-mid-mem' (POOL_8_16) with 7.5 allocatable cores, and 12.0 allocatable ram 'hyperplane-small-high-mem' (POOL_8_64) with 7.5 allocatable cores, and 58.0 allocatable ram 'hyperplane-med' (POOL_16_16) with 15.0 allocatable cores, and 12.0 allocatable ram 'hyperplane-med-mid-mem' (POOL_16_32) with 15.0 allocatable cores, and 27.0 allocatable ram 'hyperplane-med-high-mem' (POOL_16_128) with 15.0 allocatable cores, and 110.0 allocatable ram 'hyperplane-large' (POOL_32_32) with 28.0 allocatable cores, and 27.0 allocatable ram 'hyperplane-xxl-high-mem' (POOL_96_768) with 94.0 allocatable cores, and 675.0 allocatable ram Once you specify your number of cores and memory, Shakudo Platform will automatically choose the most appropriate pool from the above. You can also choose one of the above with the quickstart_ray function. If you are aiming for a specific pool, ensure your nprocs * nthreads <= cores_per_worker <= the number of allocatable cores and nprocs * ram_gb_per_proc <= allocatable ram. For example, if you would like to use a DASK_POOL_16_16 worker, you may want to choose the following cluster initalization nprocs = 5 nthreads = 3 ram_gb_per_proc = 2.4 Notes about choosing workers and specs \u00b6 A stable setup for jobs with aggregation (data transfer) is one with a minimum of 10x of the data size. Worker memory usage is about 10% initially. If at any point any worker\u2019s memory usage exceeds 75%, the job is very likely to fail (see Worker freeze policies ). If a job consists of only parallel-friendly operations (no sorting, shuffling, or moving large chunks of data), use more CPU (for example 16_16 , 32_32 ). Otherwise use more memory (for example 16_128 ). If a job requires both huge data and a large number of tasks, split it into multiple jobs to avoid errors. For example, an optimized setting for 1TB group-by job is to split into 10 pieces (100GB each) and use 24 of 32_32 nodes. You can further convert the piece indicator to a parameter like chunk_id and convert the code into a pipeline job, then run 10 pipeline jobs concurrently to save more time. Use the steps below to estimate how many nodes and workers you will need: Check data size (uncompressed). For example, 100GB Choose operation type to find a multiplier of memory: light (x4), medium (x8), heavy(x48). Multiply your data size from step 1 by this multiplier. For example, a group-by will be medium, which requires 100G x 8 ~ 800GB total memory Use the number of tasks (heavy vs. light) to determine the number of nodes. If the sequence of operations has many tasks, (computationally heavy), use 32_32 . Otherwise use 16_128 . Multiply your required memory from step 2 by 32 or 128 depending on computation load. For example, 800GB/32GB = 25 nodes, or 800GB/128GB = 8 of 16_128 nodes. At this point, you should have an approximate Dask pool spec and number of workers. Add-on step: Setup automatic retry if in pipeline mode. Sometimes pipelines error out when spinning up nodes, or HTTP error, canceled error. These can be fixed by retrying. Worker freeze policies \u00b6 Below are the defaults for worker memory limits and actions to avoid memory blowup. distributed: worker: memory: target: 0.60 # target fraction to stay below spill: 0.70 # fraction at which we spill to disk pause: 0.80 # fraction at which we pause worker threads terminate: 0.95 # fraction at which we terminate the worker","title":"Dask on Shakudo Platform"},{"location":"dask_hyperplane.html#working-with-dask","text":"","title":"Working with Dask"},{"location":"dask_hyperplane.html#initializing-a-dask-cluster","text":"Quickstart a distributed Dask cluster using the following. from hyperplane.notebook_common import quickstart_dask ray_cluster = quickstart_dask ( num_workers = 4 , size = 'hyperplane-med-high-mem' ) Initialize a distributed Dask cluster with ease using the following: from hyperplane import notebook_common as nc client , cluster = nc . initialize_cluster ( num_workers = 2 , nthreads = 1 , nprocs = 15 , ram_gb_per_proc = 0.7 , cores_per_worker = 15 ) Initialize a distributed Dask cluster with Dask defaults using the following: from hyperplane import notebook_common as nc client , cluster = nc . initialize_dask_cluster ( num_workers = 2 , total_cores_per_worker = 15 , total_ram_per_worker = 12 )","title":"Initializing a Dask cluster"},{"location":"dask_hyperplane.html#choosing-a-cluster-config","text":"Shakudo Platform currently comes with nine pre-configured worker pools: 'hyperplane-xs-high-mem' (POOL_4_32) with 3.5 allocatable cores, and 27.0 allocatable ram 'hyperplane-small' (POOL_8_8) with 7.0 allocatable cores, and 5.0 allocatable ram 'hyperplane-small-mid-mem' (POOL_8_16) with 7.5 allocatable cores, and 12.0 allocatable ram 'hyperplane-small-high-mem' (POOL_8_64) with 7.5 allocatable cores, and 58.0 allocatable ram 'hyperplane-med' (POOL_16_16) with 15.0 allocatable cores, and 12.0 allocatable ram 'hyperplane-med-mid-mem' (POOL_16_32) with 15.0 allocatable cores, and 27.0 allocatable ram 'hyperplane-med-high-mem' (POOL_16_128) with 15.0 allocatable cores, and 110.0 allocatable ram 'hyperplane-large' (POOL_32_32) with 28.0 allocatable cores, and 27.0 allocatable ram 'hyperplane-xxl-high-mem' (POOL_96_768) with 94.0 allocatable cores, and 675.0 allocatable ram Once you specify your number of cores and memory, Shakudo Platform will automatically choose the most appropriate pool from the above. You can also choose one of the above with the quickstart_ray function. If you are aiming for a specific pool, ensure your nprocs * nthreads <= cores_per_worker <= the number of allocatable cores and nprocs * ram_gb_per_proc <= allocatable ram. For example, if you would like to use a DASK_POOL_16_16 worker, you may want to choose the following cluster initalization nprocs = 5 nthreads = 3 ram_gb_per_proc = 2.4","title":"Choosing a cluster config"},{"location":"dask_hyperplane.html#notes-about-choosing-workers-and-specs","text":"A stable setup for jobs with aggregation (data transfer) is one with a minimum of 10x of the data size. Worker memory usage is about 10% initially. If at any point any worker\u2019s memory usage exceeds 75%, the job is very likely to fail (see Worker freeze policies ). If a job consists of only parallel-friendly operations (no sorting, shuffling, or moving large chunks of data), use more CPU (for example 16_16 , 32_32 ). Otherwise use more memory (for example 16_128 ). If a job requires both huge data and a large number of tasks, split it into multiple jobs to avoid errors. For example, an optimized setting for 1TB group-by job is to split into 10 pieces (100GB each) and use 24 of 32_32 nodes. You can further convert the piece indicator to a parameter like chunk_id and convert the code into a pipeline job, then run 10 pipeline jobs concurrently to save more time. Use the steps below to estimate how many nodes and workers you will need: Check data size (uncompressed). For example, 100GB Choose operation type to find a multiplier of memory: light (x4), medium (x8), heavy(x48). Multiply your data size from step 1 by this multiplier. For example, a group-by will be medium, which requires 100G x 8 ~ 800GB total memory Use the number of tasks (heavy vs. light) to determine the number of nodes. If the sequence of operations has many tasks, (computationally heavy), use 32_32 . Otherwise use 16_128 . Multiply your required memory from step 2 by 32 or 128 depending on computation load. For example, 800GB/32GB = 25 nodes, or 800GB/128GB = 8 of 16_128 nodes. At this point, you should have an approximate Dask pool spec and number of workers. Add-on step: Setup automatic retry if in pipeline mode. Sometimes pipelines error out when spinning up nodes, or HTTP error, canceled error. These can be fixed by retrying.","title":"Notes about choosing workers and specs"},{"location":"dask_hyperplane.html#worker-freeze-policies","text":"Below are the defaults for worker memory limits and actions to avoid memory blowup. distributed: worker: memory: target: 0.60 # target fraction to stay below spill: 0.70 # fraction at which we spill to disk pause: 0.80 # fraction at which we pause worker threads terminate: 0.95 # fraction at which we terminate the worker","title":"Worker freeze policies"},{"location":"daskify_workflow.html","text":"Daskify your workflow \u00b6 When using distributed dask with KubeCluster, we recommend storing your input and output data on a cloud storage system to take advantage of read/write parallelism across different machines. Scaling number of workers \u00b6 To scale up a dask cluster using notebook_common.initialize_cluster() , consider the following workflow: Refer to How to Choose a Cluster Config for a general guideline on choosing node sizes and configs. Generally, you may want fewer n_procs if you intend to use more memory per process and more n_procs if your functions do not require much memory. Once you have tested your configuration on a smaller sample of data, you can scale up the number of workers linearly. For example, if you are parallelizing a for loop or a list of inputs, consider first testing with a small portion of the list that can fit into one worker (let's say a list of 4 inputs), then scaling up the number of workers linearly (use 10 workers for 40 list items). Restart your notebook in between switching to different cluster configurations to avoid common issues like missing packages or data (clusters will automatically stop when the notebook is stopped or restarted). Code Examples \u00b6 Below are several code examples for common tasks to daskify your workflow. Dask map to parallelize for loops \u00b6 Dask map is useful for optimizing code that runs in a loop. The general format is the following: ### original: run a for loop or over a list results = [] for a , b in list ( zip ([ 1 , 2 , 3 , 4 ], [ 2 , 3 , 4 , 5 ])): results += [ a_function ( a , b )] ### dask: use map to run over various inputs def a_function ( a : int , b : int ): ... return a * b ## ensure the returned value does not require too much space futures = client . map ( a_function , [ 1 , 2 , 3 , 4 ], [ 2 , 3 , 4 , 5 ]) results = client . gather ( futures ) futures will show the status of each process [f.result for f in futures] or client.gather(futures) will show the results of each process. Ensure that the returned value does not require too much memory, or you will have errors when gathering the results For example, the following snippet optimizes a function that reads a list of files one by one. ## original: this snippet reads a list of files one by one import xarray as xr import gcsfs fs = gcsfs . GCSFileSystem ( project = 'myproject' , token = None ) files_list = [ 'file1' , 'file2' , 'file3' , 'file4' ] gcsmap = gcsfs . mapping . GCSMap ( f 'gs://my-bucket/ { files_list [ 0 ] } ' , gcs = fs ) Glob = xr . open_zarr ( store = gcsmap ) . load () ## add other datasets sequentially for filepath in files_list [ 1 :]: gcsmap = gcsfs . mapping . GCSMap ( f 'gs://my-bucket/ { filepath } ' , gcs = fs ) ds = xr . open_zarr ( store = gcsmap ) . load () Glob = xr . merge ([ Glob , ds ], compat = \"no_conflicts\" , combine_attrs = \"no_conflicts\" ) ## dask: create a function to read one file, then use client.map the function and list of files import xarray as xr import gcsfs fs = gcsfs . GCSFileSystem ( project = 'myproject' , token = None ) files_list = [ 'file1' , 'file2' , 'file3' , 'file4' ] def read_files ( gsfilepath ): gcsmap = gcsfs . mapping . GCSMap ( f \"gs://my-bucket/ { gsfilepath } \" , gcs = fs ) ds = xr . open_zarr ( store = gcsmap ) . load () . persist () return ds dss = client . map ( read_files , files_list ) ds_list = client . gather ( dss ) print ( len ( ds_list )) # output: 4 Glob = xr . merge ( ds_list , compat = \"no_conflicts\" , combine_attrs = \"no_conflicts\" ) Scatter your data to each worker \u00b6 In cases where your function requires some relatively small dataset, consider scattering it to the workers ahead of time. Ensure that your workers are all ready before doing this. sm_data = pd . read_csv ( 'small.csv' ) def a_func ( a ): ... sm_data [ 'col_a' ] = sm_data [ 'col_a' ] * a ... return a client . scatter ( sm_data , broadcast = True ) ## pre-scatter your data client . map ( a_func , [ 1 , 2 , 3 , 4 ]) ## so your map function can use it Install a package not currently in your image \u00b6 Workers will have the same image as your current jhub. Therefore, you will have to wait until all workers are up and running, then install any missing packages using the following: client , cluster = nc . initialize_cluster ( n_workers = 4 ) ## wait until all workers are ready def install (): import os os . system ( 'pip install package' ) client . run ( install ) For missing packages that you intend use often, please contact the Shakudo team to have them pushed to your image. Save and read files from a cloud filesystem instead of your local \u00b6 For compatible formats (zarr, parquet, pandas csvs) ### read and write a pyarrow parquet to google storage import pyarrow.parquet as pq import gcsfs fs = gcsfs . GCSFileSystem () table = pa . Table ( ... ) ## a pyarrow table pq . write_table ( table , f 'gs://my-bucket/file_name' , compression = 'snappy' , filesystem = fs ) pq . read_table ( f 'gs://my-bucket/file_name' , filesystem = fs ) ### read and write a dask dataframe to gcs import dask.dataframe as dd ddf = dd . DataFrame ( ... ) ## a dask dataframe ddf . to_parquet ( 'gcs://mybucket/mypath/output.parquet' ) ## to_csv, to_pickle, etc. also works Download files from gcs to your workers \u00b6 This is useful if your data cannot be read directly with gcsfs like the examples above. def download_file ( filepath : str ): from google.cloud import storage storage_client = storage . Client () bucket = storage_client . get_bucket ( bucket_name ) blob = bucket . blob ( f \" { prefix } / { filepath } \" ) ## ensure you create the existing parent folders if necessary: Path ( '/' . join ( filepath . split ( '/' )[: - 1 ])) . mkdir ( parents = True , exist_ok = True ) blob . download_to_filename ( filepath ) client . map ( download_file , filepaths )","title":"Daskify your workflow"},{"location":"daskify_workflow.html#daskify-your-workflow","text":"When using distributed dask with KubeCluster, we recommend storing your input and output data on a cloud storage system to take advantage of read/write parallelism across different machines.","title":"Daskify your workflow"},{"location":"daskify_workflow.html#scaling-number-of-workers","text":"To scale up a dask cluster using notebook_common.initialize_cluster() , consider the following workflow: Refer to How to Choose a Cluster Config for a general guideline on choosing node sizes and configs. Generally, you may want fewer n_procs if you intend to use more memory per process and more n_procs if your functions do not require much memory. Once you have tested your configuration on a smaller sample of data, you can scale up the number of workers linearly. For example, if you are parallelizing a for loop or a list of inputs, consider first testing with a small portion of the list that can fit into one worker (let's say a list of 4 inputs), then scaling up the number of workers linearly (use 10 workers for 40 list items). Restart your notebook in between switching to different cluster configurations to avoid common issues like missing packages or data (clusters will automatically stop when the notebook is stopped or restarted).","title":"Scaling number of workers"},{"location":"daskify_workflow.html#code-examples","text":"Below are several code examples for common tasks to daskify your workflow.","title":"Code Examples"},{"location":"daskify_workflow.html#dask-map-to-parallelize-for-loops","text":"Dask map is useful for optimizing code that runs in a loop. The general format is the following: ### original: run a for loop or over a list results = [] for a , b in list ( zip ([ 1 , 2 , 3 , 4 ], [ 2 , 3 , 4 , 5 ])): results += [ a_function ( a , b )] ### dask: use map to run over various inputs def a_function ( a : int , b : int ): ... return a * b ## ensure the returned value does not require too much space futures = client . map ( a_function , [ 1 , 2 , 3 , 4 ], [ 2 , 3 , 4 , 5 ]) results = client . gather ( futures ) futures will show the status of each process [f.result for f in futures] or client.gather(futures) will show the results of each process. Ensure that the returned value does not require too much memory, or you will have errors when gathering the results For example, the following snippet optimizes a function that reads a list of files one by one. ## original: this snippet reads a list of files one by one import xarray as xr import gcsfs fs = gcsfs . GCSFileSystem ( project = 'myproject' , token = None ) files_list = [ 'file1' , 'file2' , 'file3' , 'file4' ] gcsmap = gcsfs . mapping . GCSMap ( f 'gs://my-bucket/ { files_list [ 0 ] } ' , gcs = fs ) Glob = xr . open_zarr ( store = gcsmap ) . load () ## add other datasets sequentially for filepath in files_list [ 1 :]: gcsmap = gcsfs . mapping . GCSMap ( f 'gs://my-bucket/ { filepath } ' , gcs = fs ) ds = xr . open_zarr ( store = gcsmap ) . load () Glob = xr . merge ([ Glob , ds ], compat = \"no_conflicts\" , combine_attrs = \"no_conflicts\" ) ## dask: create a function to read one file, then use client.map the function and list of files import xarray as xr import gcsfs fs = gcsfs . GCSFileSystem ( project = 'myproject' , token = None ) files_list = [ 'file1' , 'file2' , 'file3' , 'file4' ] def read_files ( gsfilepath ): gcsmap = gcsfs . mapping . GCSMap ( f \"gs://my-bucket/ { gsfilepath } \" , gcs = fs ) ds = xr . open_zarr ( store = gcsmap ) . load () . persist () return ds dss = client . map ( read_files , files_list ) ds_list = client . gather ( dss ) print ( len ( ds_list )) # output: 4 Glob = xr . merge ( ds_list , compat = \"no_conflicts\" , combine_attrs = \"no_conflicts\" )","title":"Dask map to parallelize for loops"},{"location":"daskify_workflow.html#scatter-your-data-to-each-worker","text":"In cases where your function requires some relatively small dataset, consider scattering it to the workers ahead of time. Ensure that your workers are all ready before doing this. sm_data = pd . read_csv ( 'small.csv' ) def a_func ( a ): ... sm_data [ 'col_a' ] = sm_data [ 'col_a' ] * a ... return a client . scatter ( sm_data , broadcast = True ) ## pre-scatter your data client . map ( a_func , [ 1 , 2 , 3 , 4 ]) ## so your map function can use it","title":"Scatter your data to each worker"},{"location":"daskify_workflow.html#install-a-package-not-currently-in-your-image","text":"Workers will have the same image as your current jhub. Therefore, you will have to wait until all workers are up and running, then install any missing packages using the following: client , cluster = nc . initialize_cluster ( n_workers = 4 ) ## wait until all workers are ready def install (): import os os . system ( 'pip install package' ) client . run ( install ) For missing packages that you intend use often, please contact the Shakudo team to have them pushed to your image.","title":"Install a package not currently in your image"},{"location":"daskify_workflow.html#save-and-read-files-from-a-cloud-filesystem-instead-of-your-local","text":"For compatible formats (zarr, parquet, pandas csvs) ### read and write a pyarrow parquet to google storage import pyarrow.parquet as pq import gcsfs fs = gcsfs . GCSFileSystem () table = pa . Table ( ... ) ## a pyarrow table pq . write_table ( table , f 'gs://my-bucket/file_name' , compression = 'snappy' , filesystem = fs ) pq . read_table ( f 'gs://my-bucket/file_name' , filesystem = fs ) ### read and write a dask dataframe to gcs import dask.dataframe as dd ddf = dd . DataFrame ( ... ) ## a dask dataframe ddf . to_parquet ( 'gcs://mybucket/mypath/output.parquet' ) ## to_csv, to_pickle, etc. also works","title":"Save and read files from a cloud filesystem instead of your local"},{"location":"daskify_workflow.html#download-files-from-gcs-to-your-workers","text":"This is useful if your data cannot be read directly with gcsfs like the examples above. def download_file ( filepath : str ): from google.cloud import storage storage_client = storage . Client () bucket = storage_client . get_bucket ( bucket_name ) blob = bucket . blob ( f \" { prefix } / { filepath } \" ) ## ensure you create the existing parent folders if necessary: Path ( '/' . join ( filepath . split ( '/' )[: - 1 ])) . mkdir ( parents = True , exist_ok = True ) blob . download_to_filename ( filepath ) client . map ( download_file , filepaths )","title":"Download files from gcs to your workers"},{"location":"debug_jobs.html","text":"To debug your pipeline job follow the following steps: Connect your IDE to a pipeline job \u00b6 From the Shakudo Platform dashboard jobs tab, start a new job by pressing the + button. Toggle on the Debug setting. Notice that the change is automatically reflected in the GraphQL query as well: debuggable: true Specify the pipeline yaml path as any other job and press Create . Once the job is created from the list of jobs in the dashboard click on the three dots to expand the menu and find the option <> Connect Debugger . Click the button to have the ssh command to connect your job to your IDE copied onto your clipboard. Open your IDE such as VSCode. Make sure that you have the Remote SSH extension installed. If not go to this page and follow step 1. under First Time Connecting . Navigate to the Remote Explorer tab (if using VSCode, if not navigate to where you would add remote development ssh connections). Add ssh target by clicking on + button and pasting the ssh connection command in your clipboard. Once you hit enter it will prompt you to specify where to save. Select the config file which you would like to update. Once the host is added you should see that the jobs root folder is added to the list of folders under your ssh targets. Click on the open file icon to open an ssh host in a new window. Notice in your terminal that it has routed to the main root folder of your pipeline job. Debug your pipeline job \u00b6 You can use the VSCode UI set breakpoints by clicking to the left of the line number you would like to add the breakpoints to. The added breakpoints will be indicated by a red dot. Make sure you have the Python extension installed - this only has to be done once however, if the config file is deleted then you would have to install again. Once the breakpoints are added, navigate to the drop down beside the icon to run the python file at the top right of your window. From there select Debug Python File . You can now use the UI to debug by running the job line by line. If you would like to stop the current debugging session but would like to be able to connect to the job again, click the bottom left SSH: [name.de.hyperplane.dev](http://name.de.hyperplane.dev) and click close remote connection . If you want to stop the debugging completely, then type in stop_debug in your terminal. The ssh connection will be terminated and the job will be marked as done on the Shakudo Platform dashboard (i.e. you will not be able to connect back to this job again. To debug again you will need to create a new debug job). The following video traces the steps above on how to debug a pipeline job.","title":"Debug a pipeline job"},{"location":"debug_jobs.html#connect-your-ide-to-a-pipeline-job","text":"From the Shakudo Platform dashboard jobs tab, start a new job by pressing the + button. Toggle on the Debug setting. Notice that the change is automatically reflected in the GraphQL query as well: debuggable: true Specify the pipeline yaml path as any other job and press Create . Once the job is created from the list of jobs in the dashboard click on the three dots to expand the menu and find the option <> Connect Debugger . Click the button to have the ssh command to connect your job to your IDE copied onto your clipboard. Open your IDE such as VSCode. Make sure that you have the Remote SSH extension installed. If not go to this page and follow step 1. under First Time Connecting . Navigate to the Remote Explorer tab (if using VSCode, if not navigate to where you would add remote development ssh connections). Add ssh target by clicking on + button and pasting the ssh connection command in your clipboard. Once you hit enter it will prompt you to specify where to save. Select the config file which you would like to update. Once the host is added you should see that the jobs root folder is added to the list of folders under your ssh targets. Click on the open file icon to open an ssh host in a new window. Notice in your terminal that it has routed to the main root folder of your pipeline job.","title":"Connect your IDE to a pipeline job"},{"location":"debug_jobs.html#debug-your-pipeline-job","text":"You can use the VSCode UI set breakpoints by clicking to the left of the line number you would like to add the breakpoints to. The added breakpoints will be indicated by a red dot. Make sure you have the Python extension installed - this only has to be done once however, if the config file is deleted then you would have to install again. Once the breakpoints are added, navigate to the drop down beside the icon to run the python file at the top right of your window. From there select Debug Python File . You can now use the UI to debug by running the job line by line. If you would like to stop the current debugging session but would like to be able to connect to the job again, click the bottom left SSH: [name.de.hyperplane.dev](http://name.de.hyperplane.dev) and click close remote connection . If you want to stop the debugging completely, then type in stop_debug in your terminal. The ssh connection will be terminated and the job will be marked as done on the Shakudo Platform dashboard (i.e. you will not be able to connect back to this job again. To debug again you will need to create a new debug job). The following video traces the steps above on how to debug a pipeline job.","title":"Debug your pipeline job"},{"location":"eventbridge.html","text":"Submitting Data to EventBridge \u00b6 We provide multiple convenience functions for submitting data to Amazon EventBridge, depending on the level of simplicity or direct control required. If your deployment has a default eventbus configured then any event without a specified target will be sent to that. You can import some or all of the functions from this library as follows: from hyperplane.eventbridge import send_to_eventbridge, send_csv_to_eventbridge, customer_specific_send_csv_to_eventbridge, aws_eventbridge_set_cred Manually specify event parameters \u00b6 send_to_eventbridge(event_name: str, message_meta: dict or list, message_data: dict or list, event_source: str = None, eventbus_name: str = None) This function provides the most direct interface to EventBridge. The message data and metadata structures must be manually constructed, but they may have any form that EventBridge can accept. \"Source\": event_source (or name of calling function if unspecified), \"DetailType\": event_name, \"Detail\": { \"eventName\": event_name, \"payload\": { \"meta\": message_meta, \"data\": message_data, }, } \"EventBusName\": eventbus_name (or deployment default if unspecified) Parameters \u00b6 event_name ( string ) - Label used to identify and optionally filter the event message_meta ( dict or list ) - Metadata in json format describing the event message_data ( dict or list ) - Data in json format event_source ( string , optional) - Label identifying what caused the event or created the data, often a model name or experiment number eventbus_name ( string , optional) - Target eventbus name or ARN to which the event is sent, overrides system default Create an event from CSV and metadata Files \u00b6 send_csv_to_eventbridge(event_name: str, model_name: str, csv_file: str, meta_file: str, csv_filter = None, meta_filter = None, event_source = None, eventbus_name: str = None) This function reads the given CSV and metadata files and uses them to create an event, optionally filtering by metadata label or CSV header. The resulting event has the same structure as send_to_eventbridge() , with message_meta and message_data being automatically constructed from meta_file and csv_file respectively. Parameters \u00b6 event_name ( string ) - Label used to identify and optionally filter the event model_name ( string ) - Name of the model or experiment that generated the data csv_file ( string ) - Path pointing to CSV file containing data to upload, with the first row providing column labels meta_file ( string ) - Path pointing to file containing any desired event metadata in JSON format csv_filter ( list of strings , optional) - If provided, only columns from the CSV file with headers found in this list will be included meta_filter ( list of strings , optional) - If provided, only metadata properties with labels found in this list will be included event_source ( string , optional) - Label identifying what caused the event or created the data, defaults to model_name if not provided eventbus_name ( string , optional) - Target eventbus name or ARN to which the event is sent, overrides system default Per-client custom pre-processor \u00b6 customer_specific_send_csv_to_eventbridge(csv_file: str, meta_file: str, model_name: str, eventbus_name: str = None) If you have a consistent event format that you prefer, we can provide a custom function to automatically format the data in the appropriate manner. This sort of function prototype may vary, but the general outline would be something like this. Parameters \u00b6 csv_file ( string ) - Path pointing to CSV file containing data to upload meta_file ( string ) - Path pointing to file containing any desired event metadata model_name ( string ) - Name of the model or experiment that generated the data eventbus_name ( string , optional) - Target eventbus name or ARN to which the event is sent, overrides system default Manual Authorization Config \u00b6 aws_eventbridge_set_cred(aws_cred_path: str, aws_id: str = None, aws_key: str = None): This library assumes that your deployment includes the appropriate AWS keys. If you are attempting to use this library before discussing those deployment options with us, or if you would prefer to use a different account for some subset of experiments, you can use this function to create the appropriate AWS credentials file. Parameters \u00b6 aws_cred_path ( string ) - Path of credentials file to be created, should probably be $HOME/.aws/credentials for most use cases aws_id ( string , optional) - AWS access key ID for desired account, uses deployment default if unspecified aws_key ( string , optional) - AWS access key secret for desired account, uses deployment default if unspecified","title":"Submitting Data to EventBridge"},{"location":"eventbridge.html#submitting-data-to-eventbridge","text":"We provide multiple convenience functions for submitting data to Amazon EventBridge, depending on the level of simplicity or direct control required. If your deployment has a default eventbus configured then any event without a specified target will be sent to that. You can import some or all of the functions from this library as follows: from hyperplane.eventbridge import send_to_eventbridge, send_csv_to_eventbridge, customer_specific_send_csv_to_eventbridge, aws_eventbridge_set_cred","title":"Submitting Data to EventBridge"},{"location":"eventbridge.html#manually-specify-event-parameters","text":"send_to_eventbridge(event_name: str, message_meta: dict or list, message_data: dict or list, event_source: str = None, eventbus_name: str = None) This function provides the most direct interface to EventBridge. The message data and metadata structures must be manually constructed, but they may have any form that EventBridge can accept. \"Source\": event_source (or name of calling function if unspecified), \"DetailType\": event_name, \"Detail\": { \"eventName\": event_name, \"payload\": { \"meta\": message_meta, \"data\": message_data, }, } \"EventBusName\": eventbus_name (or deployment default if unspecified)","title":"Manually specify event parameters"},{"location":"eventbridge.html#parameters","text":"event_name ( string ) - Label used to identify and optionally filter the event message_meta ( dict or list ) - Metadata in json format describing the event message_data ( dict or list ) - Data in json format event_source ( string , optional) - Label identifying what caused the event or created the data, often a model name or experiment number eventbus_name ( string , optional) - Target eventbus name or ARN to which the event is sent, overrides system default","title":"Parameters"},{"location":"eventbridge.html#create-an-event-from-csv-and-metadata-files","text":"send_csv_to_eventbridge(event_name: str, model_name: str, csv_file: str, meta_file: str, csv_filter = None, meta_filter = None, event_source = None, eventbus_name: str = None) This function reads the given CSV and metadata files and uses them to create an event, optionally filtering by metadata label or CSV header. The resulting event has the same structure as send_to_eventbridge() , with message_meta and message_data being automatically constructed from meta_file and csv_file respectively.","title":"Create an event from CSV and metadata Files"},{"location":"eventbridge.html#parameters_1","text":"event_name ( string ) - Label used to identify and optionally filter the event model_name ( string ) - Name of the model or experiment that generated the data csv_file ( string ) - Path pointing to CSV file containing data to upload, with the first row providing column labels meta_file ( string ) - Path pointing to file containing any desired event metadata in JSON format csv_filter ( list of strings , optional) - If provided, only columns from the CSV file with headers found in this list will be included meta_filter ( list of strings , optional) - If provided, only metadata properties with labels found in this list will be included event_source ( string , optional) - Label identifying what caused the event or created the data, defaults to model_name if not provided eventbus_name ( string , optional) - Target eventbus name or ARN to which the event is sent, overrides system default","title":"Parameters"},{"location":"eventbridge.html#per-client-custom-pre-processor","text":"customer_specific_send_csv_to_eventbridge(csv_file: str, meta_file: str, model_name: str, eventbus_name: str = None) If you have a consistent event format that you prefer, we can provide a custom function to automatically format the data in the appropriate manner. This sort of function prototype may vary, but the general outline would be something like this.","title":"Per-client custom pre-processor"},{"location":"eventbridge.html#parameters_2","text":"csv_file ( string ) - Path pointing to CSV file containing data to upload meta_file ( string ) - Path pointing to file containing any desired event metadata model_name ( string ) - Name of the model or experiment that generated the data eventbus_name ( string , optional) - Target eventbus name or ARN to which the event is sent, overrides system default","title":"Parameters"},{"location":"eventbridge.html#manual-authorization-config","text":"aws_eventbridge_set_cred(aws_cred_path: str, aws_id: str = None, aws_key: str = None): This library assumes that your deployment includes the appropriate AWS keys. If you are attempting to use this library before discussing those deployment options with us, or if you would prefer to use a different account for some subset of experiments, you can use this function to create the appropriate AWS credentials file.","title":"Manual Authorization Config"},{"location":"eventbridge.html#parameters_3","text":"aws_cred_path ( string ) - Path of credentials file to be created, should probably be $HOME/.aws/credentials for most use cases aws_id ( string , optional) - AWS access key ID for desired account, uses deployment default if unspecified aws_key ( string , optional) - AWS access key secret for desired account, uses deployment default if unspecified","title":"Parameters"},{"location":"example_notebooks.html","text":"Example notebooks \u00b6 You can find here a list of the official notebooks and scrips provided by Shakudo Platform. Also, we would like to list here interesting content created by the community. If you wrote some notebook(s) leveraging Shakudo Platform and would like be be listed here, please open a Pull Request so it can be included under the Community notebooks. Notebook Description Data processing with Dask Speed up data preprocessing with distributed Dask cluster on Shakudo Platform Data preprocessing with RAPIDS speed up data preprocessing with RAPIDS and GPU Data preprocessing with Spark speed up data preprocessing with PySpark Data preprocessing with Spark3.2.0 speed up data preprocessing with PySpark 3.2.0 pandas like APIs Speed up inference on large data with Dask Advance example on speed inference with Dask by preload large model to Dask workers Simple pipeline job A basic pipeline to automate a jupyter notebook with parameterization Triton model prepration Convert Pytorch Keras sklearn and xgboost model checkpoints for Triton serving Shakudo Platform Triton service App Triton client in a flask APP to be served as a Shakudo Platform service GraphQL within jupyter Submit pipeline jobs using graphql queries within jupyter a notebook Question answering Tensorflow Training with Ray Tensorflow training with distributed hyperparameter tuning on Ray cluster CIFAR Pytorch Training with Ray pytorch training with distributed hyperparameter tuning and MLFlow on Ray cluster Ray lightgbm on 250GB of data Train and hyperparameter optimazation on 250GB of data under 1 min with distributed Ray Datasets and Ray Tune Ray Tune with MLFlow Simple Ray Tune example wth MLFlow Tracking Ray Tune Bayesian Optimization A collection of Ray Tune scheduler examples Streamlit UI An example of building and serving a permanent UI on Shakudo Platform","title":"Example notebooks"},{"location":"example_notebooks.html#example-notebooks","text":"You can find here a list of the official notebooks and scrips provided by Shakudo Platform. Also, we would like to list here interesting content created by the community. If you wrote some notebook(s) leveraging Shakudo Platform and would like be be listed here, please open a Pull Request so it can be included under the Community notebooks. Notebook Description Data processing with Dask Speed up data preprocessing with distributed Dask cluster on Shakudo Platform Data preprocessing with RAPIDS speed up data preprocessing with RAPIDS and GPU Data preprocessing with Spark speed up data preprocessing with PySpark Data preprocessing with Spark3.2.0 speed up data preprocessing with PySpark 3.2.0 pandas like APIs Speed up inference on large data with Dask Advance example on speed inference with Dask by preload large model to Dask workers Simple pipeline job A basic pipeline to automate a jupyter notebook with parameterization Triton model prepration Convert Pytorch Keras sklearn and xgboost model checkpoints for Triton serving Shakudo Platform Triton service App Triton client in a flask APP to be served as a Shakudo Platform service GraphQL within jupyter Submit pipeline jobs using graphql queries within jupyter a notebook Question answering Tensorflow Training with Ray Tensorflow training with distributed hyperparameter tuning on Ray cluster CIFAR Pytorch Training with Ray pytorch training with distributed hyperparameter tuning and MLFlow on Ray cluster Ray lightgbm on 250GB of data Train and hyperparameter optimazation on 250GB of data under 1 min with distributed Ray Datasets and Ray Tune Ray Tune with MLFlow Simple Ray Tune example wth MLFlow Tracking Ray Tune Bayesian Optimization A collection of Ray Tune scheduler examples Streamlit UI An example of building and serving a permanent UI on Shakudo Platform","title":"Example notebooks"},{"location":"examples.html","text":"Examples \u00b6 Pandas to Dask.dataframe \u00b6 Below are some common examples of converting pandas operations to dask-friendly code. Column to_datetime \u00b6 # pandas to_datetime import pandas as pd df1 [ 'date' ] = pd . to_datetime ( df1 [ 'date' ]) # dask to_datetime from dask import dataframe as dd df1 [ 'date' ] = dd . to_datetime ( df1 [ 'date' ]) Dataframe groupby \u00b6 # pandas groupby df_metrics = df_metrics . groupby ( pd . Grouper ( timeframe = '1day' , closed = 'right' , label = 'right' ) ) . agg ({ 'id' : pd . Series . nunique , 'num_entries' : 'sum' , 'total_runs' : 'sum' ) # dask is exactly the same df_metrics = df_metrics . groupby ( pd . Grouper ( timeframe = '1day' , closed = 'right' , label = 'right' ) ) . agg ({ 'id' : pd . Series . nunique , 'num_entries' : 'sum' , 'total_runs' : 'sum' ) Get dummies \u00b6 The following example features a more complicated groupby; get_dummies will explode in dimension in large amount of data (i.e. possibly explode the data into many columns) # pandas version do dummies first then groupby time interval to get aggrgation per time interval dfr = pd . get_dummies ( df ,[ 'col_a' , 'col_b' , 'col_c' ]) dfr = dfr . merge ( df [ 'date' ], right_index = True , left_index = True ) dfr = dfr . sort_values ( by = 'date' ) dfr = dfr . groupby ( pd . Grouper ( timeframe = '1day' , closed = 'right' , label = 'right' )) . sum () # dask version do value counts instead of getting dummies, and do pivot after groupby def agg_func ( df : pd . DataFrame , timeframe : str , ts_col : str , sec_id : str , target_col : str ) -> pd . DataFrame : \"\"\" function that group data by required timeframe for one target column df: dataframe to be aggregated timeframe: aggregation timeframe ts_col: column name of the index sec_id: column name of a secondary id target_col: target column name for processing e.g. col_a \"\"\" df = df . groupby ([ sec_id , ts_col , target_col ]) . size () . reset_index () . set_index ( ts_col ) df . columns = [ sec_id , target_col , 'count' ] df_agg = df . groupby ( pd . Grouper ( timeframe = timeframe , closed = 'right' , label = 'right' )) . apply ( lambda x : x . groupby ( target_col ) . agg ({ 'count' : 'sum' })) . reset_index () return df_agg meta_df = agg_func ( df . head ( 10 ), timeframe , ts_col , sec_id , target_col ) . dtypes . to_dict () df = df . map_partitions ( agg_func , timeframe , ts_col , sec_id , target_col , meta = meta_df ) df . columns = [ ts_col , target_col , 'count' ] # further groupby session_ts and event as there will be duplicates among partition df = df . groupby ([ ts_col , target_col ]) . agg ({ 'count' : 'sum' }) # create pivot table for end results df = df . reset_index () df = df . pivot_table ( values = \"count\" , index = ts_col , columns = target_col ) df . columns = [ target_col + '_' + i for i in list ( df . columns )] Dask map \u00b6 The following example optimizes a function that reads a list of files one by one. ## this snippet reads a list of files one by one import xarray as xr import gcsfs fs = gcsfs . GCSFileSystem ( project = 'myproject' , token = None ) files_list = [ 'file1' , 'file2' , 'file3' , 'file4' ] gcsmap = gcsfs . mapping . GCSMap ( f 'gs://my-bucket/ { files_list [ 0 ] } ' , gcs = fs ) Glob = xr . open_zarr ( store = gcsmap ) . load () ## add other datasets sequentially for filepath in files_list [ 1 :]: gcsmap = gcsfs . mapping . GCSMap ( f 'gs://my-bucket/ { filepath } ' , gcs = fs ) ds = xr . open_zarr ( store = gcsmap ) . load () Glob = xr . merge ([ Glob , ds ], compat = \"no_conflicts\" , combine_attrs = \"no_conflicts\" ) ## in dask, create a function to read one file, then use client.map the function and list of files import xarray as xr import gcsfs fs = gcsfs . GCSFileSystem ( project = 'myproject' , token = None ) files_list = [ 'file1' , 'file2' , 'file3' , 'file4' ] def read_files ( gsfilepath ): gcsmap = gcsfs . mapping . GCSMap ( f \"gs://my-bucket/ { gsfilepath } \" , gcs = fs ) ds = xr . open_zarr ( store = gcsmap ) . load () . persist () return ds dss = client . map ( read_files , files_list ) ds_list = client . gather ( dss ) print ( len ( ds_list )) # output: 4 Glob = xr . merge ( ds_list , compat = \"no_conflicts\" , combine_attrs = \"no_conflicts\" ) Parallel training and preprocessing on dask \u00b6 Sklearn training can be easily converted to distributed training with dask using joblib. import joblib with joblib . parallel_backend ( 'dask' ): grid_search . fit ( X , y ) Many sklearn preprocessing modules (e.g. OneHotEncoder, Categorize, StandardScaler, etc.), models (NaiveBayes, xgboost, clustering, etc.), and model selection utilities (KFold, train_test_split, etc.) have dask equivalents. See https://ml.dask.org/index.html for full list of equivalents.","title":"Dask optimization examples"},{"location":"examples.html#examples","text":"","title":"Examples"},{"location":"examples.html#pandas-to-daskdataframe","text":"Below are some common examples of converting pandas operations to dask-friendly code.","title":"Pandas to Dask.dataframe"},{"location":"examples.html#column-to_datetime","text":"# pandas to_datetime import pandas as pd df1 [ 'date' ] = pd . to_datetime ( df1 [ 'date' ]) # dask to_datetime from dask import dataframe as dd df1 [ 'date' ] = dd . to_datetime ( df1 [ 'date' ])","title":"Column to_datetime"},{"location":"examples.html#dataframe-groupby","text":"# pandas groupby df_metrics = df_metrics . groupby ( pd . Grouper ( timeframe = '1day' , closed = 'right' , label = 'right' ) ) . agg ({ 'id' : pd . Series . nunique , 'num_entries' : 'sum' , 'total_runs' : 'sum' ) # dask is exactly the same df_metrics = df_metrics . groupby ( pd . Grouper ( timeframe = '1day' , closed = 'right' , label = 'right' ) ) . agg ({ 'id' : pd . Series . nunique , 'num_entries' : 'sum' , 'total_runs' : 'sum' )","title":"Dataframe groupby"},{"location":"examples.html#get-dummies","text":"The following example features a more complicated groupby; get_dummies will explode in dimension in large amount of data (i.e. possibly explode the data into many columns) # pandas version do dummies first then groupby time interval to get aggrgation per time interval dfr = pd . get_dummies ( df ,[ 'col_a' , 'col_b' , 'col_c' ]) dfr = dfr . merge ( df [ 'date' ], right_index = True , left_index = True ) dfr = dfr . sort_values ( by = 'date' ) dfr = dfr . groupby ( pd . Grouper ( timeframe = '1day' , closed = 'right' , label = 'right' )) . sum () # dask version do value counts instead of getting dummies, and do pivot after groupby def agg_func ( df : pd . DataFrame , timeframe : str , ts_col : str , sec_id : str , target_col : str ) -> pd . DataFrame : \"\"\" function that group data by required timeframe for one target column df: dataframe to be aggregated timeframe: aggregation timeframe ts_col: column name of the index sec_id: column name of a secondary id target_col: target column name for processing e.g. col_a \"\"\" df = df . groupby ([ sec_id , ts_col , target_col ]) . size () . reset_index () . set_index ( ts_col ) df . columns = [ sec_id , target_col , 'count' ] df_agg = df . groupby ( pd . Grouper ( timeframe = timeframe , closed = 'right' , label = 'right' )) . apply ( lambda x : x . groupby ( target_col ) . agg ({ 'count' : 'sum' })) . reset_index () return df_agg meta_df = agg_func ( df . head ( 10 ), timeframe , ts_col , sec_id , target_col ) . dtypes . to_dict () df = df . map_partitions ( agg_func , timeframe , ts_col , sec_id , target_col , meta = meta_df ) df . columns = [ ts_col , target_col , 'count' ] # further groupby session_ts and event as there will be duplicates among partition df = df . groupby ([ ts_col , target_col ]) . agg ({ 'count' : 'sum' }) # create pivot table for end results df = df . reset_index () df = df . pivot_table ( values = \"count\" , index = ts_col , columns = target_col ) df . columns = [ target_col + '_' + i for i in list ( df . columns )]","title":"Get dummies"},{"location":"examples.html#dask-map","text":"The following example optimizes a function that reads a list of files one by one. ## this snippet reads a list of files one by one import xarray as xr import gcsfs fs = gcsfs . GCSFileSystem ( project = 'myproject' , token = None ) files_list = [ 'file1' , 'file2' , 'file3' , 'file4' ] gcsmap = gcsfs . mapping . GCSMap ( f 'gs://my-bucket/ { files_list [ 0 ] } ' , gcs = fs ) Glob = xr . open_zarr ( store = gcsmap ) . load () ## add other datasets sequentially for filepath in files_list [ 1 :]: gcsmap = gcsfs . mapping . GCSMap ( f 'gs://my-bucket/ { filepath } ' , gcs = fs ) ds = xr . open_zarr ( store = gcsmap ) . load () Glob = xr . merge ([ Glob , ds ], compat = \"no_conflicts\" , combine_attrs = \"no_conflicts\" ) ## in dask, create a function to read one file, then use client.map the function and list of files import xarray as xr import gcsfs fs = gcsfs . GCSFileSystem ( project = 'myproject' , token = None ) files_list = [ 'file1' , 'file2' , 'file3' , 'file4' ] def read_files ( gsfilepath ): gcsmap = gcsfs . mapping . GCSMap ( f \"gs://my-bucket/ { gsfilepath } \" , gcs = fs ) ds = xr . open_zarr ( store = gcsmap ) . load () . persist () return ds dss = client . map ( read_files , files_list ) ds_list = client . gather ( dss ) print ( len ( ds_list )) # output: 4 Glob = xr . merge ( ds_list , compat = \"no_conflicts\" , combine_attrs = \"no_conflicts\" )","title":"Dask map"},{"location":"examples.html#parallel-training-and-preprocessing-on-dask","text":"Sklearn training can be easily converted to distributed training with dask using joblib. import joblib with joblib . parallel_backend ( 'dask' ): grid_search . fit ( X , y ) Many sklearn preprocessing modules (e.g. OneHotEncoder, Categorize, StandardScaler, etc.), models (NaiveBayes, xgboost, clustering, etc.), and model selection utilities (KFold, train_test_split, etc.) have dask equivalents. See https://ml.dask.org/index.html for full list of equivalents.","title":"Parallel training and preprocessing on dask"},{"location":"faq.html","text":"FAQ \u00b6 1. Shakudo Platform dashboard or session is unavailable \u00b6 This is usually caused by being logged out, session timedout, or old cookie and cache in the browser. Solutions \u00b6 Check if you are logged out Hard refresh the page by pressing Shift+Command+R You'll be redirected to the login page if you are logged out Check if your session is timed out Hard refresh the dashboard by pressing Shift+Command+R Check if your session is still alive on the sessions page Check if it's a cache issue Go to the menu of your browser -> Preference -> Privacy and Securities -> Cookies and other site data -> All cookies and site data or search for Cookies and other site data in the Preference page. In the search bar of the Cookies and other site data page , type to search any cookie related to hyperplane.dev . Delete all the cookies related to hyperplane.dev page Go back to yourdomain.hyperplane.dev page and login with your credentials.","title":"FAQ"},{"location":"faq.html#faq","text":"","title":"FAQ"},{"location":"faq.html#1-shakudo-platform-dashboard-or-session-is-unavailable","text":"This is usually caused by being logged out, session timedout, or old cookie and cache in the browser.","title":"1. Shakudo Platform dashboard or session is unavailable"},{"location":"faq.html#solutions","text":"Check if you are logged out Hard refresh the page by pressing Shift+Command+R You'll be redirected to the login page if you are logged out Check if your session is timed out Hard refresh the dashboard by pressing Shift+Command+R Check if your session is still alive on the sessions page Check if it's a cache issue Go to the menu of your browser -> Preference -> Privacy and Securities -> Cookies and other site data -> All cookies and site data or search for Cookies and other site data in the Preference page. In the search bar of the Cookies and other site data page , type to search any cookie related to hyperplane.dev . Delete all the cookies related to hyperplane.dev page Go back to yourdomain.hyperplane.dev page and login with your credentials.","title":"Solutions"},{"location":"frontend_tools.html","text":"Shakudo Platform can host any frontend application and expose a permanant public URL. There is a growing list of fully integrated tools for building and serving frontend applications. Streamlit \u00b6 Streamlit can be used when you need to quickly expose a frontend for your Shakudo Platform service. Below is a sample frontend UI build with streamlit. How to use Steamlit on Shakudo Platform \u00b6 Check out this full example of an image recognition App and follow the instructions on the README. To get started with your own App, replace the dummy function in sent_infer_request_in_cluster in skexample.py with your own inference function. Go to the service tab on the Shakudo Platform dashboard and start a service with the `streamlit_pipeline.yaml'. Choose the service URL prefix for your Shakudo Platform Service, this will be the URL of your frontend. Assume it's my_awesome_frontend . After you hit the create button of the service and the service is up, you can then go to https://yourdomain.hyperplane.dev/my_awesome_frontend to see the frontend. Voila \u00b6 Voil\u00e0 turns Jupyter notebooks into standalone web applications. On Shakudo Platform dashboards image, you can use Voila to serve a notebook or a directory of notebooks. How to use Voila on Hyerplane \u00b6 Checkout these examples of a Voila app To get started with your own app, replace the plotting in the jupyter notebooks. Voila support custom Jupyter widgets such as bqplot . Checkout the list of example voila notebooks and supported widgets To view the Voila dashboard on your Jupyter session, simply click on the Voila icon on your JupyterLab menu To serve the Voila app as a Shakudo Platform Service, go to the service tab on the Shakudo Platform dashboard and start a service with the voila/pipeline.yaml . Choose the service URL prefix for your Shakudo Platform Service, this will be the URL of your frontend. Assume it's my_voila_frontend . After you hit the create button of the service and the service is up, you can then go to https://yourdomain.hyperplane.dev/my_voila_frontend to see the frontend.","title":"Frontend on Shakudo Platform"},{"location":"frontend_tools.html#streamlit","text":"Streamlit can be used when you need to quickly expose a frontend for your Shakudo Platform service. Below is a sample frontend UI build with streamlit.","title":"Streamlit"},{"location":"frontend_tools.html#how-to-use-steamlit-on-shakudo-platform","text":"Check out this full example of an image recognition App and follow the instructions on the README. To get started with your own App, replace the dummy function in sent_infer_request_in_cluster in skexample.py with your own inference function. Go to the service tab on the Shakudo Platform dashboard and start a service with the `streamlit_pipeline.yaml'. Choose the service URL prefix for your Shakudo Platform Service, this will be the URL of your frontend. Assume it's my_awesome_frontend . After you hit the create button of the service and the service is up, you can then go to https://yourdomain.hyperplane.dev/my_awesome_frontend to see the frontend.","title":"How to use Steamlit on Shakudo Platform"},{"location":"frontend_tools.html#voila","text":"Voil\u00e0 turns Jupyter notebooks into standalone web applications. On Shakudo Platform dashboards image, you can use Voila to serve a notebook or a directory of notebooks.","title":"Voila"},{"location":"frontend_tools.html#how-to-use-voila-on-hyerplane","text":"Checkout these examples of a Voila app To get started with your own app, replace the plotting in the jupyter notebooks. Voila support custom Jupyter widgets such as bqplot . Checkout the list of example voila notebooks and supported widgets To view the Voila dashboard on your Jupyter session, simply click on the Voila icon on your JupyterLab menu To serve the Voila app as a Shakudo Platform Service, go to the service tab on the Shakudo Platform dashboard and start a service with the voila/pipeline.yaml . Choose the service URL prefix for your Shakudo Platform Service, this will be the URL of your frontend. Assume it's my_voila_frontend . After you hit the create button of the service and the service is up, you can then go to https://yourdomain.hyperplane.dev/my_voila_frontend to see the frontend.","title":"How to use Voila on Hyerplane"},{"location":"git_image_upload.html","text":"Uploading Output Images to Github \u00b6 We have provided an example script to allow you to upload a picture and embed it in a commit comment on GitHub as part of your pipeline. This can be used, among other things, to post test results in a convenient place where they are easily associated with code changes that produced them. The example script is located at https://github.com/devsentient/ds-29faf59/blob/main/git_comment/image_to_comment.sh Setup - GitHub PAT \u00b6 To use this script you must set up a GitHub Personal Access Token with 'repo' permissions (as that is where it will be saving your image). For a detailed guide see: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token Your GitHub username and the generated token must then be made available via kubernetes secrets. Add your base64 encoded username and token to the appropriate YAML as follows: apiVersion : v1 data : gh_token : base64encodedusername gh_username : base64encodedtoken kind : Secret metadata : labels : app.kubernetes.io/managed-by : pulumi name : custom-deploy-secrets-4wf3iuzp namespace : pipelines-qhxswlgm type : Opaque For the ds pod, this is stored in custom-deploy-secrets-4wf3iuzp . Once the secrets are stored in the kubernetes pod, export them either as files ( gh_username and gh_token in /etc/hyperplane/secrets/ ) or environment variables ( $KBN_SECRET_GHA_USERNAME and $KBN_SECRET_GHA_TOKEN ). Most variables can be easily hard-coded by modifying the script (eg. to reduce parameter clutter), but please do not save the authorization token in the file itself. If running the script fails with a \"message\": \"Not Found\" error, then it is likely that either the GitHub PAT is configured incorrectly or the username/token secrets are not correctly exported. Parameters \u00b6 The following settings control how the image is uploaded. They may configure via job parameters or by modifying the default values within the script. If the option is set both within the script defaults and via parameters, the parameters will take priority. gh_owner - Name of the target GitHub repository's owner. gh_repo - Name of the target GitHub repository. gh_commit_sha - Hash ID of the commit on which to make the comment. png_path - Path to the image file to be uploaded. png_description - HTML alt-text to be embedded in the image when posted. Optional - if not included the alt text will be blank. png_upload_dir - This string will be used as a target directory into which the image will be uploaded. The directory does not need to already exist. Set to '' to upload to the repo root directory. Do not include a leading / character. Optional - default is 'graph_output_dir' . png_add_timestamp - Set to 1 to append a timestamp of the image creation time to the image name before upload, or 0 to leave as is. Enable this if your image generating function uses the same (or repeating) filenames. Timestamp granularity is in seconds, and will need to be changed if you are generating images faster than that. Optional - default is 1 . In a pipeline \u00b6 To use this script in a pipeline, add it to the end of your YAML file and enter the relevant job parameters Example pipeline: \u00b6 pipeline: name: \"Image upload test pipe\" tasks: - name: \"Other steps that save images to disk\" ... ... - name: \"Upload result image\" type: \"bash script\" bash_script_path: \"git_comment/image_to_comment.sh\" Command line invocation \u00b6 Parameters may be passed to this as command line arguments, which will override the above Shakudo Platform job parameters. The secrets $KBN_SECRET_GHA_USERNAME and $KBN_SECRET_GHA_TOKEN must still exist. Syntax: image_to_comment.sh gh_owner gh_repo gh_commit_sha png_path png_description This is necessary if you wish to upload multiple images per job. Supply the command line arguments in your YAML file as follows: - name: \"Upload first result image\" type: \"bash script\" bash_script_path: \"git_comment/image_to_comment.sh gh_owner_1 gh_repo_1 gh_commit_sha_1 png_path_1 png_description_1\" - name: \"Upload second result image\" type: \"bash script\" bash_script_path: \"git_comment/image_to_comment.sh gh_owner_2 gh_repo_2 gh_commit_sha_2 png_path_2 png_description_2\" It is possible to use $HYPERPLANE_JOB_PARAMETER_PARAMNAME variables in place of any or all command line options, if wish to use the dashboard for configuration rather than setting those options in your YAML file. Caution \u00b6 This script does not perform input sanitation and when invoked with command line arguments should not be passed raw user-supplied data from any public source. There shouldn't be a risk when using it as intended, though - if unsanitized data is being passed directly to the command line then there are much bigger problems happening elsewhere in the pipeline.","title":"Uploading Output Images to Github"},{"location":"git_image_upload.html#uploading-output-images-to-github","text":"We have provided an example script to allow you to upload a picture and embed it in a commit comment on GitHub as part of your pipeline. This can be used, among other things, to post test results in a convenient place where they are easily associated with code changes that produced them. The example script is located at https://github.com/devsentient/ds-29faf59/blob/main/git_comment/image_to_comment.sh","title":"Uploading Output Images to Github"},{"location":"git_image_upload.html#setup-github-pat","text":"To use this script you must set up a GitHub Personal Access Token with 'repo' permissions (as that is where it will be saving your image). For a detailed guide see: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token Your GitHub username and the generated token must then be made available via kubernetes secrets. Add your base64 encoded username and token to the appropriate YAML as follows: apiVersion : v1 data : gh_token : base64encodedusername gh_username : base64encodedtoken kind : Secret metadata : labels : app.kubernetes.io/managed-by : pulumi name : custom-deploy-secrets-4wf3iuzp namespace : pipelines-qhxswlgm type : Opaque For the ds pod, this is stored in custom-deploy-secrets-4wf3iuzp . Once the secrets are stored in the kubernetes pod, export them either as files ( gh_username and gh_token in /etc/hyperplane/secrets/ ) or environment variables ( $KBN_SECRET_GHA_USERNAME and $KBN_SECRET_GHA_TOKEN ). Most variables can be easily hard-coded by modifying the script (eg. to reduce parameter clutter), but please do not save the authorization token in the file itself. If running the script fails with a \"message\": \"Not Found\" error, then it is likely that either the GitHub PAT is configured incorrectly or the username/token secrets are not correctly exported.","title":"Setup - GitHub PAT"},{"location":"git_image_upload.html#parameters","text":"The following settings control how the image is uploaded. They may configure via job parameters or by modifying the default values within the script. If the option is set both within the script defaults and via parameters, the parameters will take priority. gh_owner - Name of the target GitHub repository's owner. gh_repo - Name of the target GitHub repository. gh_commit_sha - Hash ID of the commit on which to make the comment. png_path - Path to the image file to be uploaded. png_description - HTML alt-text to be embedded in the image when posted. Optional - if not included the alt text will be blank. png_upload_dir - This string will be used as a target directory into which the image will be uploaded. The directory does not need to already exist. Set to '' to upload to the repo root directory. Do not include a leading / character. Optional - default is 'graph_output_dir' . png_add_timestamp - Set to 1 to append a timestamp of the image creation time to the image name before upload, or 0 to leave as is. Enable this if your image generating function uses the same (or repeating) filenames. Timestamp granularity is in seconds, and will need to be changed if you are generating images faster than that. Optional - default is 1 .","title":"Parameters"},{"location":"git_image_upload.html#in-a-pipeline","text":"To use this script in a pipeline, add it to the end of your YAML file and enter the relevant job parameters","title":"In a pipeline"},{"location":"git_image_upload.html#example-pipeline","text":"pipeline: name: \"Image upload test pipe\" tasks: - name: \"Other steps that save images to disk\" ... ... - name: \"Upload result image\" type: \"bash script\" bash_script_path: \"git_comment/image_to_comment.sh\"","title":"Example pipeline:"},{"location":"git_image_upload.html#command-line-invocation","text":"Parameters may be passed to this as command line arguments, which will override the above Shakudo Platform job parameters. The secrets $KBN_SECRET_GHA_USERNAME and $KBN_SECRET_GHA_TOKEN must still exist. Syntax: image_to_comment.sh gh_owner gh_repo gh_commit_sha png_path png_description This is necessary if you wish to upload multiple images per job. Supply the command line arguments in your YAML file as follows: - name: \"Upload first result image\" type: \"bash script\" bash_script_path: \"git_comment/image_to_comment.sh gh_owner_1 gh_repo_1 gh_commit_sha_1 png_path_1 png_description_1\" - name: \"Upload second result image\" type: \"bash script\" bash_script_path: \"git_comment/image_to_comment.sh gh_owner_2 gh_repo_2 gh_commit_sha_2 png_path_2 png_description_2\" It is possible to use $HYPERPLANE_JOB_PARAMETER_PARAMNAME variables in place of any or all command line options, if wish to use the dashboard for configuration rather than setting those options in your YAML file.","title":"Command line invocation"},{"location":"git_image_upload.html#caution","text":"This script does not perform input sanitation and when invoked with command line arguments should not be passed raw user-supplied data from any public source. There shouldn't be a risk when using it as intended, though - if unsanitized data is being passed directly to the command line then there are much bigger problems happening elsewhere in the pipeline.","title":"Caution"},{"location":"graphql_queries.html","text":"Graphql Queries Examples \u00b6 Below are some common graphql queries for submission, checking status, etc. Submit a pipeline job \u00b6 mu tat io n submi t Job { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , t imeou t : 1500 , ac t iveTimeou t : 1000 , maxRe tr ies : 5 , pipeli ne YamlPa t h : \"example_pipeline.yaml\" , parame ters : { crea te : [ { key : \"a_param\" , value : \"pass parameters as strings\" }, ] } } ) { id ru n Id } } Cancel a job \u00b6 mu tat io n ca n celJob { upda te Pipeli ne Job( where : { id : \"your-job-id\" } da ta : { s tatus : { se t : \"cancelled\" } } ) { id } } Cancel all scheduled jobs \u00b6 mu tat io n ca n celScheduledJob { upda te Ma n yPipeli ne Job( where : { s tatus : { equals : \"scheduled\" } } da ta : { s tatus : { se t : \"cancelled\" } } ) { cou nt } } Check the status of a job \u00b6 query checkS tatus { pipeli ne Job (where : { id : \"your-job-id\" } ) { s tatus } }","title":"Graphql query examples"},{"location":"graphql_queries.html#graphql-queries-examples","text":"Below are some common graphql queries for submission, checking status, etc.","title":"Graphql Queries Examples"},{"location":"graphql_queries.html#submit-a-pipeline-job","text":"mu tat io n submi t Job { crea te Pipeli ne Job (da ta : { jobType : \"basic\" , t imeou t : 1500 , ac t iveTimeou t : 1000 , maxRe tr ies : 5 , pipeli ne YamlPa t h : \"example_pipeline.yaml\" , parame ters : { crea te : [ { key : \"a_param\" , value : \"pass parameters as strings\" }, ] } } ) { id ru n Id } }","title":"Submit a pipeline job"},{"location":"graphql_queries.html#cancel-a-job","text":"mu tat io n ca n celJob { upda te Pipeli ne Job( where : { id : \"your-job-id\" } da ta : { s tatus : { se t : \"cancelled\" } } ) { id } }","title":"Cancel a job"},{"location":"graphql_queries.html#cancel-all-scheduled-jobs","text":"mu tat io n ca n celScheduledJob { upda te Ma n yPipeli ne Job( where : { s tatus : { equals : \"scheduled\" } } da ta : { s tatus : { se t : \"cancelled\" } } ) { cou nt } }","title":"Cancel all scheduled jobs"},{"location":"graphql_queries.html#check-the-status-of-a-job","text":"query checkS tatus { pipeli ne Job (where : { id : \"your-job-id\" } ) { s tatus } }","title":"Check the status of a job"},{"location":"hyperplane_gql_queries.html","text":"Queries \u00b6 Sessions \u00b6 Get Sessions Cancel a session by session_id Count sessions Jobs \u00b6 Creat a pipeline job Cancel a pipeline job Get a list of jobs Get job status Get scheduled jobs status Get job parameters Detele job parameters Update job parameters Services \u00b6 Get a list of services Get Sessions \u00b6 Get a list of Sessions. @params limit: number of rows to pull @params email: hyperplaneEmail of ther user @params status: status of the Sessions query hyperhubSessions($limit: Int!, $email: String, $status: String, $imageType: String) { hyperHubSessions(orderBy:{startTime: desc}, take: $limit, where: { hyperplaneUserEmail: {equals: $email}, status: {equals: $status}, }) { id hyperplaneUserEmail status imageType jLabUrl notebookURI estimatedCost department resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime } countHyperHubSessions } Cancel a session by session_id \u00b6 Cancel a Session @params id: id of Session to cancel mutation($id: String!) { updateHyperHubSession(where: {id: $id}, data: { status: {set: \"cancelled\"} }) { id status } } Count sessions \u00b6 Count the number of sessions based on the filters provided by the parameters @params email: hyperplaneUserEmail - e.g. show sessions only for user user@example.com @params imageType: Sessions image type e.g. \"basic\" query hyperhubSessions($email: String, $imageType: String, $status: String) { hyperHubSessions(where: { hyperplaneUserEmail: {equals: $email}, imageType: {equals: $imageType} status: {equals: $status} }) { id } } Creat a pipeline job \u00b6 Create a pipeline job with the specified inputs @params parameters: Type ParameterCreateNestedManyWithoutPipelineJobInput format is specified below: ParameterCreateNestedManyWithoutPipelineJobInput = {create: [{key: \"some_key\", value: \"some_value\"}, {key: \"some_key_2\", value: \"some_value_2\"}]} mutation ($type: String!, $timeout: Int!, $activeTimeout: Int, $maxRetries: Int!, $yamlPath: String!, $exposedPort: String, $schedule: String, $parameters: ParameterCreateNestedManyWithoutPipelineJobInput ) { createPipelineJob (data: { jobType: $type, timeout: $timeout, activeTimeout: $activeTimeout maxRetries: $maxRetries, pipelineYamlPath: $yamlPath, exposedPort: $exposedPort, parameters: $parameters, schedule: $schedule }) { id pinned pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } } Cancel a pipeline job \u00b6 Cancel a job (Stop job from running) @params id: id of the pipeline job mutation ($id: String!) { updatePipelineJob(where: {id: $id}, data: { status: {set: \"cancelled\"} }) { id } } Get a list of jobs \u00b6 Show the list of jobs @params take: the number of rows to show query nextJobs($limit: Int!) { pipelineJobs(orderBy: [{pinned: desc},{ startTime: desc}], take: $limit) { id pipelineYamlPath schedule status output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries jobType estimatedCost owner department } } Get job status \u00b6 Count the number of jobs based on their statuses. For example, failed, pending, or cancelled jobs. The timeFrame parameter specifies the timeframe which will be considered. For instance: * T_10M = past 10 minutes * T_24H = past 24 hours query { COUNT_ALL_TOTAL: getJobStat(stat: COUNT_ALL, timeFrame: TOTAL) COUNT_CANCELLED_TOTAL: getJobStat(stat: COUNT_CANCELLED, timeFrame: TOTAL) COUNT_DONE_TOTAL: getJobStat(stat: COUNT_DONE, timeFrame: TOTAL) COUNT_FAILED_TOTAL: getJobStat(stat: COUNT_FAILED, timeFrame: TOTAL) COUNT_IN_PROGRESS_TOTAL: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: TOTAL ) COUNT_PENDING_TOTAL: getJobStat(stat: COUNT_PENDING, timeFrame: TOTAL) COUNT_SCHEDULED_TOTAL: getJobStat(stat: COUNT_SCHEDULED, timeFrame: TOTAL) COUNT_TIMED_OUT_TOTAL: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: TOTAL) COUNT_ALL_T_10M: getJobStat(stat: COUNT_ALL, timeFrame: T_10M) COUNT_CANCELLED_T_10M: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_10M) COUNT_DONE_T_10M: getJobStat(stat: COUNT_DONE, timeFrame: T_10M) COUNT_FAILED_T_10M: getJobStat(stat: COUNT_FAILED, timeFrame: T_10M) COUNT_IN_PROGRESS_T_10M: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_10M ) COUNT_PENDING_T_10M: getJobStat(stat: COUNT_PENDING, timeFrame: T_10M) COUNT_SCHEDULED_T_10M: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_10M) COUNT_TIMED_OUT_T_10M: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_10M) COUNT_ALL_T_1H: getJobStat(stat: COUNT_ALL, timeFrame: T_1H) COUNT_CANCELLED_T_1H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_1H) COUNT_DONE_T_1H: getJobStat(stat: COUNT_DONE, timeFrame: T_1H) COUNT_FAILED_T_1H: getJobStat(stat: COUNT_FAILED, timeFrame: T_1H) COUNT_IN_PROGRESS_T_1H: getJobStat(stat: COUNT_IN_PROGRESS, timeFrame: T_1H) COUNT_PENDING_T_1H: getJobStat(stat: COUNT_PENDING, timeFrame: T_1H) COUNT_SCHEDULED_T_1H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_1H) COUNT_TIMED_OUT_T_1H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_1H) COUNT_ALL_T_24H: getJobStat(stat: COUNT_ALL, timeFrame: T_24H) COUNT_CANCELLED_T_24H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_24H) COUNT_DONE_T_24H: getJobStat(stat: COUNT_DONE, timeFrame: T_24H) COUNT_FAILED_T_24H: getJobStat(stat: COUNT_FAILED, timeFrame: T_24H) COUNT_IN_PROGRESS_T_24H: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_24H ) COUNT_PENDING_T_24H: getJobStat(stat: COUNT_PENDING, timeFrame: T_24H) COUNT_SCHEDULED_T_24H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_24H) COUNT_TIMED_OUT_T_24H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_24H) } Get scheduled jobs status \u00b6 Count the number of scheduled jobs based on their statuses. For example, failed, pending, or cancelled jobs. The differnece with the query above is that the belew parameter is added to only count scheduled jobs for each time frame with status: SCHEDULED query { COUNT_ALL_TOTAL: getJobStat(stat: COUNT_ALL, timeFrame: TOTAL, status: SCHEDULED) COUNT_CANCELLED_TOTAL: getJobStat(stat: COUNT_CANCELLED, timeFrame: TOTAL, status: SCHEDULED) COUNT_DONE_TOTAL: getJobStat(stat: COUNT_DONE, timeFrame: TOTAL, status: SCHEDULED) COUNT_FAILED_TOTAL: getJobStat(stat: COUNT_FAILED, timeFrame: TOTAL, status: SCHEDULED) COUNT_IN_PROGRESS_TOTAL: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: TOTAL ) COUNT_PENDING_TOTAL: getJobStat(stat: COUNT_PENDING, timeFrame: TOTAL, status: SCHEDULED) COUNT_SCHEDULED_TOTAL: getJobStat(stat: COUNT_SCHEDULED, timeFrame: TOTAL, status: SCHEDULED) COUNT_TIMED_OUT_TOTAL: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: TOTAL, status: SCHEDULED) COUNT_ALL_T_10M: getJobStat(stat: COUNT_ALL, timeFrame: T_10M) COUNT_CANCELLED_T_10M: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_10M, status: SCHEDULED) COUNT_DONE_T_10M: getJobStat(stat: COUNT_DONE, timeFrame: T_10M, status: SCHEDULED) COUNT_FAILED_T_10M: getJobStat(stat: COUNT_FAILED, timeFrame: T_10M, status: SCHEDULED) COUNT_IN_PROGRESS_T_10M: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_10M ) COUNT_PENDING_T_10M: getJobStat(stat: COUNT_PENDING, timeFrame: T_10M, status: SCHEDULED) COUNT_SCHEDULED_T_10M: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_10M, status: SCHEDULED) COUNT_TIMED_OUT_T_10M: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_10M, status: SCHEDULED) COUNT_ALL_T_1H: getJobStat(stat: COUNT_ALL, timeFrame: T_1H, status: SCHEDULED) COUNT_CANCELLED_T_1H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_1H, status: SCHEDULED) COUNT_DONE_T_1H: getJobStat(stat: COUNT_DONE, timeFrame: T_1H, status: SCHEDULED) COUNT_FAILED_T_1H: getJobStat(stat: COUNT_FAILED, timeFrame: T_1H, status: SCHEDULED) COUNT_IN_PROGRESS_T_1H: getJobStat(stat: COUNT_IN_PROGRESS, timeFrame: T_1H, status: SCHEDULED) COUNT_PENDING_T_1H: getJobStat(stat: COUNT_PENDING, timeFrame: T_1H, status: SCHEDULED) COUNT_SCHEDULED_T_1H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_1H, status: SCHEDULED) COUNT_TIMED_OUT_T_1H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_1H, status: SCHEDULED) COUNT_ALL_T_24H: getJobStat(stat: COUNT_ALL, timeFrame: T_24H) COUNT_CANCELLED_T_24H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_24H, status: SCHEDULED) COUNT_DONE_T_24H: getJobStat(stat: COUNT_DONE, timeFrame: T_24H, status: SCHEDULED) COUNT_FAILED_T_24H: getJobStat(stat: COUNT_FAILED, timeFrame: T_24H, status: SCHEDULED) COUNT_IN_PROGRESS_T_24H: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_24H , status: SCHEDULED ) COUNT_PENDING_T_24H: getJobStat(stat: COUNT_PENDING, timeFrame: T_24H, status: SCHEDULED) COUNT_SCHEDULED_T_24H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_24H, status: SCHEDULED) COUNT_TIMED_OUT_T_24H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_24H, status: SCHEDULED) } Get a list of services \u00b6 Get list of services - services are pipeline jobs which have an active timeout and timeout of -1 (Never ending jobs) query nextJobs($offset: Int, $limit: Int!, $status: String!) { pipelineJobs(orderBy: [{pinned: desc},{ startTime: desc}], take: $limit, skip: $offset, where: { AND: [ {activeTimeout: {equals: -1}}, {timeout: {equals: -1}}, {status: {equals: $status}} ] }) { id exposedPort pinned pipelineYamlPath schedule status statusReason startTime completionTime daskDashboardUrl timeout output outputNotebooksPath activeTimeout duration jobType schedule estimatedCost owner department maxRetries } } Get job parameters \u00b6 Get the list of parameters for a pipeline job @params id: The if of the pipeline job query nextJobs($id: String!) { pipelineJobs(where: {id: {equals: $id}}) { parameters { key value id pipelineJobId } } } Detele job parameters \u00b6 Delete a parameter for a pipeline job @params id: The if of the pipeline job mutation($jobId: String!, $parameterId: String!) { updatePipelineJob(where: {id: $jobId}, data: { parameters: {disconnect: {id: $parameterId}} }) { id } } Update job parameters \u00b6 Update the parameter key and value @params id: parameter id mutation ($parameterId: String!, $keyValue: String, $valueValue: String) { updateParameter(where: {id: $parameterId}, data: { key: {set: $keyValue} value: {set: $valueValue} }) { id } }","title":"Advanced Graphql queries"},{"location":"hyperplane_gql_queries.html#queries","text":"","title":"Queries"},{"location":"hyperplane_gql_queries.html#sessions","text":"Get Sessions Cancel a session by session_id Count sessions","title":"Sessions"},{"location":"hyperplane_gql_queries.html#jobs","text":"Creat a pipeline job Cancel a pipeline job Get a list of jobs Get job status Get scheduled jobs status Get job parameters Detele job parameters Update job parameters","title":"Jobs"},{"location":"hyperplane_gql_queries.html#services","text":"Get a list of services","title":"Services"},{"location":"hyperplane_gql_queries.html#get-sessions","text":"Get a list of Sessions. @params limit: number of rows to pull @params email: hyperplaneEmail of ther user @params status: status of the Sessions query hyperhubSessions($limit: Int!, $email: String, $status: String, $imageType: String) { hyperHubSessions(orderBy:{startTime: desc}, take: $limit, where: { hyperplaneUserEmail: {equals: $email}, status: {equals: $status}, }) { id hyperplaneUserEmail status imageType jLabUrl notebookURI estimatedCost department resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime } countHyperHubSessions }","title":"Get Sessions"},{"location":"hyperplane_gql_queries.html#cancel-a-session-by-session_id","text":"Cancel a Session @params id: id of Session to cancel mutation($id: String!) { updateHyperHubSession(where: {id: $id}, data: { status: {set: \"cancelled\"} }) { id status } }","title":"Cancel a session by session_id"},{"location":"hyperplane_gql_queries.html#count-sessions","text":"Count the number of sessions based on the filters provided by the parameters @params email: hyperplaneUserEmail - e.g. show sessions only for user user@example.com @params imageType: Sessions image type e.g. \"basic\" query hyperhubSessions($email: String, $imageType: String, $status: String) { hyperHubSessions(where: { hyperplaneUserEmail: {equals: $email}, imageType: {equals: $imageType} status: {equals: $status} }) { id } }","title":"Count sessions"},{"location":"hyperplane_gql_queries.html#creat-a-pipeline-job","text":"Create a pipeline job with the specified inputs @params parameters: Type ParameterCreateNestedManyWithoutPipelineJobInput format is specified below: ParameterCreateNestedManyWithoutPipelineJobInput = {create: [{key: \"some_key\", value: \"some_value\"}, {key: \"some_key_2\", value: \"some_value_2\"}]} mutation ($type: String!, $timeout: Int!, $activeTimeout: Int, $maxRetries: Int!, $yamlPath: String!, $exposedPort: String, $schedule: String, $parameters: ParameterCreateNestedManyWithoutPipelineJobInput ) { createPipelineJob (data: { jobType: $type, timeout: $timeout, activeTimeout: $activeTimeout maxRetries: $maxRetries, pipelineYamlPath: $yamlPath, exposedPort: $exposedPort, parameters: $parameters, schedule: $schedule }) { id pinned pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }","title":"Creat a pipeline job"},{"location":"hyperplane_gql_queries.html#cancel-a-pipeline-job","text":"Cancel a job (Stop job from running) @params id: id of the pipeline job mutation ($id: String!) { updatePipelineJob(where: {id: $id}, data: { status: {set: \"cancelled\"} }) { id } }","title":"Cancel a pipeline job"},{"location":"hyperplane_gql_queries.html#get-a-list-of-jobs","text":"Show the list of jobs @params take: the number of rows to show query nextJobs($limit: Int!) { pipelineJobs(orderBy: [{pinned: desc},{ startTime: desc}], take: $limit) { id pipelineYamlPath schedule status output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries jobType estimatedCost owner department } }","title":"Get a list of jobs"},{"location":"hyperplane_gql_queries.html#get-job-status","text":"Count the number of jobs based on their statuses. For example, failed, pending, or cancelled jobs. The timeFrame parameter specifies the timeframe which will be considered. For instance: * T_10M = past 10 minutes * T_24H = past 24 hours query { COUNT_ALL_TOTAL: getJobStat(stat: COUNT_ALL, timeFrame: TOTAL) COUNT_CANCELLED_TOTAL: getJobStat(stat: COUNT_CANCELLED, timeFrame: TOTAL) COUNT_DONE_TOTAL: getJobStat(stat: COUNT_DONE, timeFrame: TOTAL) COUNT_FAILED_TOTAL: getJobStat(stat: COUNT_FAILED, timeFrame: TOTAL) COUNT_IN_PROGRESS_TOTAL: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: TOTAL ) COUNT_PENDING_TOTAL: getJobStat(stat: COUNT_PENDING, timeFrame: TOTAL) COUNT_SCHEDULED_TOTAL: getJobStat(stat: COUNT_SCHEDULED, timeFrame: TOTAL) COUNT_TIMED_OUT_TOTAL: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: TOTAL) COUNT_ALL_T_10M: getJobStat(stat: COUNT_ALL, timeFrame: T_10M) COUNT_CANCELLED_T_10M: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_10M) COUNT_DONE_T_10M: getJobStat(stat: COUNT_DONE, timeFrame: T_10M) COUNT_FAILED_T_10M: getJobStat(stat: COUNT_FAILED, timeFrame: T_10M) COUNT_IN_PROGRESS_T_10M: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_10M ) COUNT_PENDING_T_10M: getJobStat(stat: COUNT_PENDING, timeFrame: T_10M) COUNT_SCHEDULED_T_10M: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_10M) COUNT_TIMED_OUT_T_10M: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_10M) COUNT_ALL_T_1H: getJobStat(stat: COUNT_ALL, timeFrame: T_1H) COUNT_CANCELLED_T_1H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_1H) COUNT_DONE_T_1H: getJobStat(stat: COUNT_DONE, timeFrame: T_1H) COUNT_FAILED_T_1H: getJobStat(stat: COUNT_FAILED, timeFrame: T_1H) COUNT_IN_PROGRESS_T_1H: getJobStat(stat: COUNT_IN_PROGRESS, timeFrame: T_1H) COUNT_PENDING_T_1H: getJobStat(stat: COUNT_PENDING, timeFrame: T_1H) COUNT_SCHEDULED_T_1H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_1H) COUNT_TIMED_OUT_T_1H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_1H) COUNT_ALL_T_24H: getJobStat(stat: COUNT_ALL, timeFrame: T_24H) COUNT_CANCELLED_T_24H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_24H) COUNT_DONE_T_24H: getJobStat(stat: COUNT_DONE, timeFrame: T_24H) COUNT_FAILED_T_24H: getJobStat(stat: COUNT_FAILED, timeFrame: T_24H) COUNT_IN_PROGRESS_T_24H: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_24H ) COUNT_PENDING_T_24H: getJobStat(stat: COUNT_PENDING, timeFrame: T_24H) COUNT_SCHEDULED_T_24H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_24H) COUNT_TIMED_OUT_T_24H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_24H) }","title":"Get job status"},{"location":"hyperplane_gql_queries.html#get-scheduled-jobs-status","text":"Count the number of scheduled jobs based on their statuses. For example, failed, pending, or cancelled jobs. The differnece with the query above is that the belew parameter is added to only count scheduled jobs for each time frame with status: SCHEDULED query { COUNT_ALL_TOTAL: getJobStat(stat: COUNT_ALL, timeFrame: TOTAL, status: SCHEDULED) COUNT_CANCELLED_TOTAL: getJobStat(stat: COUNT_CANCELLED, timeFrame: TOTAL, status: SCHEDULED) COUNT_DONE_TOTAL: getJobStat(stat: COUNT_DONE, timeFrame: TOTAL, status: SCHEDULED) COUNT_FAILED_TOTAL: getJobStat(stat: COUNT_FAILED, timeFrame: TOTAL, status: SCHEDULED) COUNT_IN_PROGRESS_TOTAL: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: TOTAL ) COUNT_PENDING_TOTAL: getJobStat(stat: COUNT_PENDING, timeFrame: TOTAL, status: SCHEDULED) COUNT_SCHEDULED_TOTAL: getJobStat(stat: COUNT_SCHEDULED, timeFrame: TOTAL, status: SCHEDULED) COUNT_TIMED_OUT_TOTAL: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: TOTAL, status: SCHEDULED) COUNT_ALL_T_10M: getJobStat(stat: COUNT_ALL, timeFrame: T_10M) COUNT_CANCELLED_T_10M: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_10M, status: SCHEDULED) COUNT_DONE_T_10M: getJobStat(stat: COUNT_DONE, timeFrame: T_10M, status: SCHEDULED) COUNT_FAILED_T_10M: getJobStat(stat: COUNT_FAILED, timeFrame: T_10M, status: SCHEDULED) COUNT_IN_PROGRESS_T_10M: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_10M ) COUNT_PENDING_T_10M: getJobStat(stat: COUNT_PENDING, timeFrame: T_10M, status: SCHEDULED) COUNT_SCHEDULED_T_10M: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_10M, status: SCHEDULED) COUNT_TIMED_OUT_T_10M: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_10M, status: SCHEDULED) COUNT_ALL_T_1H: getJobStat(stat: COUNT_ALL, timeFrame: T_1H, status: SCHEDULED) COUNT_CANCELLED_T_1H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_1H, status: SCHEDULED) COUNT_DONE_T_1H: getJobStat(stat: COUNT_DONE, timeFrame: T_1H, status: SCHEDULED) COUNT_FAILED_T_1H: getJobStat(stat: COUNT_FAILED, timeFrame: T_1H, status: SCHEDULED) COUNT_IN_PROGRESS_T_1H: getJobStat(stat: COUNT_IN_PROGRESS, timeFrame: T_1H, status: SCHEDULED) COUNT_PENDING_T_1H: getJobStat(stat: COUNT_PENDING, timeFrame: T_1H, status: SCHEDULED) COUNT_SCHEDULED_T_1H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_1H, status: SCHEDULED) COUNT_TIMED_OUT_T_1H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_1H, status: SCHEDULED) COUNT_ALL_T_24H: getJobStat(stat: COUNT_ALL, timeFrame: T_24H) COUNT_CANCELLED_T_24H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_24H, status: SCHEDULED) COUNT_DONE_T_24H: getJobStat(stat: COUNT_DONE, timeFrame: T_24H, status: SCHEDULED) COUNT_FAILED_T_24H: getJobStat(stat: COUNT_FAILED, timeFrame: T_24H, status: SCHEDULED) COUNT_IN_PROGRESS_T_24H: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_24H , status: SCHEDULED ) COUNT_PENDING_T_24H: getJobStat(stat: COUNT_PENDING, timeFrame: T_24H, status: SCHEDULED) COUNT_SCHEDULED_T_24H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_24H, status: SCHEDULED) COUNT_TIMED_OUT_T_24H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_24H, status: SCHEDULED) }","title":"Get scheduled jobs status"},{"location":"hyperplane_gql_queries.html#get-a-list-of-services","text":"Get list of services - services are pipeline jobs which have an active timeout and timeout of -1 (Never ending jobs) query nextJobs($offset: Int, $limit: Int!, $status: String!) { pipelineJobs(orderBy: [{pinned: desc},{ startTime: desc}], take: $limit, skip: $offset, where: { AND: [ {activeTimeout: {equals: -1}}, {timeout: {equals: -1}}, {status: {equals: $status}} ] }) { id exposedPort pinned pipelineYamlPath schedule status statusReason startTime completionTime daskDashboardUrl timeout output outputNotebooksPath activeTimeout duration jobType schedule estimatedCost owner department maxRetries } }","title":"Get a list of services"},{"location":"hyperplane_gql_queries.html#get-job-parameters","text":"Get the list of parameters for a pipeline job @params id: The if of the pipeline job query nextJobs($id: String!) { pipelineJobs(where: {id: {equals: $id}}) { parameters { key value id pipelineJobId } } }","title":"Get job parameters"},{"location":"hyperplane_gql_queries.html#detele-job-parameters","text":"Delete a parameter for a pipeline job @params id: The if of the pipeline job mutation($jobId: String!, $parameterId: String!) { updatePipelineJob(where: {id: $jobId}, data: { parameters: {disconnect: {id: $parameterId}} }) { id } }","title":"Detele job parameters"},{"location":"hyperplane_gql_queries.html#update-job-parameters","text":"Update the parameter key and value @params id: parameter id mutation ($parameterId: String!, $keyValue: String, $valueValue: String) { updateParameter(where: {id: $parameterId}, data: { key: {set: $keyValue} value: {set: $valueValue} }) { id } }","title":"Update job parameters"},{"location":"hyperplane_tools.html","text":"Shakudo Platform tools \u00b6 Shakudo Platform offers tools to improve your pipelining. Create slack alerts \u00b6 Whether you are running scheduled pipelines or one-time jobs, posting slack alerts can be done simply with the following: Using incoming webhooks for a specific channel \u00b6 Ensure you have provided an environment variable for your webhook. This may already be set up for you. import os os . environ [ \"WEBHOOK_URL\" ] = \"https://hooks.slack.com/services/TXXXXXXX/BXXXXXXXXX/\" Post a message from hyperplane import notebook_common as nc nc . post_message ( \"Testing an incoming webhook\" , channel = \"#your-public-channel\" ) Post a dict from hyperplane import notebook_common as nc nc . post_table ({ \"a\" : \"a value\" , \"header b\" : \"some value\" }, channel = \"#your-public-channel\" ) Using slack token \u00b6 Ensure you have specified your slack token. This may already be set up for you. import os os . environ [ \"SLACK_TOKEN\" ] = \"your-slack-t0ken\" Post a message from hyperplane import notebook_common as nc sh = nc . SlackHelper () sh . post_message ( 'Testing' , '#some-channel' ) Post a dict from hyperplane import notebook_common as nc sh = nc . SlackHelper () sh . post_table ({ \"a\" : \"a value\" , \"header b\" : \"some value\" }, '#some-channel' ) Graphql queries in jupyter notebooks \u00b6 Submit a job from the notebook \u00b6 You can paste the following snippet in a cell to run a pipeline from your jupyter notebook newjob = await nc . submit_pipeline_job ( 'hello.yaml' , parameters = { \"a\" : 1 , \"b\" : 1 }) newjob returns {'id': 'd6f2f4a3-aef7-477e-bc0b-f3a27b0ac0cb', 'runId': None} Check on a job status in your notebook \u00b6 results = [ \"a-job-id\" , \"another-job-id\" ] res = await nc . checkjobs ( results , loop = True ) print ( res ) loop=True will refresh the output every 5 seconds. ### Jobs summary 0 / 2 in progress 0 / 2 pending 2 / 2 processed 2 done | 0 timed out | 0 cancelled | 0 failed Progress: 100.0% ### Dask dashboards a-job-id done None another-job-id done None Cancel a job from the notebook \u00b6 You can cancel a job by passing the id of your job. await nc . cancel_pipeline_job ( 'some-id' ) returns {'id': 'some-id', 'status': 'cancelled} Persistence for scheduling \u00b6 When you have scheduled jobs, you may want to store values from previous runs. To set a persistent value, use the following snippet: nc . set_current_job_output ( \"message in str\" ) To retrieve values from previous runs, use the following code: nc . get_previous_job_runs ( n = time_segments )","title":"Shakudo Platform tools"},{"location":"hyperplane_tools.html#shakudo-platform-tools","text":"Shakudo Platform offers tools to improve your pipelining.","title":"Shakudo Platform tools"},{"location":"hyperplane_tools.html#create-slack-alerts","text":"Whether you are running scheduled pipelines or one-time jobs, posting slack alerts can be done simply with the following:","title":"Create slack alerts"},{"location":"hyperplane_tools.html#using-incoming-webhooks-for-a-specific-channel","text":"Ensure you have provided an environment variable for your webhook. This may already be set up for you. import os os . environ [ \"WEBHOOK_URL\" ] = \"https://hooks.slack.com/services/TXXXXXXX/BXXXXXXXXX/\" Post a message from hyperplane import notebook_common as nc nc . post_message ( \"Testing an incoming webhook\" , channel = \"#your-public-channel\" ) Post a dict from hyperplane import notebook_common as nc nc . post_table ({ \"a\" : \"a value\" , \"header b\" : \"some value\" }, channel = \"#your-public-channel\" )","title":"Using incoming webhooks for a specific channel"},{"location":"hyperplane_tools.html#using-slack-token","text":"Ensure you have specified your slack token. This may already be set up for you. import os os . environ [ \"SLACK_TOKEN\" ] = \"your-slack-t0ken\" Post a message from hyperplane import notebook_common as nc sh = nc . SlackHelper () sh . post_message ( 'Testing' , '#some-channel' ) Post a dict from hyperplane import notebook_common as nc sh = nc . SlackHelper () sh . post_table ({ \"a\" : \"a value\" , \"header b\" : \"some value\" }, '#some-channel' )","title":"Using slack token"},{"location":"hyperplane_tools.html#graphql-queries-in-jupyter-notebooks","text":"","title":"Graphql queries in jupyter notebooks"},{"location":"hyperplane_tools.html#submit-a-job-from-the-notebook","text":"You can paste the following snippet in a cell to run a pipeline from your jupyter notebook newjob = await nc . submit_pipeline_job ( 'hello.yaml' , parameters = { \"a\" : 1 , \"b\" : 1 }) newjob returns {'id': 'd6f2f4a3-aef7-477e-bc0b-f3a27b0ac0cb', 'runId': None}","title":"Submit a job from the notebook"},{"location":"hyperplane_tools.html#check-on-a-job-status-in-your-notebook","text":"results = [ \"a-job-id\" , \"another-job-id\" ] res = await nc . checkjobs ( results , loop = True ) print ( res ) loop=True will refresh the output every 5 seconds. ### Jobs summary 0 / 2 in progress 0 / 2 pending 2 / 2 processed 2 done | 0 timed out | 0 cancelled | 0 failed Progress: 100.0% ### Dask dashboards a-job-id done None another-job-id done None","title":"Check on a job status in your notebook"},{"location":"hyperplane_tools.html#cancel-a-job-from-the-notebook","text":"You can cancel a job by passing the id of your job. await nc . cancel_pipeline_job ( 'some-id' ) returns {'id': 'some-id', 'status': 'cancelled}","title":"Cancel a job from the notebook"},{"location":"hyperplane_tools.html#persistence-for-scheduling","text":"When you have scheduled jobs, you may want to store values from previous runs. To set a persistent value, use the following snippet: nc . set_current_job_output ( \"message in str\" ) To retrieve values from previous runs, use the following code: nc . get_previous_job_runs ( n = time_segments )","title":"Persistence for scheduling"},{"location":"image_jobtype_packages.html","text":"Images and jobTypes \u00b6 Images that you choose in the Jupyter instance starting screen will come with pre-configured packages. You will also be able to choose which an image for your job pipeline through the jobType parameter in your graphql query or Job Type field in the job submission panels. The preconfigured images will include the following packages: Basic \u00b6 conda packages \u00b6 adtk==0.6.2 aiobotocore==1.4.2 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 alembic==1.4.1 ansiwrap==0.8.4 awscli==1.19.106 black==21.12b0 blessings==1.7 boto3==1.17.106 botocore==1.20.106 certifi==2021.10.8 cffi==1.14.0 charset-normalizer==2.0.10 click==7.1.2 colorama==0.4.3 conda==4.8.3 conda-package-handling==1.7.0 cryptography==2.9.2 dask==2021.4.1 dask-glm==0.2.0 dask-kubernetes==subtree dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 flask==2.0.2 future==0.18.2 ghapi==0.1.19 gitdb==4.0.9 gitpython==3.1.24 google-api-core==1.31.5 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==0.6.0 gql==3.0.0a6 graphql-core==3.1.5 grpcio==1.43.0 gunicorn==20.1.0 hiredis==2.0.0 hyperplane==0.9 idna==3.3 ipywidgets==7.6.5 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.23 lightgbm==3.3.1 locket==0.2.1 lz4==3.1.3 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 modin==0.9.0 multidict==5.1.0 multipledispatch==0.6.0 nbconvert==5.6.1 nbdev==1.1.14 numpy==1.19.5 nvidia-ml-py3==7.352.0 opencensus==0.8.0 opencensus-context==0.1.2 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 pathspec==0.9.0 patsy==0.5.2 pdpyras==4.3.0 pip==20.0.2 platformdirs==2.4.1 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 ptvsd==4.3.2 py-spy==0.3.11 pyarrow==2.0.0 pyasn1-modules==0.2.8 pybigquery==0.5.0 pycosat==0.6.3 pycparser==2.20 pydantic==1.9.0 pydata-google-auth==1.3.0 pyopenssl==19.1.0 python-dotenv==0.15.0 python-editor==1.0.4 qtconsole==5.2.2 qtpy==2.0.0 querystring-parser==1.2.4 ray==1.5.2 redis==3.5.3 requests==2.27.0 ruamel-yaml==0.15.87 s3fs==2021.8.0 s3transfer==0.4.2 setuptools==60.2.0 six==1.14.0 slack-sdk==3.4.2 slackclient==2.9.3 smmap==5.0.0 sqlparse==0.4.2 statsmodels==0.13.1 tenacity==8.0.1 termcolor==1.1.0 textwrap3==0.9.2 tomli==1.2.3 tqdm==4.46.0 typing-inspect==0.7.1 urllib3==1.25.8 werkzeug==2.0.2 wheel==0.34.2 widgetsnbextension==3.5.2 wrapt==1.13.3 xgboost==0.90 yarl==1.6.3 zict==2.0.0 pip packages \u00b6 adtk==0.6.2 aiobotocore==1.4.2 aiohttp==3.8.1 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 aiosignal==1.2.0 alembic==1.4.1 ansiwrap==0.8.4 async-generator==1.10 async-timeout==4.0.2 attrs==21.4.0 awscli==1.19.106 black==21.12b0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto3==1.17.106 botocore==1.20.106 cachetools==4.2.4 certifi==2021.10.8 certipy==0.1.3 chardet==3.0.4 charset-normalizer==2.0.10 click==7.1.2 cloudpickle==2.0.0 colorama==0.4.3 conda==4.11.0 cytoolz==0.11.2 dask==2021.4.1 dask-glm==0.2.0 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docker-pycreds==0.4.0 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.2 frozenlist==1.2.0 fsspec==2021.11.1 future==0.18.2 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 google-api-core==1.31.5 google-auth==2.3.3 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==0.6.0 gql==3.0.0a6 graphql-core==3.1.5 grpcio==1.43.0 gunicorn==20.1.0 HeapDict==1.0.1 hiredis==2.0.0 idna==3.3 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.23 lightgbm==3.3.1 llvmlite==0.37.0 locket==0.2.1 lz4==3.1.3 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 modin==0.9.0 msgpack==1.0.3 multidict==5.2.0 multipledispatch==0.6.0 nbconvert==5.6.1 nbdev==1.1.14 numpy==1.19.5 nvidia-ml-py3==7.352.0 oauthlib==3.1.1 opencensus==0.8.0 opencensus-context==0.1.2 pamela==1.0.0 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 partd==1.2.0 pathspec==0.9.0 patsy==0.5.2 pdpyras==4.3.0 platformdirs==2.4.1 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 psutil==5.9.0 ptvsd==4.3.2 py-spy==0.3.11 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.8 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 python-dateutil==2.8.2 python-dotenv==0.15.0 python-editor==1.0.4 PyYAML==6.0 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.5.2 redis==3.5.3 requests==2.27.0 requests-oauthlib==1.3.0 rsa==4.8 ruamel_yaml==0.15.87 s3fs==2021.8.0 s3transfer==0.4.2 six==1.16.0 slack-sdk==3.4.2 slackclient==2.9.3 smmap==5.0.0 sortedcontainers==2.4.0 sqlparse==0.4.2 statsmodels==0.13.1 tblib==1.7.0 tenacity==8.0.1 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tomli==1.2.3 toolz==0.11.2 tornado==6.1 typing-inspect==0.7.1 urllib3==1.26.7 webencodings==0.5.1 websocket-client==1.2.3 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 xgboost==0.90 yarl==1.7.2 zict==2.0.0 GPU \u00b6 adtk==0.6.2 aiohttp-cors==0.7.0 aioredis==1.3.1 ansiwrap==0.8.4 appdirs==1.4.4 async-timeout==3.0.1 black==21.4b2 blazingsql==0.18.0 blessings==1.7 blinker==1.4 blosc==1.7.0 brotlipy==0.7.0 bsql-engine==0.6 certifi==2020.12.5 certipy==0.1.3 click-plugins==1.1.1 cligj==0.7.1 colorama==0.4.4 colorful==0.5.4 conda==4.8.3 confluent-kafka==1.5.0 cudf==0.18.2 cudf-kafka==0.18.2 cugraph==0.18.0+0.g65ec965f.dirty cuml==0.18.0 cusignal==0.18.0 cuspatial==0.18.0 custreamz==0.18.2 cuxfilter==0.18.0 cycler==0.10.0 cytoolz==0.11.0 dask-cuda==0.18.0 dask-cudf==0.18.2 dask-glm==0.2.0 dask-kubernetes==2021.3.1 dask-ml==1.7.0 dask-xgboost==0.1.11 datashape==0.5.4 docker-pycreds==0.4.0 entrypoints==0.3 fastcore==1.3.19 fastrelease==0.1.11 fastrlock==0.6 filelock==3.0.12 Fiona==1.8.18 fire==0.4.0 future==0.18.2 GDAL==3.1.4 ghapi==0.1.16 google-api-core==1.26.3 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.1.2 google-resumable-media==1.2.0 googleapis-common-protos==1.53.0 gpustat==0.6.0 gql==3.0.0a5 graphql-core==3.1.4 grpcio==1.37.0 HeapDict==1.0.1 hiredis==2.0.0 jupyter==1.0.0 jupyter-console==6.4.0 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.18 lightgbm==3.1.1 llvmlite==0.36.0 locket==0.2.1 lz4==3.1.3 MarkupSafe==1.1.1 mistune==0.8.4 modin==0.9.1 multipledispatch==0.6.0 munch==2.5.0 nbdev==1.1.14 nvidia-ml-py3==7.352.0 oauthlib==3.0.1 olefile==0.46 opencensus==0.7.12 opencensus-context==0.1.2 pamela==1.0.0 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 pathspec==0.8.1 patsy==0.5.1 proto-plus==1.18.1 protobuf==3.13.0 ptvsd==4.3.2 py-spy==0.3.5 pyarrow==1.0.1 pyasn1==0.4.8 pyasn1-modules==0.2.7 pybigquery==0.5.0 pycosat==0.6.3 pyct==0.4.6 pycurl==7.43.0.6 pydata-google-auth==1.2.0 PyHive==0.6.1 python-dotenv==0.15.0 python-editor==1.0.4 PyYAML==5.4.1 pyzmq==20.0.0 qtconsole==5.0.3 QtPy==1.9.0 ray==1.2.0 redis==3.5.3 regex==2021.4.4 rmm==0.18.0 sacremoses==0.0.45 sasl==0.2.1 slack-sdk==3.4.2 slackclient==2.9.3 statsmodels==0.12.2 tenacity==7.0.0 termcolor==1.1.0 terminado==0.9.4 text-unidecode==1.3 textwrap3==0.9.2 thrift==0.11.0 thrift-sasl==0.4.2 tokenizers==0.10.2 torch==1.7.1+cu110 torchaudio==0.7.2 torchvision==0.8.2+cu110 transformers==4.5.1 treelite==1.0.0 treelite-runtime==1.0.0 typing-inspect==0.6.0 webencodings==0.5.1 widgetsnbextension==3.5.1 zict==2.0.0 Deep learning (on CPU) \u00b6 adtk==0.6.2 aiobotocore==1.4.2 aiohttp==3.8.1 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 aiosignal==1.2.0 alembic==1.4.1 ansiwrap==0.8.4 async-generator==1.10 async-timeout==4.0.2 attrs==21.4.0 awscli==1.19.106 black==21.12b0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto3==1.17.106 botocore==1.20.106 cachetools==4.2.4 certifi==2021.10.8 certipy==0.1.3 chardet==3.0.4 charset-normalizer==2.0.10 click==7.1.2 cloudpickle==2.0.0 colorama==0.4.3 conda==4.11.0 cytoolz==0.11.2 dask==2021.4.1 dask-glm==0.2.0 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docker-pycreds==0.4.0 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.2 frozenlist==1.2.0 fsspec==2021.11.1 future==0.18.2 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 google-api-core==1.31.5 google-auth==2.3.3 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==0.6.0 gql==3.0.0a6 graphql-core==3.1.5 grpcio==1.43.0 gunicorn==20.1.0 HeapDict==1.0.1 hiredis==2.0.0 idna==3.3 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.23 lightgbm==3.3.1 llvmlite==0.37.0 locket==0.2.1 lz4==3.1.3 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 modin==0.9.0 msgpack==1.0.3 multidict==5.2.0 multipledispatch==0.6.0 nbconvert==5.6.1 nbdev==1.1.14 numpy==1.19.5 nvidia-ml-py3==7.352.0 oauthlib==3.1.1 opencensus==0.8.0 opencensus-context==0.1.2 pamela==1.0.0 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 partd==1.2.0 pathspec==0.9.0 patsy==0.5.2 pdpyras==4.3.0 platformdirs==2.4.1 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 psutil==5.9.0 ptvsd==4.3.2 py-spy==0.3.11 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.8 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 python-dateutil==2.8.2 python-dotenv==0.15.0 python-editor==1.0.4 PyYAML==6.0 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.5.2 redis==3.5.3 requests==2.27.0 requests-oauthlib==1.3.0 rsa==4.8 ruamel_yaml==0.15.87 s3fs==2021.8.0 s3transfer==0.4.2 six==1.16.0 slack-sdk==3.4.2 slackclient==2.9.3 smmap==5.0.0 sortedcontainers==2.4.0 sqlparse==0.4.2 statsmodels==0.13.1 tblib==1.7.0 tenacity==8.0.1 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tomli==1.2.3 toolz==0.11.2 tornado==6.1 typing-inspect==0.7.1 urllib3==1.26.7 webencodings==0.5.1 websocket-client==1.2.3 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 xgboost==0.90 yarl==1.7.2 zict==2.0.0 Spark on Ray \u00b6 absl-py==0.15.0 adtk==0.6.2 aiobotocore==1.4.2 aiohttp==3.7.4 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 alembic==1.4.1 ansiwrap==0.8.4 argcomplete==2.0.0 astunparse==1.6.3 async-generator==1.10 async-timeout==3.0.1 attrs==20.3.0 awscli==1.19.106 bayesian-optimization==1.2.0 black==21.12b0 blessed==1.19.0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto==2.49.0 boto3==1.17.106 botocore==1.20.106 brotlipy==0.7.0 certifi==2021.10.8 certipy==0.1.3 chardet==3.0.4 click==7.1.2 colorama==0.4.3 colorful==0.5.4 conda==4.11.0 convertdate==2.3.2 crcmod==1.7 cytoolz==0.11.2 dask-glm==0.2.0 dask-kubernetes==2021.3.1 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docker-pycreds==0.4.0 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fasteners==0.16.3 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.2 flatbuffers==1.12 flit_core fsspec==2021.7.0 future==0.18.2 gast==0.3.3 gcs-oauth2-boto-plugin==3.0 gensim==4.0.0 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 gluonts==0.8.1 google-api-core==1.31.5 google-apitools==0.5.32 google-auth==1.35.0 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-pasta==0.2.0 google-reauth==0.1.1 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==1.0.0b1 gql==3.0.0a6 graphql-core==3.1.5 graphviz==0.8.4 grpcio==1.32.0 gsutil==4.67 gunicorn==20.1.0 h5py==2.10.0 HeapDict==1.0.1 hijri-converter==2.2.2 hiredis==2.0.0 holidays==0.12 httplib2==0.20.2 hyperopt==0.2.5 ipython-genutils==0.2.0 ipywidgets==7.6.3 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 Keras-Preprocessing==1.1.2 korean-lunar-calendar==0.2.1 kubernetes==18.20.0 kubernetes-asyncio==19.15.0 libcst==0.3.23 lightgbm==3.3.1 lightning-bolts==0.3.4 llvmlite==0.37.0 locket==0.2.0 lz4==3.1.3 Markdown==3.3.6 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 mock==2.0.0 modin==0.9.0 monotonic==1.6 multipledispatch==0.6.0 mxnet==1.8.0.post0 nbconvert==5.6.1 nbdev==1.1.14 netifaces==0.11.0 networkx==2.6.3 numpy==1.19.5 nvidia-ml-py3==7.352.0 oauth2client==4.1.3 opencensus==0.8.0 opencensus-context==0.1.2 opt-einsum==3.3.0 pamela==1.0.0 pandas==1.1.4 pandas-gbq==0.14.1 papermill==2.3.1 pathspec==0.9.0 patsy==0.5.2 pbr==5.8.0 pdftotext==2.1.5 pdpyras==4.3.0 platformdirs==2.4.1 plotly==5.5.0 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 ptvsd==4.3.2 py-spy==0.3.11 py4j==0.10.9.2 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.7 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 pyDeprecate==0.3.1 PyMeeus==0.5.11 pynndescent==0.5.5 python-dotenv==0.15.0 python-editor==1.0.4 pytorch-lightning==1.4.3 PyYAML==5.4.1 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.9.1 raydp==0.4.1 redis==3.5.3 regex==2021.11.10 retry-decorator==1.1.1 rsa==4.7.2 s3fs==2021.8.0 s3transfer==0.4.2 sacremoses==0.0.46 six==1.15.0 slack-sdk==3.4.2 slackclient==2.9.3 smart-open==5.2.1 smmap==5.0.0 sqlparse==0.4.2 statsmodels==0.13.1 tenacity==8.0.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow==2.4.1 tensorflow-estimator==2.4.0 tensorflow-hub==0.11.0 tensorvision==0.1.dev2 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tokenizers==0.10.3 tomli==1.2.3 torch==1.7.1+cpu torchmetrics==0.6.2 torchvision==0.8.2+cpu transformers==4.4.2 typing==3.7.4.3 typing-extensions==3.10.0.2 typing-inspect==0.7.1 umap-learn==0.5.1 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wordcloud==1.8.1 wrapt==1.12.1 xgboost==0.90 zict==2.0.0 Ray \u00b6 absl-py==0.15.0 adtk==0.6.2 aiobotocore==1.4.2 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 alembic==1.4.1 ansiwrap==0.8.4 argcomplete==2.0.0 astunparse==1.6.3 async-generator==1.10 attrs==20.3.0 awscli==1.19.106 bayesian-optimization==1.2.0 black==21.12b0 blessed==1.19.0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto==2.49.0 boto3==1.17.106 botocore==1.20.106 brotlipy==0.7.0 certifi==2021.10.8 certipy==0.1.3 click==7.1.2 colorama==0.4.3 colorful==0.5.4 conda==4.11.0 convertdate==2.3.2 crcmod==1.7 cytoolz==0.11.2 dask-glm==0.2.0 dask-kubernetes==2021.3.1 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docker-pycreds==0.4.0 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fasteners==0.16.3 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.2 flatbuffers==1.12 fsspec==2021.7.0 future==0.18.2 gast==0.3.3 gcs-oauth2-boto-plugin==3.0 gensim==4.0.0 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 gluonts==0.8.1 google-api-core==1.31.5 google-apitools==0.5.32 google-auth==1.35.0 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-pasta==0.2.0 google-reauth==0.1.1 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==1.0.0b1 gql==3.0.0a6 graphql-core==3.1.5 graphviz==0.8.4 grpcio==1.32.0 gsutil==4.67 gunicorn==20.1.0 h5py==2.10.0 HeapDict==1.0.1 hijri-converter==2.2.2 hiredis==2.0.0 holidays==0.12 httplib2==0.20.2 hyperopt==0.2.5 ipython-genutils==0.2.0 ipywidgets==7.6.3 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 Keras-Preprocessing==1.1.2 korean-lunar-calendar==0.2.1 kubernetes==18.20.0 kubernetes-asyncio==19.15.0 libcst==0.3.23 lightgbm==3.3.1 lightning-bolts==0.3.4 llvmlite==0.37.0 locket==0.2.0 lz4==3.1.3 Markdown==3.3.6 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 mock==2.0.0 modin==0.12.1 modin-spreadsheet==0.1.2 monotonic==1.6 multipledispatch==0.6.0 mxnet==1.8.0.post0 nbconvert==5.6.1 nbdev==1.1.14 networkx==2.6.3 numpy==1.19.5 nvidia-ml-py3==7.352.0 oauth2client==4.1.3 opencensus==0.8.0 opencensus-context==0.1.2 opt-einsum==3.3.0 pamela==1.0.0 pandas==1.3.5 pandas-gbq==0.14.1 papermill==2.3.1 pathspec==0.9.0 patsy==0.5.2 pbr==5.8.0 pdftotext==2.1.5 pdpyras==4.3.0 platformdirs==2.4.1 plumbum==1.7.2 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 ptvsd==4.3.2 py-spy==0.3.11 py4j==0.10.9 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.7 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 pyDeprecate==0.3.1 PyMeeus==0.5.11 pynndescent==0.5.5 pyspark==3.0.3 python-dotenv==0.15.0 python-editor==1.0.4 pytorch-lightning==1.4.3 PyYAML==5.4.1 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.9.0 raydp==0.3.0 redis==3.5.3 regex==2021.11.10 retry-decorator==1.1.1 rpyc==4.1.5 rsa==4.7.2 s3fs==2021.8.0 s3transfer==0.4.2 sacremoses==0.0.46 six==1.15.0 slack-sdk==3.4.2 slackclient==2.9.3 smart-open==5.2.1 smmap==5.0.0 sqlparse==0.4.2 statsmodels==0.13.1 tenacity==8.0.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.0 tensorboardX==2.4.1 tensorflow==2.4.1 tensorflow-estimator==2.4.0 tensorflow-hub==0.11.0 tensorvision==0.1.dev2 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tokenizers==0.10.3 tomli==1.2.3 torch==1.7.1+cpu torchmetrics==0.6.2 torchvision==0.8.2+cpu transformers==4.4.2 typing==3.7.4.3 typing-extensions==3.10.0.2 typing-inspect==0.7.1 umap-learn==0.5.1 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wordcloud==1.8.1 wrapt==1.12.1 xgboost==1.5.1 xgboost-ray==0.1.6 zict==2.0.0 Triton Model Serving \u00b6 adtk==0.6.2 aiobotocore==1.4.2 aiofiles==0.5.0 aiohttp==3.8.1 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 aiosignal==1.2.0 alembic==1.4.1 aniso8601==7.0.0 ansiwrap==0.8.4 asgiref==3.4.1 async-exit-stack==1.0.1 async-generator==1.10 async-timeout==4.0.2 attrdict==2.0.1 attrs==21.4.0 awscli==1.19.106 black==21.12b0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto3==1.17.106 botocore==1.20.106 brotlipy==0.7.0 cachetools==4.2.4 certifi==2021.10.8 certipy==0.1.3 charset-normalizer==2.0.10 click==7.1.2 cloudpickle==2.0.0 colorama==0.4.3 conda==4.11.0 cytoolz==0.11.2 dask==2021.4.1 dask-glm==0.2.0 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 Django==3.2.6 dnspython==2.1.0 docker-pycreds==0.4.0 docutils==0.15.2 email-validator==1.1.3 fastapi==0.68.1 fastavro==1.3.1 fastcore==1.3.27 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.1 frozenlist==1.2.0 fsspec==2021.11.1 future==0.18.2 gevent==21.1.2 geventhttpclient==1.4.4 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 google-api-core==1.31.5 google-auth==2.3.3 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==0.6.0 gql==3.0.0a6 graphene==2.1.9 graphql-core==3.1.2 graphql-relay==2.0.1 greenlet==1.1.0 grpcio==1.39.0 gunicorn==20.1.0 h11==0.12.0 HeapDict==1.0.1 hiredis==2.0.0 httptools==0.1.2 idna==3.3 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==1.1.0 Jinja2==2.11.3 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.23 lightgbm==3.3.1 llvmlite==0.37.0 locket==0.2.1 lz4==3.1.3 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 modin==0.9.0 msgpack==1.0.3 multidict==5.2.0 multipledispatch==0.6.0 nbconvert==5.6.1 nbdev==1.1.14 numpy==1.21.1 nvidia-ml-py3==7.352.0 nvidia-pyindex==1.0.9 oauthlib==3.1.1 opencensus==0.8.0 opencensus-context==0.1.2 orjson==3.6.5 pamela==1.0.0 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 partd==1.2.0 pathspec==0.9.0 patsy==0.5.2 pdpyras==4.3.0 Pillow==8.3.1 platformdirs==2.4.1 prometheus-flask-exporter==0.18.7 promise==2.3 proto-plus==1.19.8 protobuf==3.17.3 psutil==5.9.0 ptvsd==4.3.2 py-spy==0.3.11 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.8 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 python-dateutil==2.8.2 python-dotenv==0.15.0 python-editor==1.0.4 python-multipart==0.0.5 python-rapidjson==1.4 PyYAML==6.0 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.5.2 redis==3.5.3 requests==2.27.0 requests-oauthlib==1.3.0 rsa==4.8 Rx==1.6.1 s3fs==2021.8.0 s3transfer==0.4.2 six==1.16.0 slack-sdk==3.4.2 slackclient==2.9.3 smmap==5.0.0 sortedcontainers==2.4.0 sqlparse==0.4.2 starlette==0.14.2 statsmodels==0.13.1 tblib==1.7.0 tenacity==8.0.1 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tomli==1.2.3 toolz==0.11.2 tornado==6.1 tritonclient==2.13.0 typing-inspect==0.7.1 ujson==4.3.0 urllib3==1.26.7 uvicorn==0.13.4 uvloop==0.16.0 watchgod==0.7 webencodings==0.5.1 websocket-client==1.2.3 websockets==8.1 Werkzeug==2.0.1 widgetsnbextension==3.5.2 wrapt==1.13.3 xgboost==0.90 yarl==1.7.2 zict==2.0.0 zope.event==4.5.0 zope.interface==5.4.0 Crypto \u00b6 absl-py==1.0.0 altair==4.2.0 anyio==3.4.0 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asgiref==3.4.1 astor==0.8.1 astunparse==1.6.3 attrs==21.4.0 backcall==0.2.0 backports.zoneinfo==0.2.1 base58==2.1.1 bleach==4.1.0 blinker==1.4 bokeh==2.4.1 brotlipy==0.7.0 cachetools==4.2.4 certifi==2021.5.30 click==7.1.2 conda==4.10.3 debugpy==1.5.1 decorator==5.1.0 defusedxml==0.7.1 entrypoints==0.3 fastapi==0.70.1 filelock==3.4.2 Flask==2.0.1 flatbuffers==2.0 gast==0.4.0 gevent==21.12.0 geventhttpclient==1.4.4 gitdb==4.0.9 GitPython==3.1.24 google-auth==2.3.3 google-auth-oauthlib==0.4.6 google-pasta==0.2.0 greenlet==1.1.2 grpcio==1.39.0 h11==0.12.0 h5py==3.6.0 huggingface-hub==0.0.12 importlib-metadata==4.10.0 importlib-resources==5.4.0 ipykernel==6.6.1 ipython==7.31.0 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jedi==0.18.1 Jinja2==3.0.3 joblib==1.1.0 jsonschema==4.3.3 jupyter-client==7.1.0 jupyter-core==4.9.1 jupyterlab-pygments==0.1.2 jupyterlab-widgets==1.0.2 keras==2.7.0 Keras-Preprocessing==1.1.2 libclang==12.0.0 Markdown==3.3.6 MarkupSafe==2.0.1 matplotlib-inline==0.1.3 mistune==0.8.4 nbclient==0.5.9 nbconvert==6.4.0 nbformat==5.1.3 nest-asyncio==1.5.4 notebook==6.4.6 numpy==1.22.0 oauthlib==3.1.1 opencv-python==4.5.5.62 opt-einsum==3.3.0 packaging==21.3 pandas==1.3.5 pandocfilters==1.5.0 parso==0.8.3 pexpect==4.8.0 pickleshare==0.7.5 Pillow==9.0.0 prometheus-client==0.12.0 prompt-toolkit==3.0.24 protobuf==3.19.1 ptyprocess==0.7.0 pyarrow==6.0.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycosat==0.6.3 pydantic==1.9.0 pydeck==0.7.1 Pygments==2.11.2 Pympler==1.0.1 pyparsing==3.0.6 pyrsistent==0.18.0 python-dateutil==2.8.2 python-rapidjson==1.5 pytz==2021.3 pytz-deprecation-shim==0.1.0.post0 PyYAML==6.0 pyzmq==22.3.0 regex==2021.11.10 requests-oauthlib==1.3.0 rsa==4.8 sacremoses==0.0.46 Send2Trash==1.8.0 smmap==5.0.0 sniffio==1.2.0 starlette==0.16.0 streamlit==1.3.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow==2.7.0 tensorflow-estimator==2.7.0 tensorflow-io-gcs-filesystem==0.23.1 termcolor==1.1.0 terminado==0.12.1 testpath==0.5.0 tokenizers==0.10.3 toml==0.10.2 toolz==0.11.2 torch==1.9.0 tornado==6.1 traitlets==5.1.1 transformers==4.9.2 tritonclient==2.17.0 typing-extensions==4.0.1 tzdata==2021.5 tzlocal==4.1 uvicorn==0.16.0 validators==0.18.2 watchdog==2.1.6 wcwidth==0.2.5 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 zipp==3.7.0 zope.event==4.5.0 zope.interface==5.4.0 Climate \u00b6 This image contains the packages below in addition to everything in basic . conda packages \u00b6 zarr=2.6.1 xclim=0.22.0 pangeo-notebook==0.0.2 xarray-spatial=0.1.2 intake=0.6.1 intake-esm=2020.12.18 rioxarray=0.3.1 cartopy=0.18.0 nc-time-axis=1.2.0 xesmf=0.5.2 seaborn=0.11.1 pip packages \u00b6 h3==3.7.2 climate_indices==1.0.9 SAS \u00b6 absl-py==1.0.0 altair==4.2.0 anyio==3.4.0 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asgiref==3.4.1 astor==0.8.1 astunparse==1.6.3 attrs==21.4.0 backcall==0.2.0 backports.zoneinfo==0.2.1 base58==2.1.1 bleach==4.1.0 blinker==1.4 bokeh==2.4.1 brotlipy==0.7.0 cachetools==4.2.4 certifi==2021.5.30 click==7.1.2 conda==4.10.3 debugpy==1.5.1 decorator==5.1.0 defusedxml==0.7.1 entrypoints==0.3 fastapi==0.70.1 filelock==3.4.2 Flask==2.0.1 flatbuffers==2.0 gast==0.4.0 gevent==21.12.0 geventhttpclient==1.4.4 gitdb==4.0.9 GitPython==3.1.24 google-auth==2.3.3 google-auth-oauthlib==0.4.6 google-pasta==0.2.0 greenlet==1.1.2 grpcio==1.39.0 h11==0.12.0 h5py==3.6.0 huggingface-hub==0.0.12 importlib-metadata==4.10.0 importlib-resources==5.4.0 ipykernel==6.6.1 ipython==7.31.0 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jedi==0.18.1 Jinja2==3.0.3 joblib==1.1.0 jsonschema==4.3.3 jupyter-client==7.1.0 jupyter-core==4.9.1 jupyterlab-pygments==0.1.2 jupyterlab-widgets==1.0.2 keras==2.7.0 Keras-Preprocessing==1.1.2 libclang==12.0.0 Markdown==3.3.6 MarkupSafe==2.0.1 matplotlib-inline==0.1.3 mistune==0.8.4 nbclient==0.5.9 nbconvert==6.4.0 nbformat==5.1.3 nest-asyncio==1.5.4 notebook==6.4.6 numpy==1.22.0 oauthlib==3.1.1 opencv-python==4.5.5.62 opt-einsum==3.3.0 packaging==21.3 pandas==1.3.5 pandocfilters==1.5.0 parso==0.8.3 pexpect==4.8.0 pickleshare==0.7.5 Pillow==9.0.0 prometheus-client==0.12.0 prompt-toolkit==3.0.24 protobuf==3.19.1 ptyprocess==0.7.0 pyarrow==6.0.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycosat==0.6.3 pydantic==1.9.0 pydeck==0.7.1 Pygments==2.11.2 Pympler==1.0.1 pyparsing==3.0.6 pyrsistent==0.18.0 python-dateutil==2.8.2 python-rapidjson==1.5 pytz==2021.3 pytz-deprecation-shim==0.1.0.post0 PyYAML==6.0 pyzmq==22.3.0 regex==2021.11.10 requests-oauthlib==1.3.0 rsa==4.8 sacremoses==0.0.46 Send2Trash==1.8.0 smmap==5.0.0 sniffio==1.2.0 starlette==0.16.0 streamlit==1.3.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow==2.7.0 tensorflow-estimator==2.7.0 tensorflow-io-gcs-filesystem==0.23.1 termcolor==1.1.0 terminado==0.12.1 testpath==0.5.0 tokenizers==0.10.3 toml==0.10.2 toolz==0.11.2 torch==1.9.0 tornado==6.1 traitlets==5.1.1 transformers==4.9.2 tritonclient==2.17.0 typing-extensions==4.0.1 tzdata==2021.5 tzlocal==4.1 uvicorn==0.16.0 validators==0.18.2 watchdog==2.1.6 wcwidth==0.2.5 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 zipp==3.7.0 zope.event==4.5.0 zope.interface==5.4.0 R \u00b6 absl-py==1.0.0 altair==4.2.0 anyio==3.4.0 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asgiref==3.4.1 astor==0.8.1 astunparse==1.6.3 attrs==21.4.0 backcall==0.2.0 backports.zoneinfo==0.2.1 base58==2.1.1 bleach==4.1.0 blinker==1.4 bokeh==2.4.1 brotlipy==0.7.0 cachetools==4.2.4 certifi==2021.5.30 click==7.1.2 conda==4.10.3 debugpy==1.5.1 decorator==5.1.0 defusedxml==0.7.1 entrypoints==0.3 fastapi==0.70.1 filelock==3.4.2 Flask==2.0.1 flatbuffers==2.0 gast==0.4.0 gevent==21.12.0 geventhttpclient==1.4.4 gitdb==4.0.9 GitPython==3.1.24 google-auth==2.3.3 google-auth-oauthlib==0.4.6 google-pasta==0.2.0 greenlet==1.1.2 grpcio==1.39.0 h11==0.12.0 h5py==3.6.0 huggingface-hub==0.0.12 importlib-metadata==4.10.0 importlib-resources==5.4.0 ipykernel==6.6.1 ipython==7.31.0 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jedi==0.18.1 Jinja2==3.0.3 joblib==1.1.0 jsonschema==4.3.3 jupyter-client==7.1.0 jupyter-core==4.9.1 jupyterlab-pygments==0.1.2 jupyterlab-widgets==1.0.2 keras==2.7.0 Keras-Preprocessing==1.1.2 libclang==12.0.0 Markdown==3.3.6 MarkupSafe==2.0.1 matplotlib-inline==0.1.3 mistune==0.8.4 nbclient==0.5.9 nbconvert==6.4.0 nbformat==5.1.3 nest-asyncio==1.5.4 notebook==6.4.6 numpy==1.22.0 oauthlib==3.1.1 opencv-python==4.5.5.62 opt-einsum==3.3.0 packaging==21.3 pandas==1.3.5 pandocfilters==1.5.0 parso==0.8.3 pexpect==4.8.0 pickleshare==0.7.5 Pillow==9.0.0 prometheus-client==0.12.0 prompt-toolkit==3.0.24 protobuf==3.19.1 ptyprocess==0.7.0 pyarrow==6.0.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycosat==0.6.3 pydantic==1.9.0 pydeck==0.7.1 Pygments==2.11.2 Pympler==1.0.1 pyparsing==3.0.6 pyrsistent==0.18.0 python-dateutil==2.8.2 python-rapidjson==1.5 pytz==2021.3 pytz-deprecation-shim==0.1.0.post0 PyYAML==6.0 pyzmq==22.3.0 regex==2021.11.10 requests-oauthlib==1.3.0 rsa==4.8 sacremoses==0.0.46 Send2Trash==1.8.0 smmap==5.0.0 sniffio==1.2.0 starlette==0.16.0 streamlit==1.3.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow==2.7.0 tensorflow-estimator==2.7.0 tensorflow-io-gcs-filesystem==0.23.1 termcolor==1.1.0 terminado==0.12.1 testpath==0.5.0 tokenizers==0.10.3 toml==0.10.2 toolz==0.11.2 torch==1.9.0 tornado==6.1 traitlets==5.1.1 transformers==4.9.2 tritonclient==2.17.0 typing-extensions==4.0.1 tzdata==2021.5 tzlocal==4.1 uvicorn==0.16.0 validators==0.18.2 watchdog==2.1.6 wcwidth==0.2.5 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 zipp==3.7.0 zope.event==4.5.0 zope.interface==5.4.0","title":"Images and jobTypes"},{"location":"image_jobtype_packages.html#images-and-jobtypes","text":"Images that you choose in the Jupyter instance starting screen will come with pre-configured packages. You will also be able to choose which an image for your job pipeline through the jobType parameter in your graphql query or Job Type field in the job submission panels. The preconfigured images will include the following packages:","title":"Images and jobTypes"},{"location":"image_jobtype_packages.html#basic","text":"","title":"Basic"},{"location":"image_jobtype_packages.html#conda-packages","text":"adtk==0.6.2 aiobotocore==1.4.2 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 alembic==1.4.1 ansiwrap==0.8.4 awscli==1.19.106 black==21.12b0 blessings==1.7 boto3==1.17.106 botocore==1.20.106 certifi==2021.10.8 cffi==1.14.0 charset-normalizer==2.0.10 click==7.1.2 colorama==0.4.3 conda==4.8.3 conda-package-handling==1.7.0 cryptography==2.9.2 dask==2021.4.1 dask-glm==0.2.0 dask-kubernetes==subtree dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 flask==2.0.2 future==0.18.2 ghapi==0.1.19 gitdb==4.0.9 gitpython==3.1.24 google-api-core==1.31.5 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==0.6.0 gql==3.0.0a6 graphql-core==3.1.5 grpcio==1.43.0 gunicorn==20.1.0 hiredis==2.0.0 hyperplane==0.9 idna==3.3 ipywidgets==7.6.5 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.23 lightgbm==3.3.1 locket==0.2.1 lz4==3.1.3 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 modin==0.9.0 multidict==5.1.0 multipledispatch==0.6.0 nbconvert==5.6.1 nbdev==1.1.14 numpy==1.19.5 nvidia-ml-py3==7.352.0 opencensus==0.8.0 opencensus-context==0.1.2 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 pathspec==0.9.0 patsy==0.5.2 pdpyras==4.3.0 pip==20.0.2 platformdirs==2.4.1 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 ptvsd==4.3.2 py-spy==0.3.11 pyarrow==2.0.0 pyasn1-modules==0.2.8 pybigquery==0.5.0 pycosat==0.6.3 pycparser==2.20 pydantic==1.9.0 pydata-google-auth==1.3.0 pyopenssl==19.1.0 python-dotenv==0.15.0 python-editor==1.0.4 qtconsole==5.2.2 qtpy==2.0.0 querystring-parser==1.2.4 ray==1.5.2 redis==3.5.3 requests==2.27.0 ruamel-yaml==0.15.87 s3fs==2021.8.0 s3transfer==0.4.2 setuptools==60.2.0 six==1.14.0 slack-sdk==3.4.2 slackclient==2.9.3 smmap==5.0.0 sqlparse==0.4.2 statsmodels==0.13.1 tenacity==8.0.1 termcolor==1.1.0 textwrap3==0.9.2 tomli==1.2.3 tqdm==4.46.0 typing-inspect==0.7.1 urllib3==1.25.8 werkzeug==2.0.2 wheel==0.34.2 widgetsnbextension==3.5.2 wrapt==1.13.3 xgboost==0.90 yarl==1.6.3 zict==2.0.0","title":"conda packages"},{"location":"image_jobtype_packages.html#pip-packages","text":"adtk==0.6.2 aiobotocore==1.4.2 aiohttp==3.8.1 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 aiosignal==1.2.0 alembic==1.4.1 ansiwrap==0.8.4 async-generator==1.10 async-timeout==4.0.2 attrs==21.4.0 awscli==1.19.106 black==21.12b0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto3==1.17.106 botocore==1.20.106 cachetools==4.2.4 certifi==2021.10.8 certipy==0.1.3 chardet==3.0.4 charset-normalizer==2.0.10 click==7.1.2 cloudpickle==2.0.0 colorama==0.4.3 conda==4.11.0 cytoolz==0.11.2 dask==2021.4.1 dask-glm==0.2.0 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docker-pycreds==0.4.0 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.2 frozenlist==1.2.0 fsspec==2021.11.1 future==0.18.2 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 google-api-core==1.31.5 google-auth==2.3.3 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==0.6.0 gql==3.0.0a6 graphql-core==3.1.5 grpcio==1.43.0 gunicorn==20.1.0 HeapDict==1.0.1 hiredis==2.0.0 idna==3.3 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.23 lightgbm==3.3.1 llvmlite==0.37.0 locket==0.2.1 lz4==3.1.3 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 modin==0.9.0 msgpack==1.0.3 multidict==5.2.0 multipledispatch==0.6.0 nbconvert==5.6.1 nbdev==1.1.14 numpy==1.19.5 nvidia-ml-py3==7.352.0 oauthlib==3.1.1 opencensus==0.8.0 opencensus-context==0.1.2 pamela==1.0.0 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 partd==1.2.0 pathspec==0.9.0 patsy==0.5.2 pdpyras==4.3.0 platformdirs==2.4.1 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 psutil==5.9.0 ptvsd==4.3.2 py-spy==0.3.11 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.8 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 python-dateutil==2.8.2 python-dotenv==0.15.0 python-editor==1.0.4 PyYAML==6.0 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.5.2 redis==3.5.3 requests==2.27.0 requests-oauthlib==1.3.0 rsa==4.8 ruamel_yaml==0.15.87 s3fs==2021.8.0 s3transfer==0.4.2 six==1.16.0 slack-sdk==3.4.2 slackclient==2.9.3 smmap==5.0.0 sortedcontainers==2.4.0 sqlparse==0.4.2 statsmodels==0.13.1 tblib==1.7.0 tenacity==8.0.1 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tomli==1.2.3 toolz==0.11.2 tornado==6.1 typing-inspect==0.7.1 urllib3==1.26.7 webencodings==0.5.1 websocket-client==1.2.3 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 xgboost==0.90 yarl==1.7.2 zict==2.0.0","title":"pip packages"},{"location":"image_jobtype_packages.html#gpu","text":"adtk==0.6.2 aiohttp-cors==0.7.0 aioredis==1.3.1 ansiwrap==0.8.4 appdirs==1.4.4 async-timeout==3.0.1 black==21.4b2 blazingsql==0.18.0 blessings==1.7 blinker==1.4 blosc==1.7.0 brotlipy==0.7.0 bsql-engine==0.6 certifi==2020.12.5 certipy==0.1.3 click-plugins==1.1.1 cligj==0.7.1 colorama==0.4.4 colorful==0.5.4 conda==4.8.3 confluent-kafka==1.5.0 cudf==0.18.2 cudf-kafka==0.18.2 cugraph==0.18.0+0.g65ec965f.dirty cuml==0.18.0 cusignal==0.18.0 cuspatial==0.18.0 custreamz==0.18.2 cuxfilter==0.18.0 cycler==0.10.0 cytoolz==0.11.0 dask-cuda==0.18.0 dask-cudf==0.18.2 dask-glm==0.2.0 dask-kubernetes==2021.3.1 dask-ml==1.7.0 dask-xgboost==0.1.11 datashape==0.5.4 docker-pycreds==0.4.0 entrypoints==0.3 fastcore==1.3.19 fastrelease==0.1.11 fastrlock==0.6 filelock==3.0.12 Fiona==1.8.18 fire==0.4.0 future==0.18.2 GDAL==3.1.4 ghapi==0.1.16 google-api-core==1.26.3 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.1.2 google-resumable-media==1.2.0 googleapis-common-protos==1.53.0 gpustat==0.6.0 gql==3.0.0a5 graphql-core==3.1.4 grpcio==1.37.0 HeapDict==1.0.1 hiredis==2.0.0 jupyter==1.0.0 jupyter-console==6.4.0 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.18 lightgbm==3.1.1 llvmlite==0.36.0 locket==0.2.1 lz4==3.1.3 MarkupSafe==1.1.1 mistune==0.8.4 modin==0.9.1 multipledispatch==0.6.0 munch==2.5.0 nbdev==1.1.14 nvidia-ml-py3==7.352.0 oauthlib==3.0.1 olefile==0.46 opencensus==0.7.12 opencensus-context==0.1.2 pamela==1.0.0 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 pathspec==0.8.1 patsy==0.5.1 proto-plus==1.18.1 protobuf==3.13.0 ptvsd==4.3.2 py-spy==0.3.5 pyarrow==1.0.1 pyasn1==0.4.8 pyasn1-modules==0.2.7 pybigquery==0.5.0 pycosat==0.6.3 pyct==0.4.6 pycurl==7.43.0.6 pydata-google-auth==1.2.0 PyHive==0.6.1 python-dotenv==0.15.0 python-editor==1.0.4 PyYAML==5.4.1 pyzmq==20.0.0 qtconsole==5.0.3 QtPy==1.9.0 ray==1.2.0 redis==3.5.3 regex==2021.4.4 rmm==0.18.0 sacremoses==0.0.45 sasl==0.2.1 slack-sdk==3.4.2 slackclient==2.9.3 statsmodels==0.12.2 tenacity==7.0.0 termcolor==1.1.0 terminado==0.9.4 text-unidecode==1.3 textwrap3==0.9.2 thrift==0.11.0 thrift-sasl==0.4.2 tokenizers==0.10.2 torch==1.7.1+cu110 torchaudio==0.7.2 torchvision==0.8.2+cu110 transformers==4.5.1 treelite==1.0.0 treelite-runtime==1.0.0 typing-inspect==0.6.0 webencodings==0.5.1 widgetsnbextension==3.5.1 zict==2.0.0","title":"GPU"},{"location":"image_jobtype_packages.html#deep-learning-on-cpu","text":"adtk==0.6.2 aiobotocore==1.4.2 aiohttp==3.8.1 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 aiosignal==1.2.0 alembic==1.4.1 ansiwrap==0.8.4 async-generator==1.10 async-timeout==4.0.2 attrs==21.4.0 awscli==1.19.106 black==21.12b0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto3==1.17.106 botocore==1.20.106 cachetools==4.2.4 certifi==2021.10.8 certipy==0.1.3 chardet==3.0.4 charset-normalizer==2.0.10 click==7.1.2 cloudpickle==2.0.0 colorama==0.4.3 conda==4.11.0 cytoolz==0.11.2 dask==2021.4.1 dask-glm==0.2.0 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docker-pycreds==0.4.0 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.2 frozenlist==1.2.0 fsspec==2021.11.1 future==0.18.2 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 google-api-core==1.31.5 google-auth==2.3.3 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==0.6.0 gql==3.0.0a6 graphql-core==3.1.5 grpcio==1.43.0 gunicorn==20.1.0 HeapDict==1.0.1 hiredis==2.0.0 idna==3.3 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.23 lightgbm==3.3.1 llvmlite==0.37.0 locket==0.2.1 lz4==3.1.3 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 modin==0.9.0 msgpack==1.0.3 multidict==5.2.0 multipledispatch==0.6.0 nbconvert==5.6.1 nbdev==1.1.14 numpy==1.19.5 nvidia-ml-py3==7.352.0 oauthlib==3.1.1 opencensus==0.8.0 opencensus-context==0.1.2 pamela==1.0.0 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 partd==1.2.0 pathspec==0.9.0 patsy==0.5.2 pdpyras==4.3.0 platformdirs==2.4.1 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 psutil==5.9.0 ptvsd==4.3.2 py-spy==0.3.11 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.8 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 python-dateutil==2.8.2 python-dotenv==0.15.0 python-editor==1.0.4 PyYAML==6.0 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.5.2 redis==3.5.3 requests==2.27.0 requests-oauthlib==1.3.0 rsa==4.8 ruamel_yaml==0.15.87 s3fs==2021.8.0 s3transfer==0.4.2 six==1.16.0 slack-sdk==3.4.2 slackclient==2.9.3 smmap==5.0.0 sortedcontainers==2.4.0 sqlparse==0.4.2 statsmodels==0.13.1 tblib==1.7.0 tenacity==8.0.1 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tomli==1.2.3 toolz==0.11.2 tornado==6.1 typing-inspect==0.7.1 urllib3==1.26.7 webencodings==0.5.1 websocket-client==1.2.3 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 xgboost==0.90 yarl==1.7.2 zict==2.0.0","title":"Deep learning (on CPU)"},{"location":"image_jobtype_packages.html#spark-on-ray","text":"absl-py==0.15.0 adtk==0.6.2 aiobotocore==1.4.2 aiohttp==3.7.4 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 alembic==1.4.1 ansiwrap==0.8.4 argcomplete==2.0.0 astunparse==1.6.3 async-generator==1.10 async-timeout==3.0.1 attrs==20.3.0 awscli==1.19.106 bayesian-optimization==1.2.0 black==21.12b0 blessed==1.19.0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto==2.49.0 boto3==1.17.106 botocore==1.20.106 brotlipy==0.7.0 certifi==2021.10.8 certipy==0.1.3 chardet==3.0.4 click==7.1.2 colorama==0.4.3 colorful==0.5.4 conda==4.11.0 convertdate==2.3.2 crcmod==1.7 cytoolz==0.11.2 dask-glm==0.2.0 dask-kubernetes==2021.3.1 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docker-pycreds==0.4.0 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fasteners==0.16.3 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.2 flatbuffers==1.12 flit_core fsspec==2021.7.0 future==0.18.2 gast==0.3.3 gcs-oauth2-boto-plugin==3.0 gensim==4.0.0 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 gluonts==0.8.1 google-api-core==1.31.5 google-apitools==0.5.32 google-auth==1.35.0 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-pasta==0.2.0 google-reauth==0.1.1 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==1.0.0b1 gql==3.0.0a6 graphql-core==3.1.5 graphviz==0.8.4 grpcio==1.32.0 gsutil==4.67 gunicorn==20.1.0 h5py==2.10.0 HeapDict==1.0.1 hijri-converter==2.2.2 hiredis==2.0.0 holidays==0.12 httplib2==0.20.2 hyperopt==0.2.5 ipython-genutils==0.2.0 ipywidgets==7.6.3 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 Keras-Preprocessing==1.1.2 korean-lunar-calendar==0.2.1 kubernetes==18.20.0 kubernetes-asyncio==19.15.0 libcst==0.3.23 lightgbm==3.3.1 lightning-bolts==0.3.4 llvmlite==0.37.0 locket==0.2.0 lz4==3.1.3 Markdown==3.3.6 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 mock==2.0.0 modin==0.9.0 monotonic==1.6 multipledispatch==0.6.0 mxnet==1.8.0.post0 nbconvert==5.6.1 nbdev==1.1.14 netifaces==0.11.0 networkx==2.6.3 numpy==1.19.5 nvidia-ml-py3==7.352.0 oauth2client==4.1.3 opencensus==0.8.0 opencensus-context==0.1.2 opt-einsum==3.3.0 pamela==1.0.0 pandas==1.1.4 pandas-gbq==0.14.1 papermill==2.3.1 pathspec==0.9.0 patsy==0.5.2 pbr==5.8.0 pdftotext==2.1.5 pdpyras==4.3.0 platformdirs==2.4.1 plotly==5.5.0 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 ptvsd==4.3.2 py-spy==0.3.11 py4j==0.10.9.2 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.7 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 pyDeprecate==0.3.1 PyMeeus==0.5.11 pynndescent==0.5.5 python-dotenv==0.15.0 python-editor==1.0.4 pytorch-lightning==1.4.3 PyYAML==5.4.1 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.9.1 raydp==0.4.1 redis==3.5.3 regex==2021.11.10 retry-decorator==1.1.1 rsa==4.7.2 s3fs==2021.8.0 s3transfer==0.4.2 sacremoses==0.0.46 six==1.15.0 slack-sdk==3.4.2 slackclient==2.9.3 smart-open==5.2.1 smmap==5.0.0 sqlparse==0.4.2 statsmodels==0.13.1 tenacity==8.0.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow==2.4.1 tensorflow-estimator==2.4.0 tensorflow-hub==0.11.0 tensorvision==0.1.dev2 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tokenizers==0.10.3 tomli==1.2.3 torch==1.7.1+cpu torchmetrics==0.6.2 torchvision==0.8.2+cpu transformers==4.4.2 typing==3.7.4.3 typing-extensions==3.10.0.2 typing-inspect==0.7.1 umap-learn==0.5.1 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wordcloud==1.8.1 wrapt==1.12.1 xgboost==0.90 zict==2.0.0","title":"Spark on Ray"},{"location":"image_jobtype_packages.html#ray","text":"absl-py==0.15.0 adtk==0.6.2 aiobotocore==1.4.2 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 alembic==1.4.1 ansiwrap==0.8.4 argcomplete==2.0.0 astunparse==1.6.3 async-generator==1.10 attrs==20.3.0 awscli==1.19.106 bayesian-optimization==1.2.0 black==21.12b0 blessed==1.19.0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto==2.49.0 boto3==1.17.106 botocore==1.20.106 brotlipy==0.7.0 certifi==2021.10.8 certipy==0.1.3 click==7.1.2 colorama==0.4.3 colorful==0.5.4 conda==4.11.0 convertdate==2.3.2 crcmod==1.7 cytoolz==0.11.2 dask-glm==0.2.0 dask-kubernetes==2021.3.1 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 docker-pycreds==0.4.0 docutils==0.15.2 fastavro==1.3.1 fastcore==1.3.27 fasteners==0.16.3 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.2 flatbuffers==1.12 fsspec==2021.7.0 future==0.18.2 gast==0.3.3 gcs-oauth2-boto-plugin==3.0 gensim==4.0.0 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 gluonts==0.8.1 google-api-core==1.31.5 google-apitools==0.5.32 google-auth==1.35.0 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-pasta==0.2.0 google-reauth==0.1.1 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==1.0.0b1 gql==3.0.0a6 graphql-core==3.1.5 graphviz==0.8.4 grpcio==1.32.0 gsutil==4.67 gunicorn==20.1.0 h5py==2.10.0 HeapDict==1.0.1 hijri-converter==2.2.2 hiredis==2.0.0 holidays==0.12 httplib2==0.20.2 hyperopt==0.2.5 ipython-genutils==0.2.0 ipywidgets==7.6.3 itsdangerous==2.0.1 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 Keras-Preprocessing==1.1.2 korean-lunar-calendar==0.2.1 kubernetes==18.20.0 kubernetes-asyncio==19.15.0 libcst==0.3.23 lightgbm==3.3.1 lightning-bolts==0.3.4 llvmlite==0.37.0 locket==0.2.0 lz4==3.1.3 Markdown==3.3.6 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 mock==2.0.0 modin==0.12.1 modin-spreadsheet==0.1.2 monotonic==1.6 multipledispatch==0.6.0 mxnet==1.8.0.post0 nbconvert==5.6.1 nbdev==1.1.14 networkx==2.6.3 numpy==1.19.5 nvidia-ml-py3==7.352.0 oauth2client==4.1.3 opencensus==0.8.0 opencensus-context==0.1.2 opt-einsum==3.3.0 pamela==1.0.0 pandas==1.3.5 pandas-gbq==0.14.1 papermill==2.3.1 pathspec==0.9.0 patsy==0.5.2 pbr==5.8.0 pdftotext==2.1.5 pdpyras==4.3.0 platformdirs==2.4.1 plumbum==1.7.2 prometheus-flask-exporter==0.18.7 proto-plus==1.19.8 protobuf==3.19.1 ptvsd==4.3.2 py-spy==0.3.11 py4j==0.10.9 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.7 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 pyDeprecate==0.3.1 PyMeeus==0.5.11 pynndescent==0.5.5 pyspark==3.0.3 python-dotenv==0.15.0 python-editor==1.0.4 pytorch-lightning==1.4.3 PyYAML==5.4.1 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.9.0 raydp==0.3.0 redis==3.5.3 regex==2021.11.10 retry-decorator==1.1.1 rpyc==4.1.5 rsa==4.7.2 s3fs==2021.8.0 s3transfer==0.4.2 sacremoses==0.0.46 six==1.15.0 slack-sdk==3.4.2 slackclient==2.9.3 smart-open==5.2.1 smmap==5.0.0 sqlparse==0.4.2 statsmodels==0.13.1 tenacity==8.0.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.0 tensorboardX==2.4.1 tensorflow==2.4.1 tensorflow-estimator==2.4.0 tensorflow-hub==0.11.0 tensorvision==0.1.dev2 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tokenizers==0.10.3 tomli==1.2.3 torch==1.7.1+cpu torchmetrics==0.6.2 torchvision==0.8.2+cpu transformers==4.4.2 typing==3.7.4.3 typing-extensions==3.10.0.2 typing-inspect==0.7.1 umap-learn==0.5.1 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wordcloud==1.8.1 wrapt==1.12.1 xgboost==1.5.1 xgboost-ray==0.1.6 zict==2.0.0","title":"Ray"},{"location":"image_jobtype_packages.html#triton-model-serving","text":"adtk==0.6.2 aiobotocore==1.4.2 aiofiles==0.5.0 aiohttp==3.8.1 aiohttp-cors==0.7.0 aioitertools==0.8.0 aioredis==1.3.1 aiosignal==1.2.0 alembic==1.4.1 aniso8601==7.0.0 ansiwrap==0.8.4 asgiref==3.4.1 async-exit-stack==1.0.1 async-generator==1.10 async-timeout==4.0.2 attrdict==2.0.1 attrs==21.4.0 awscli==1.19.106 black==21.12b0 blessings==1.7 blinker==1.4 blosc==1.7.0 boto3==1.17.106 botocore==1.20.106 brotlipy==0.7.0 cachetools==4.2.4 certifi==2021.10.8 certipy==0.1.3 charset-normalizer==2.0.10 click==7.1.2 cloudpickle==2.0.0 colorama==0.4.3 conda==4.11.0 cytoolz==0.11.2 dask==2021.4.1 dask-glm==0.2.0 dask-ml==1.9.0 dask-xgboost==0.1.11 databricks-cli==0.16.2 distributed==2021.3.1 Django==3.2.6 dnspython==2.1.0 docker-pycreds==0.4.0 docutils==0.15.2 email-validator==1.1.3 fastapi==0.68.1 fastavro==1.3.1 fastcore==1.3.27 fastrelease==0.1.12 filelock==3.4.2 fire==0.4.0 Flask==2.0.1 frozenlist==1.2.0 fsspec==2021.11.1 future==0.18.2 gevent==21.1.2 geventhttpclient==1.4.4 ghapi==0.1.19 gitdb==4.0.9 GitPython==3.1.24 google-api-core==1.31.5 google-auth==2.3.3 google-cloud-bigquery==2.6.1 google-cloud-bigquery-storage==2.2.1 google-cloud-core==1.5.0 google-cloud-storage==1.35.0 google-crc32c==1.3.0 google-resumable-media==1.3.3 googleapis-common-protos==1.54.0 gpustat==0.6.0 gql==3.0.0a6 graphene==2.1.9 graphql-core==3.1.2 graphql-relay==2.0.1 greenlet==1.1.0 grpcio==1.39.0 gunicorn==20.1.0 h11==0.12.0 HeapDict==1.0.1 hiredis==2.0.0 httptools==0.1.2 idna==3.3 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==1.1.0 Jinja2==2.11.3 jmespath==0.10.0 jupyter==1.0.0 jupyter-client==6.1.12 jupyter-console==6.4.0 jupyterlab-widgets==1.0.2 jupytext==1.11.1 kubernetes==12.0.1 kubernetes-asyncio==12.1.0 libcst==0.3.23 lightgbm==3.3.1 llvmlite==0.37.0 locket==0.2.1 lz4==3.1.3 markdown-it-py==0.6.2 mdit-py-plugins==0.2.6 mlflow==1.17.0 modin==0.9.0 msgpack==1.0.3 multidict==5.2.0 multipledispatch==0.6.0 nbconvert==5.6.1 nbdev==1.1.14 numpy==1.21.1 nvidia-ml-py3==7.352.0 nvidia-pyindex==1.0.9 oauthlib==3.1.1 opencensus==0.8.0 opencensus-context==0.1.2 orjson==3.6.5 pamela==1.0.0 pandas==1.2.3 pandas-gbq==0.14.1 papermill==2.3.1 partd==1.2.0 pathspec==0.9.0 patsy==0.5.2 pdpyras==4.3.0 Pillow==8.3.1 platformdirs==2.4.1 prometheus-flask-exporter==0.18.7 promise==2.3 proto-plus==1.19.8 protobuf==3.17.3 psutil==5.9.0 ptvsd==4.3.2 py-spy==0.3.11 pyarrow==2.0.0 pyasn1==0.4.8 pyasn1-modules==0.2.8 pybigquery==0.5.0 pycosat==0.6.3 pycurl==7.44.1 pydantic==1.9.0 pydata-google-auth==1.3.0 python-dateutil==2.8.2 python-dotenv==0.15.0 python-editor==1.0.4 python-multipart==0.0.5 python-rapidjson==1.4 PyYAML==6.0 qtconsole==5.2.2 QtPy==2.0.0 querystring-parser==1.2.4 ray==1.5.2 redis==3.5.3 requests==2.27.0 requests-oauthlib==1.3.0 rsa==4.8 Rx==1.6.1 s3fs==2021.8.0 s3transfer==0.4.2 six==1.16.0 slack-sdk==3.4.2 slackclient==2.9.3 smmap==5.0.0 sortedcontainers==2.4.0 sqlparse==0.4.2 starlette==0.14.2 statsmodels==0.13.1 tblib==1.7.0 tenacity==8.0.1 termcolor==1.1.0 text-unidecode==1.3 textwrap3==0.9.2 tomli==1.2.3 toolz==0.11.2 tornado==6.1 tritonclient==2.13.0 typing-inspect==0.7.1 ujson==4.3.0 urllib3==1.26.7 uvicorn==0.13.4 uvloop==0.16.0 watchgod==0.7 webencodings==0.5.1 websocket-client==1.2.3 websockets==8.1 Werkzeug==2.0.1 widgetsnbextension==3.5.2 wrapt==1.13.3 xgboost==0.90 yarl==1.7.2 zict==2.0.0 zope.event==4.5.0 zope.interface==5.4.0","title":"Triton Model Serving"},{"location":"image_jobtype_packages.html#crypto","text":"absl-py==1.0.0 altair==4.2.0 anyio==3.4.0 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asgiref==3.4.1 astor==0.8.1 astunparse==1.6.3 attrs==21.4.0 backcall==0.2.0 backports.zoneinfo==0.2.1 base58==2.1.1 bleach==4.1.0 blinker==1.4 bokeh==2.4.1 brotlipy==0.7.0 cachetools==4.2.4 certifi==2021.5.30 click==7.1.2 conda==4.10.3 debugpy==1.5.1 decorator==5.1.0 defusedxml==0.7.1 entrypoints==0.3 fastapi==0.70.1 filelock==3.4.2 Flask==2.0.1 flatbuffers==2.0 gast==0.4.0 gevent==21.12.0 geventhttpclient==1.4.4 gitdb==4.0.9 GitPython==3.1.24 google-auth==2.3.3 google-auth-oauthlib==0.4.6 google-pasta==0.2.0 greenlet==1.1.2 grpcio==1.39.0 h11==0.12.0 h5py==3.6.0 huggingface-hub==0.0.12 importlib-metadata==4.10.0 importlib-resources==5.4.0 ipykernel==6.6.1 ipython==7.31.0 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jedi==0.18.1 Jinja2==3.0.3 joblib==1.1.0 jsonschema==4.3.3 jupyter-client==7.1.0 jupyter-core==4.9.1 jupyterlab-pygments==0.1.2 jupyterlab-widgets==1.0.2 keras==2.7.0 Keras-Preprocessing==1.1.2 libclang==12.0.0 Markdown==3.3.6 MarkupSafe==2.0.1 matplotlib-inline==0.1.3 mistune==0.8.4 nbclient==0.5.9 nbconvert==6.4.0 nbformat==5.1.3 nest-asyncio==1.5.4 notebook==6.4.6 numpy==1.22.0 oauthlib==3.1.1 opencv-python==4.5.5.62 opt-einsum==3.3.0 packaging==21.3 pandas==1.3.5 pandocfilters==1.5.0 parso==0.8.3 pexpect==4.8.0 pickleshare==0.7.5 Pillow==9.0.0 prometheus-client==0.12.0 prompt-toolkit==3.0.24 protobuf==3.19.1 ptyprocess==0.7.0 pyarrow==6.0.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycosat==0.6.3 pydantic==1.9.0 pydeck==0.7.1 Pygments==2.11.2 Pympler==1.0.1 pyparsing==3.0.6 pyrsistent==0.18.0 python-dateutil==2.8.2 python-rapidjson==1.5 pytz==2021.3 pytz-deprecation-shim==0.1.0.post0 PyYAML==6.0 pyzmq==22.3.0 regex==2021.11.10 requests-oauthlib==1.3.0 rsa==4.8 sacremoses==0.0.46 Send2Trash==1.8.0 smmap==5.0.0 sniffio==1.2.0 starlette==0.16.0 streamlit==1.3.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow==2.7.0 tensorflow-estimator==2.7.0 tensorflow-io-gcs-filesystem==0.23.1 termcolor==1.1.0 terminado==0.12.1 testpath==0.5.0 tokenizers==0.10.3 toml==0.10.2 toolz==0.11.2 torch==1.9.0 tornado==6.1 traitlets==5.1.1 transformers==4.9.2 tritonclient==2.17.0 typing-extensions==4.0.1 tzdata==2021.5 tzlocal==4.1 uvicorn==0.16.0 validators==0.18.2 watchdog==2.1.6 wcwidth==0.2.5 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 zipp==3.7.0 zope.event==4.5.0 zope.interface==5.4.0","title":"Crypto"},{"location":"image_jobtype_packages.html#climate","text":"This image contains the packages below in addition to everything in basic .","title":"Climate"},{"location":"image_jobtype_packages.html#conda-packages_1","text":"zarr=2.6.1 xclim=0.22.0 pangeo-notebook==0.0.2 xarray-spatial=0.1.2 intake=0.6.1 intake-esm=2020.12.18 rioxarray=0.3.1 cartopy=0.18.0 nc-time-axis=1.2.0 xesmf=0.5.2 seaborn=0.11.1","title":"conda packages"},{"location":"image_jobtype_packages.html#pip-packages_1","text":"h3==3.7.2 climate_indices==1.0.9","title":"pip packages"},{"location":"image_jobtype_packages.html#sas","text":"absl-py==1.0.0 altair==4.2.0 anyio==3.4.0 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asgiref==3.4.1 astor==0.8.1 astunparse==1.6.3 attrs==21.4.0 backcall==0.2.0 backports.zoneinfo==0.2.1 base58==2.1.1 bleach==4.1.0 blinker==1.4 bokeh==2.4.1 brotlipy==0.7.0 cachetools==4.2.4 certifi==2021.5.30 click==7.1.2 conda==4.10.3 debugpy==1.5.1 decorator==5.1.0 defusedxml==0.7.1 entrypoints==0.3 fastapi==0.70.1 filelock==3.4.2 Flask==2.0.1 flatbuffers==2.0 gast==0.4.0 gevent==21.12.0 geventhttpclient==1.4.4 gitdb==4.0.9 GitPython==3.1.24 google-auth==2.3.3 google-auth-oauthlib==0.4.6 google-pasta==0.2.0 greenlet==1.1.2 grpcio==1.39.0 h11==0.12.0 h5py==3.6.0 huggingface-hub==0.0.12 importlib-metadata==4.10.0 importlib-resources==5.4.0 ipykernel==6.6.1 ipython==7.31.0 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jedi==0.18.1 Jinja2==3.0.3 joblib==1.1.0 jsonschema==4.3.3 jupyter-client==7.1.0 jupyter-core==4.9.1 jupyterlab-pygments==0.1.2 jupyterlab-widgets==1.0.2 keras==2.7.0 Keras-Preprocessing==1.1.2 libclang==12.0.0 Markdown==3.3.6 MarkupSafe==2.0.1 matplotlib-inline==0.1.3 mistune==0.8.4 nbclient==0.5.9 nbconvert==6.4.0 nbformat==5.1.3 nest-asyncio==1.5.4 notebook==6.4.6 numpy==1.22.0 oauthlib==3.1.1 opencv-python==4.5.5.62 opt-einsum==3.3.0 packaging==21.3 pandas==1.3.5 pandocfilters==1.5.0 parso==0.8.3 pexpect==4.8.0 pickleshare==0.7.5 Pillow==9.0.0 prometheus-client==0.12.0 prompt-toolkit==3.0.24 protobuf==3.19.1 ptyprocess==0.7.0 pyarrow==6.0.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycosat==0.6.3 pydantic==1.9.0 pydeck==0.7.1 Pygments==2.11.2 Pympler==1.0.1 pyparsing==3.0.6 pyrsistent==0.18.0 python-dateutil==2.8.2 python-rapidjson==1.5 pytz==2021.3 pytz-deprecation-shim==0.1.0.post0 PyYAML==6.0 pyzmq==22.3.0 regex==2021.11.10 requests-oauthlib==1.3.0 rsa==4.8 sacremoses==0.0.46 Send2Trash==1.8.0 smmap==5.0.0 sniffio==1.2.0 starlette==0.16.0 streamlit==1.3.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow==2.7.0 tensorflow-estimator==2.7.0 tensorflow-io-gcs-filesystem==0.23.1 termcolor==1.1.0 terminado==0.12.1 testpath==0.5.0 tokenizers==0.10.3 toml==0.10.2 toolz==0.11.2 torch==1.9.0 tornado==6.1 traitlets==5.1.1 transformers==4.9.2 tritonclient==2.17.0 typing-extensions==4.0.1 tzdata==2021.5 tzlocal==4.1 uvicorn==0.16.0 validators==0.18.2 watchdog==2.1.6 wcwidth==0.2.5 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 zipp==3.7.0 zope.event==4.5.0 zope.interface==5.4.0","title":"SAS"},{"location":"image_jobtype_packages.html#r","text":"absl-py==1.0.0 altair==4.2.0 anyio==3.4.0 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asgiref==3.4.1 astor==0.8.1 astunparse==1.6.3 attrs==21.4.0 backcall==0.2.0 backports.zoneinfo==0.2.1 base58==2.1.1 bleach==4.1.0 blinker==1.4 bokeh==2.4.1 brotlipy==0.7.0 cachetools==4.2.4 certifi==2021.5.30 click==7.1.2 conda==4.10.3 debugpy==1.5.1 decorator==5.1.0 defusedxml==0.7.1 entrypoints==0.3 fastapi==0.70.1 filelock==3.4.2 Flask==2.0.1 flatbuffers==2.0 gast==0.4.0 gevent==21.12.0 geventhttpclient==1.4.4 gitdb==4.0.9 GitPython==3.1.24 google-auth==2.3.3 google-auth-oauthlib==0.4.6 google-pasta==0.2.0 greenlet==1.1.2 grpcio==1.39.0 h11==0.12.0 h5py==3.6.0 huggingface-hub==0.0.12 importlib-metadata==4.10.0 importlib-resources==5.4.0 ipykernel==6.6.1 ipython==7.31.0 ipython-genutils==0.2.0 ipywidgets==7.6.5 itsdangerous==2.0.1 jedi==0.18.1 Jinja2==3.0.3 joblib==1.1.0 jsonschema==4.3.3 jupyter-client==7.1.0 jupyter-core==4.9.1 jupyterlab-pygments==0.1.2 jupyterlab-widgets==1.0.2 keras==2.7.0 Keras-Preprocessing==1.1.2 libclang==12.0.0 Markdown==3.3.6 MarkupSafe==2.0.1 matplotlib-inline==0.1.3 mistune==0.8.4 nbclient==0.5.9 nbconvert==6.4.0 nbformat==5.1.3 nest-asyncio==1.5.4 notebook==6.4.6 numpy==1.22.0 oauthlib==3.1.1 opencv-python==4.5.5.62 opt-einsum==3.3.0 packaging==21.3 pandas==1.3.5 pandocfilters==1.5.0 parso==0.8.3 pexpect==4.8.0 pickleshare==0.7.5 Pillow==9.0.0 prometheus-client==0.12.0 prompt-toolkit==3.0.24 protobuf==3.19.1 ptyprocess==0.7.0 pyarrow==6.0.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycosat==0.6.3 pydantic==1.9.0 pydeck==0.7.1 Pygments==2.11.2 Pympler==1.0.1 pyparsing==3.0.6 pyrsistent==0.18.0 python-dateutil==2.8.2 python-rapidjson==1.5 pytz==2021.3 pytz-deprecation-shim==0.1.0.post0 PyYAML==6.0 pyzmq==22.3.0 regex==2021.11.10 requests-oauthlib==1.3.0 rsa==4.8 sacremoses==0.0.46 Send2Trash==1.8.0 smmap==5.0.0 sniffio==1.2.0 starlette==0.16.0 streamlit==1.3.1 tensorboard==2.7.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow==2.7.0 tensorflow-estimator==2.7.0 tensorflow-io-gcs-filesystem==0.23.1 termcolor==1.1.0 terminado==0.12.1 testpath==0.5.0 tokenizers==0.10.3 toml==0.10.2 toolz==0.11.2 torch==1.9.0 tornado==6.1 traitlets==5.1.1 transformers==4.9.2 tritonclient==2.17.0 typing-extensions==4.0.1 tzdata==2021.5 tzlocal==4.1 uvicorn==0.16.0 validators==0.18.2 watchdog==2.1.6 wcwidth==0.2.5 webencodings==0.5.1 Werkzeug==2.0.2 widgetsnbextension==3.5.2 wrapt==1.13.3 zipp==3.7.0 zope.event==4.5.0 zope.interface==5.4.0","title":"R"},{"location":"jwt.html","text":"Trigger Jobs Anywhere \u00b6 Shakudo Platform provide GraphQL queries to submit a job. With a JSON Web Token ( JWT ), one can submit jobs outside of the Shakudo Platform Platform, for example within your Air Flow pipeline or from the frontend of another product. Prerequisites \u00b6 Before you can trigger jobs outside of Shakudo Platform, we'll need to enable JWT on your Shakudo Platform; simply notify us and obtain the user name and password from the Shakudo support channel. An example \u00b6 Below is a sample script that submit a job using JWT outside of Shakudo Platform. You can try it out with the following steps: 1. Saving it as a Python script locally, for example ~/Downloads/my_queries.py . 2. On your terminal of your local machine run python my_queries.py , remember to change the placeholders in the code to approximate values. my_user_name and my_password can be obtained are your JWT credentials and path_to_your_yaml should be an existing yaml path in your repository. 3. If successful the code will return a job_id of the new job, if fail it'll return the error details 4. (optional) Go to the jobs pannel on the Shakudo Platform dashboard and find the newly submmited job with the returned job_id . \"\"\" Run this script outside of Shakudo Platform \"\"\" import requests from requests.exceptions import HTTPError import json import os def get_token (): \"\"\" function to generate JWT token \"\"\" token_endpoint = f \"https:// { HYPERPLANE_DOMAIN } /auth/realms/Hyperplane/protocol/openid-connect/token\" headers = { \"Content-Type\" : \"application/x-www-form-urlencoded\" } data = { \"grant_type\" : \"password\" , \"username\" : \"my_user_name\" , ## obtain this from shakudo team \"password\" : \"my_password\" , ## obtain this from shakudo team \"client_id\" : \"istio\" } try : res = requests . post ( token_endpoint , data = data , headers = headers ) . json () token = res [ 'access_token' ] print ( 'token obtained' ) except requests . exceptions . HTTPError as err : raise SystemExit ( err ) return token ## An example GraphQL query that submits a job. Can replace it with any GraphQL query query = \"\"\" mutation submitModelPipeline { createPipelineJob (data: { jobName: \"sample_job\", jobType: \"basic\", timeout: 3600, activeTimeout: 3600, maxRetries: 2, pipelineYamlPath: \"path_to_your_yaml.yml\", customTrigger: \"\" debuggable: false, parameters: { create: [ ]} }) { id status } } \"\"\" if __name__ == \"__main__\" : accessToken = get_token () endpoint = f \"https:// { HYPERPLANE_DOMAIN } /graphql\" headers = { \"Authorization\" : f \"Bearer { accessToken } \" } r = requests . post ( endpoint , json = { \"query\" : query }, headers = headers ) try : print ( 'query submitted sucessfully \\n ' ) print ( json . dumps ( r . json (), indent = 2 )) except HTTPError as e : print ( e . response . text ) raise Exception ( f \"Query failed to run with a { r . status_code } .\" ) Extension \u00b6 This script can be modified to submit any Shakudo Platform GraphQL queries by replacing the query string. Read more on available GraphQL queries on Shakudo Platform and try them out in the GraphQL playground, which is accessible through the Shakudo Platform Dashboard at https://[your_shakudo_name].hyperplane.dev/graphql","title":"Trigger Jobs anywhere"},{"location":"jwt.html#trigger-jobs-anywhere","text":"Shakudo Platform provide GraphQL queries to submit a job. With a JSON Web Token ( JWT ), one can submit jobs outside of the Shakudo Platform Platform, for example within your Air Flow pipeline or from the frontend of another product.","title":"Trigger Jobs Anywhere"},{"location":"jwt.html#prerequisites","text":"Before you can trigger jobs outside of Shakudo Platform, we'll need to enable JWT on your Shakudo Platform; simply notify us and obtain the user name and password from the Shakudo support channel.","title":"Prerequisites"},{"location":"jwt.html#an-example","text":"Below is a sample script that submit a job using JWT outside of Shakudo Platform. You can try it out with the following steps: 1. Saving it as a Python script locally, for example ~/Downloads/my_queries.py . 2. On your terminal of your local machine run python my_queries.py , remember to change the placeholders in the code to approximate values. my_user_name and my_password can be obtained are your JWT credentials and path_to_your_yaml should be an existing yaml path in your repository. 3. If successful the code will return a job_id of the new job, if fail it'll return the error details 4. (optional) Go to the jobs pannel on the Shakudo Platform dashboard and find the newly submmited job with the returned job_id . \"\"\" Run this script outside of Shakudo Platform \"\"\" import requests from requests.exceptions import HTTPError import json import os def get_token (): \"\"\" function to generate JWT token \"\"\" token_endpoint = f \"https:// { HYPERPLANE_DOMAIN } /auth/realms/Hyperplane/protocol/openid-connect/token\" headers = { \"Content-Type\" : \"application/x-www-form-urlencoded\" } data = { \"grant_type\" : \"password\" , \"username\" : \"my_user_name\" , ## obtain this from shakudo team \"password\" : \"my_password\" , ## obtain this from shakudo team \"client_id\" : \"istio\" } try : res = requests . post ( token_endpoint , data = data , headers = headers ) . json () token = res [ 'access_token' ] print ( 'token obtained' ) except requests . exceptions . HTTPError as err : raise SystemExit ( err ) return token ## An example GraphQL query that submits a job. Can replace it with any GraphQL query query = \"\"\" mutation submitModelPipeline { createPipelineJob (data: { jobName: \"sample_job\", jobType: \"basic\", timeout: 3600, activeTimeout: 3600, maxRetries: 2, pipelineYamlPath: \"path_to_your_yaml.yml\", customTrigger: \"\" debuggable: false, parameters: { create: [ ]} }) { id status } } \"\"\" if __name__ == \"__main__\" : accessToken = get_token () endpoint = f \"https:// { HYPERPLANE_DOMAIN } /graphql\" headers = { \"Authorization\" : f \"Bearer { accessToken } \" } r = requests . post ( endpoint , json = { \"query\" : query }, headers = headers ) try : print ( 'query submitted sucessfully \\n ' ) print ( json . dumps ( r . json (), indent = 2 )) except HTTPError as e : print ( e . response . text ) raise Exception ( f \"Query failed to run with a { r . status_code } .\" )","title":"An example"},{"location":"jwt.html#extension","text":"This script can be modified to submit any Shakudo Platform GraphQL queries by replacing the query string. Read more on available GraphQL queries on Shakudo Platform and try them out in the GraphQL playground, which is accessible through the Shakudo Platform Dashboard at https://[your_shakudo_name].hyperplane.dev/graphql","title":"Extension"},{"location":"mlflow_on_shakudo.html","text":"MLFlow on Shakudo Platform \u00b6 You can work with MLFlow on Shakudo Platform. You can see the mlflow dashboard.","title":"MLFlow on Shakudo Platform"},{"location":"mlflow_on_shakudo.html#mlflow-on-shakudo-platform","text":"You can work with MLFlow on Shakudo Platform. You can see the mlflow dashboard.","title":"MLFlow on Shakudo Platform"},{"location":"nc_utils.html","text":"Utilities for notebooks \u00b6 Utilities for notebooks are under the hyperplane.notebook_common module Initialize cluster \u00b6 initialize_cluster(num_workers:int=2, local_mode:bool=False, worker_spec_yaml:str=WORKER_SPEC_TEMPLATE_1_1, timeout:int=1200, nthreads:int=1, nprocs:int=15, ram_gb_per_proc:float=0.7, cores_per_worker:int=15, scheduler_deploy_mode:str=\"remote\", dashboard_port:str=\"random\", logging:str=\"quiet\" ): Parameters \u00b6 num_workers ( int , optional , defaults to 2) - Number of workers for you dask cluster local_mode ( bool , optional , defaults to False) - whether to use local cluster or distributed KubeCluster worker_spec_yaml ( str , optional , defaults to nc.WORKER_SPEC_TEMPLATE_1_1 ) - a string yaml for cluster configs timeout ( int , optional , defaults to 1200) - time limit (seconds) for a scheduler to wait to connect before returning a timeout error nthreads ( int , optional , defaults to 1) - Number of threads per worker in your cluster nprocs ( int , optional , defaults to 15) - Number of processes per worker in your cluster ram_gb_per_proc ( float , optional , defaults to 0.7) - gb of ram per process, per worker cores_per_worker ( int , optional , defaults to 15) - number of cores per worker scheduler_deploy_mode ( str , optional , defaults to \"remote\") - where to deploy the scheduler (remote in its own worker, or locally in jhub) dashboard_port ( str , optional , defaults to \"random\") - choose a port number for your dashboard, or leave as \"random\" to have a random port, which will not conflict logging ( str , optional , defaults to \"quiet\") - logging level for printouts when initializing. Available options are verbose or quiet . Returns \u00b6 Tuple[Client, KubeCluster] Post a Slack message \u00b6 notebook_common.post_message(msg: str, channel: str = \"\") Parameters \u00b6 msg ( str , required ) - String of message to send (can include formatting) channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel Returns \u00b6 requests.models.Response","title":"Utilities for notebooks"},{"location":"nc_utils.html#utilities-for-notebooks","text":"Utilities for notebooks are under the hyperplane.notebook_common module","title":"Utilities for notebooks"},{"location":"nc_utils.html#initialize-cluster","text":"initialize_cluster(num_workers:int=2, local_mode:bool=False, worker_spec_yaml:str=WORKER_SPEC_TEMPLATE_1_1, timeout:int=1200, nthreads:int=1, nprocs:int=15, ram_gb_per_proc:float=0.7, cores_per_worker:int=15, scheduler_deploy_mode:str=\"remote\", dashboard_port:str=\"random\", logging:str=\"quiet\" ):","title":"Initialize cluster"},{"location":"nc_utils.html#parameters","text":"num_workers ( int , optional , defaults to 2) - Number of workers for you dask cluster local_mode ( bool , optional , defaults to False) - whether to use local cluster or distributed KubeCluster worker_spec_yaml ( str , optional , defaults to nc.WORKER_SPEC_TEMPLATE_1_1 ) - a string yaml for cluster configs timeout ( int , optional , defaults to 1200) - time limit (seconds) for a scheduler to wait to connect before returning a timeout error nthreads ( int , optional , defaults to 1) - Number of threads per worker in your cluster nprocs ( int , optional , defaults to 15) - Number of processes per worker in your cluster ram_gb_per_proc ( float , optional , defaults to 0.7) - gb of ram per process, per worker cores_per_worker ( int , optional , defaults to 15) - number of cores per worker scheduler_deploy_mode ( str , optional , defaults to \"remote\") - where to deploy the scheduler (remote in its own worker, or locally in jhub) dashboard_port ( str , optional , defaults to \"random\") - choose a port number for your dashboard, or leave as \"random\" to have a random port, which will not conflict logging ( str , optional , defaults to \"quiet\") - logging level for printouts when initializing. Available options are verbose or quiet .","title":"Parameters"},{"location":"nc_utils.html#returns","text":"Tuple[Client, KubeCluster]","title":"Returns"},{"location":"nc_utils.html#post-a-slack-message","text":"notebook_common.post_message(msg: str, channel: str = \"\")","title":"Post a Slack message"},{"location":"nc_utils.html#parameters_1","text":"msg ( str , required ) - String of message to send (can include formatting) channel ( str , optional , defaults to \"\") - a public channel to post the msg . If left empty, it will default to the webhook url's default channel","title":"Parameters"},{"location":"nc_utils.html#returns_1","text":"requests.models.Response","title":"Returns"},{"location":"nlp_hyperplane.html","text":"Using NLP models on Shakudo Platform \u00b6 Shakudo Platform comes with some pre-packged NLP models for common tasks. The Shakudo Platform nlp tools can be accessed through from hyperplane import hyper_nlp as nlp Discover topics and themes \u00b6 Given a list of sentences, documents, or texts, you can find the main topics % matplotlib inline ## use this in a notebook to display charts nlp . get_topics ([ \"I need to buy some green and red apples.\" , \"Oranges are also useful for juices\" , \"I really like bananas and fruits\" , \"You need to clean up your car.\" , \"I am running out of out of gas\" ], ntopic = 2 , sample_size = 50 , method = \"USE\" ) will return the topic model and a list of top words per cluster: (<topic_discovery.Topic_Model at 0x7f88abc8e9a0>, array([['out', 'of', 'I', 'am', 'running', 'gas', 'You', 'need', 'to', 'clean'], ['I', 'and', 'Oranges', 'are', 'also', 'useful', 'for', 'juices', 'need', 'to']], dtype=object)) Text extraction \u00b6 Given some text, you can ask a question to extract an arbitrary field. For example, text = ''' Shakudo Platform is an end-to-end platform designed to take AI teams from ideation to production at breakthrough speeds. We built Shakudo Platform because we needed a powerful platform for our scientists to design, develop, deploy and maintain their own work in production. Why Us? The Shakudo team grew out of advanced AI organizations across the industry. After having seen, tried and used every product out there, we came to the conclusion that there is a gap to be filled, and Shakudo Platform was born. What does this mean for you? If you are scaling up an AI organization, starting up an AI-powered product, or looking to get your existing solutions faster and more reliably to production, Shakudo Platform may be for you. ''' questions = [ \"What does Shakudo Platform do?\" ] nlp . extract_qa ( text , questions [ 0 ]) will return [{'score': 0.04723832756280899, 'start': 51, 'end': 115, 'answer': 'take AI teams from ideation to production at breakthrough speeds'}] For more details on Shakudo Platform nlp tools, see Shakudo Platform NLP Utilities Save and load \u00b6 To save serializable models, you can use the following: nlp . save_model ( model = tm , filename = \"tm_model\" ) where tm is a serializable object. To load an existing model saved with hyper_nlp, use the following: loaded_model = nlp . load_model ( filename = 'tm_model' ) Other \u00b6 \u00b6","title":"NLP on Shakudo Platform"},{"location":"nlp_hyperplane.html#using-nlp-models-on-shakudo-platform","text":"Shakudo Platform comes with some pre-packged NLP models for common tasks. The Shakudo Platform nlp tools can be accessed through from hyperplane import hyper_nlp as nlp","title":"Using NLP models on Shakudo Platform"},{"location":"nlp_hyperplane.html#discover-topics-and-themes","text":"Given a list of sentences, documents, or texts, you can find the main topics % matplotlib inline ## use this in a notebook to display charts nlp . get_topics ([ \"I need to buy some green and red apples.\" , \"Oranges are also useful for juices\" , \"I really like bananas and fruits\" , \"You need to clean up your car.\" , \"I am running out of out of gas\" ], ntopic = 2 , sample_size = 50 , method = \"USE\" ) will return the topic model and a list of top words per cluster: (<topic_discovery.Topic_Model at 0x7f88abc8e9a0>, array([['out', 'of', 'I', 'am', 'running', 'gas', 'You', 'need', 'to', 'clean'], ['I', 'and', 'Oranges', 'are', 'also', 'useful', 'for', 'juices', 'need', 'to']], dtype=object))","title":"Discover topics and themes"},{"location":"nlp_hyperplane.html#text-extraction","text":"Given some text, you can ask a question to extract an arbitrary field. For example, text = ''' Shakudo Platform is an end-to-end platform designed to take AI teams from ideation to production at breakthrough speeds. We built Shakudo Platform because we needed a powerful platform for our scientists to design, develop, deploy and maintain their own work in production. Why Us? The Shakudo team grew out of advanced AI organizations across the industry. After having seen, tried and used every product out there, we came to the conclusion that there is a gap to be filled, and Shakudo Platform was born. What does this mean for you? If you are scaling up an AI organization, starting up an AI-powered product, or looking to get your existing solutions faster and more reliably to production, Shakudo Platform may be for you. ''' questions = [ \"What does Shakudo Platform do?\" ] nlp . extract_qa ( text , questions [ 0 ]) will return [{'score': 0.04723832756280899, 'start': 51, 'end': 115, 'answer': 'take AI teams from ideation to production at breakthrough speeds'}] For more details on Shakudo Platform nlp tools, see Shakudo Platform NLP Utilities","title":"Text extraction"},{"location":"nlp_hyperplane.html#save-and-load","text":"To save serializable models, you can use the following: nlp . save_model ( model = tm , filename = \"tm_model\" ) where tm is a serializable object. To load an existing model saved with hyper_nlp, use the following: loaded_model = nlp . load_model ( filename = 'tm_model' )","title":"Save and load"},{"location":"nlp_hyperplane.html#other","text":"","title":"Other"},{"location":"nlp_hyperplane.html#_1","text":"","title":""},{"location":"nlp_utils.html","text":"Utilities for NLP \u00b6 Shakudo Platform NLP tools are are under the hyperplane.hyper_nlp module. from hyperplane import hyper_nlp as nlp Topic extraction \u00b6 get_topics(data:List[str], ntopic:int=10, sample_size:int=50000, method:str=\"USE\" ) Parameters \u00b6 data ( List[str] , required ) - A list of strings (sentences or phrases) from which you want to discover topics and themes ntopic ( int , optional ) - The number of topics/themes to return sample_size ( int , optional ) - The number of samples to discover topics from method ( str , optional ) - The method for theme extraction. Choose from TFIDF , LDA , BERT , LDA_BERT , fasttext , USE Returns \u00b6 Topic_Model, List[str] Extract text \u00b6 extract_qa( text:str, question:str, topk:int=1, return_context:bool=False, context_window:int=20) Extractive question-answering based on context. Parameters \u00b6 text ( str , required ) - Context string which the answer will be extracted from question ( str , required ) - A question that you want to ask based on the context topk ( int , optional ) - The number of top answers to return per question return_context ( bool , optional ) - Whether or not to return context around the answer context_window ( int , optional ) - If return_context , how much of the context to return Returns \u00b6 List[Dict] Clean text \u00b6 Find urls \u00b6 find_urls(s:str) Parameters \u00b6 s ( str , required ) - A string which you would like to search for urls Returns \u00b6 List[str] Remove stopwords \u00b6 clean_text(s:str, remove_list:List[str]=en_stop_words) Parameters \u00b6 s ( str , required ) - A string which you would like to clean by removing stopwords remove_list ( List[str] , optional ) - A list of strings to include as stopwords to remove Returns \u00b6 str Extract text from pdfs \u00b6 extract_digital_pdf(filepath:str, auto_clean_threshold:int=0) Parameters \u00b6 filepath ( str , required ) - A filepath location for the digital pdf to extract auto_clean_threshold ( int , optional ) - A threshold for removing words that are too short. Keep at 0 to keep all words that are not stopwords. Use any other positive integer to remove words containing fewer than auto_clean_threshold letters. Returns \u00b6 str Topic Discovery \u00b6","title":"Utilities for NLP"},{"location":"nlp_utils.html#utilities-for-nlp","text":"Shakudo Platform NLP tools are are under the hyperplane.hyper_nlp module. from hyperplane import hyper_nlp as nlp","title":"Utilities for NLP"},{"location":"nlp_utils.html#topic-extraction","text":"get_topics(data:List[str], ntopic:int=10, sample_size:int=50000, method:str=\"USE\" )","title":"Topic extraction"},{"location":"nlp_utils.html#parameters","text":"data ( List[str] , required ) - A list of strings (sentences or phrases) from which you want to discover topics and themes ntopic ( int , optional ) - The number of topics/themes to return sample_size ( int , optional ) - The number of samples to discover topics from method ( str , optional ) - The method for theme extraction. Choose from TFIDF , LDA , BERT , LDA_BERT , fasttext , USE","title":"Parameters"},{"location":"nlp_utils.html#returns","text":"Topic_Model, List[str]","title":"Returns"},{"location":"nlp_utils.html#extract-text","text":"extract_qa( text:str, question:str, topk:int=1, return_context:bool=False, context_window:int=20) Extractive question-answering based on context.","title":"Extract text"},{"location":"nlp_utils.html#parameters_1","text":"text ( str , required ) - Context string which the answer will be extracted from question ( str , required ) - A question that you want to ask based on the context topk ( int , optional ) - The number of top answers to return per question return_context ( bool , optional ) - Whether or not to return context around the answer context_window ( int , optional ) - If return_context , how much of the context to return","title":"Parameters"},{"location":"nlp_utils.html#returns_1","text":"List[Dict]","title":"Returns"},{"location":"nlp_utils.html#clean-text","text":"","title":"Clean text"},{"location":"nlp_utils.html#find-urls","text":"find_urls(s:str)","title":"Find urls"},{"location":"nlp_utils.html#parameters_2","text":"s ( str , required ) - A string which you would like to search for urls","title":"Parameters"},{"location":"nlp_utils.html#returns_2","text":"List[str]","title":"Returns"},{"location":"nlp_utils.html#remove-stopwords","text":"clean_text(s:str, remove_list:List[str]=en_stop_words)","title":"Remove stopwords"},{"location":"nlp_utils.html#parameters_3","text":"s ( str , required ) - A string which you would like to clean by removing stopwords remove_list ( List[str] , optional ) - A list of strings to include as stopwords to remove","title":"Parameters"},{"location":"nlp_utils.html#returns_3","text":"str","title":"Returns"},{"location":"nlp_utils.html#extract-text-from-pdfs","text":"extract_digital_pdf(filepath:str, auto_clean_threshold:int=0)","title":"Extract text from pdfs"},{"location":"nlp_utils.html#parameters_4","text":"filepath ( str , required ) - A filepath location for the digital pdf to extract auto_clean_threshold ( int , optional ) - A threshold for removing words that are too short. Keep at 0 to keep all words that are not stopwords. Use any other positive integer to remove words containing fewer than auto_clean_threshold letters.","title":"Parameters"},{"location":"nlp_utils.html#returns_4","text":"str","title":"Returns"},{"location":"nlp_utils.html#topic-discovery","text":"","title":"Topic Discovery"},{"location":"podspecs.html","text":"Create and use custom PodSpecs in Shakudo Platform \u00b6 Prerequisites \u00b6 kubectl + kubeconfig access to Shakudo Platform access to Shakudo Platform dashboard Create your custom image \u00b6 Now that you have the basic credentials to create a custom PodSpec, you can follow the steps below. PodSpec Fields Create a custom PodSpec using the dashboard and GraphQL Create custom PodSpec using kubectl PodSpec Fields \u00b6 @param displayName: The name of the PodSpec which is display on the dashboard @param podspecName: The name of the PodSpec use on Kubernetes - this name has to be unique @param description: A short description of what this PodSpec is @param icon: An icon which best describes your custom PodSpec @param podSpec: A YAML of the podSpec - this will be created automatically @param hyperplaneUserEmail: The user email of the user creating the PodSpec - this will be set automatically @param volumes: @param volumeMounts: Create a custom PodSpec using the dashboard \u00b6 You can navigate to the PodSpec panel on the left-side menu of the Shakudo Platform dashboard. The below example shows the required fields being populated and the graphQL mutation and YAML is created automatically to allow users to also create the PodSpec via an API call or using kubectl. Required Fields *displayName * *podspecName : (This field is populated automatically, however users can edit the field. This field should be unique and no two PodSpec's can share the same podspecName *) *description * *icon * Resources The PodSpec's resources are automatically populated by the UI using two environment variables that specify allocatable memory and CPU - however users can set these to their desired values: HYPERPLANE_ROOT_ALLOCATABLE_CPU HYPERPLANE_ROOT_ALLOCATABLE_RAM Node Selector This value chooses which node each session/job using this PodSpec will run on. For cloud customers this is pre-selected. However, for on-prem customers they can have certain nodes which they can select to run on. Volumes & Volumes Mounts Additional volumes for your sessions and jobs; see https://kubernetes.io/docs/concepts/storage/volumes/ Environment Variables The environment variables will be pre-set for any environment using the created PodSpec to run. By clicking on the YAML button on the top left side of the screen you also get access to the YAML used to create the hyperplanepodspec object. This can be copied over to a file and you can create the PodSpec using kubectl. Create custom PodSpec using kubectl \u00b6 When you want to create a custom PodSpec using kubectl, you can create the YAML automatically using the dashboard and copy over the YAML to a file and apply that file using the following command: kubectl apply -f <name_of_yaml_file> This will automatically create the hyperplanepodspec object on Kubernetes.","title":"Create and use custom PodSpecs in Shakudo Platform"},{"location":"podspecs.html#create-and-use-custom-podspecs-in-shakudo-platform","text":"","title":"Create and use custom PodSpecs in Shakudo Platform"},{"location":"podspecs.html#prerequisites","text":"kubectl + kubeconfig access to Shakudo Platform access to Shakudo Platform dashboard","title":"Prerequisites"},{"location":"podspecs.html#create-your-custom-image","text":"Now that you have the basic credentials to create a custom PodSpec, you can follow the steps below. PodSpec Fields Create a custom PodSpec using the dashboard and GraphQL Create custom PodSpec using kubectl","title":"Create your custom image"},{"location":"podspecs.html#podspec-fields","text":"@param displayName: The name of the PodSpec which is display on the dashboard @param podspecName: The name of the PodSpec use on Kubernetes - this name has to be unique @param description: A short description of what this PodSpec is @param icon: An icon which best describes your custom PodSpec @param podSpec: A YAML of the podSpec - this will be created automatically @param hyperplaneUserEmail: The user email of the user creating the PodSpec - this will be set automatically @param volumes: @param volumeMounts:","title":"PodSpec Fields"},{"location":"podspecs.html#create-a-custom-podspec-using-the-dashboard","text":"You can navigate to the PodSpec panel on the left-side menu of the Shakudo Platform dashboard. The below example shows the required fields being populated and the graphQL mutation and YAML is created automatically to allow users to also create the PodSpec via an API call or using kubectl. Required Fields *displayName * *podspecName : (This field is populated automatically, however users can edit the field. This field should be unique and no two PodSpec's can share the same podspecName *) *description * *icon * Resources The PodSpec's resources are automatically populated by the UI using two environment variables that specify allocatable memory and CPU - however users can set these to their desired values: HYPERPLANE_ROOT_ALLOCATABLE_CPU HYPERPLANE_ROOT_ALLOCATABLE_RAM Node Selector This value chooses which node each session/job using this PodSpec will run on. For cloud customers this is pre-selected. However, for on-prem customers they can have certain nodes which they can select to run on. Volumes & Volumes Mounts Additional volumes for your sessions and jobs; see https://kubernetes.io/docs/concepts/storage/volumes/ Environment Variables The environment variables will be pre-set for any environment using the created PodSpec to run. By clicking on the YAML button on the top left side of the screen you also get access to the YAML used to create the hyperplanepodspec object. This can be copied over to a file and you can create the PodSpec using kubectl.","title":"Create a custom PodSpec using the dashboard"},{"location":"podspecs.html#create-custom-podspec-using-kubectl","text":"When you want to create a custom PodSpec using kubectl, you can create the YAML automatically using the dashboard and copy over the YAML to a file and apply that file using the following command: kubectl apply -f <name_of_yaml_file> This will automatically create the hyperplanepodspec object on Kubernetes.","title":"Create custom PodSpec using kubectl"},{"location":"pullrequest.html","text":"Using Jupyterlab Pullrequests \u00b6 Shakudo Platform now supports jupyterlab pullrequests. Jupyterlab pullrequests is a JupyterLab extension for reviewing pull requests. Setup \u00b6 Setup for Jupyterlab pullrequests is quite simple. As it's already pre-installed, all you need to do is navigate to ~/.jupyter/jupyter_notebook_config.py and add the following line. c.PRConfig.access_token = '<github_personal_access_token>' This is your access token from Github, which is now used in place of a password. Instructions to generate one can be found here: Github PAT Usage \u00b6 Now that it's been setup, you should see a version control icon on the left hand side toolbar in Jupyterlab. Clicking on this after navigating to a git repository will show a a host of actions that can be taken, including making new commits, keep track of changes, looks at diffs in pullrequests, and much more.","title":"Jupyterlab Pullrequests"},{"location":"pullrequest.html#using-jupyterlab-pullrequests","text":"Shakudo Platform now supports jupyterlab pullrequests. Jupyterlab pullrequests is a JupyterLab extension for reviewing pull requests.","title":"Using Jupyterlab Pullrequests"},{"location":"pullrequest.html#setup","text":"Setup for Jupyterlab pullrequests is quite simple. As it's already pre-installed, all you need to do is navigate to ~/.jupyter/jupyter_notebook_config.py and add the following line. c.PRConfig.access_token = '<github_personal_access_token>' This is your access token from Github, which is now used in place of a password. Instructions to generate one can be found here: Github PAT","title":"Setup"},{"location":"pullrequest.html#usage","text":"Now that it's been setup, you should see a version control icon on the left hand side toolbar in Jupyterlab. Clicking on this after navigating to a git repository will show a a host of actions that can be taken, including making new commits, keep track of changes, looks at diffs in pullrequests, and much more.","title":"Usage"},{"location":"r_on_hyperplane.html","text":"R on Shakudo Platform \u00b6 Shakudo Platform's R image allows you to run your R code in jupyter notebooks. Set up \u00b6 Start by choosing the R image R Kernel \u00b6 Once you have chosen the R kernel, you can copy and paste your R code into jupyter notebooks and run as normal.","title":"R"},{"location":"r_on_hyperplane.html#r-on-shakudo-platform","text":"Shakudo Platform's R image allows you to run your R code in jupyter notebooks.","title":"R on Shakudo Platform"},{"location":"r_on_hyperplane.html#set-up","text":"Start by choosing the R image","title":"Set up"},{"location":"r_on_hyperplane.html#r-kernel","text":"Once you have chosen the R kernel, you can copy and paste your R code into jupyter notebooks and run as normal.","title":"R Kernel"},{"location":"rapids_on_hyperplane.html","text":"RAPIDS and GPU on Shakudo Platform \u00b6 RAPIDS can significantly speed up long-running preprocessing loads. Set up \u00b6 Start by choosing the GPU image Initialize cluster with GPUs \u00b6 On the Shakudo Platform GPU image, you can scale up a Dask cluster with GPUs by adding ngpus=1 to the cluster initialization. client, cluster = nc.initialize_cluster( nprocs=1, nthreads=8, ram_gb_per_proc=7, cores_per_worker=2, num_workers = 2, ngpus = 1, scheduler_deploy_mode=\"local\" ) To use Dask with gpu, import dask_cudf , for example df = dask_cudf.read_csv(file_path, assume_missing=True) RAPIDS \u00b6 Use cuml with Shakudo Platform's GPU image to take advantage of RAPIDS.","title":"RAPIDS"},{"location":"rapids_on_hyperplane.html#rapids-and-gpu-on-shakudo-platform","text":"RAPIDS can significantly speed up long-running preprocessing loads.","title":"RAPIDS and GPU on Shakudo Platform"},{"location":"rapids_on_hyperplane.html#set-up","text":"Start by choosing the GPU image","title":"Set up"},{"location":"rapids_on_hyperplane.html#initialize-cluster-with-gpus","text":"On the Shakudo Platform GPU image, you can scale up a Dask cluster with GPUs by adding ngpus=1 to the cluster initialization. client, cluster = nc.initialize_cluster( nprocs=1, nthreads=8, ram_gb_per_proc=7, cores_per_worker=2, num_workers = 2, ngpus = 1, scheduler_deploy_mode=\"local\" ) To use Dask with gpu, import dask_cudf , for example df = dask_cudf.read_csv(file_path, assume_missing=True)","title":"Initialize cluster with GPUs"},{"location":"rapids_on_hyperplane.html#rapids","text":"Use cuml with Shakudo Platform's GPU image to take advantage of RAPIDS.","title":"RAPIDS"},{"location":"ray_hyperplane.html","text":"Working with Ray \u00b6 Shakudo Platform provides simple APIs to start and shutdown distributed Ray clusters. Initializing a distributed Ray cluster \u00b6 Quickstart a distributed Ray cluster using the following. from hyperplane.ray_common import quickstart_ray ray_cluster = quickstart_ray ( num_workers = 4 , size = 'hyperplane-med-high-mem' ) Initialize a distributed Ray cluster with ease and more customizability using the following: from hyperplane.ray_common import initialize_ray_cluster ray_cluster = initialize_ray_cluster ( num_workers = 4 , cpu_core_per_worker = 4 , ram_gb_per_worker = 4 , n_gpus = 0 ) num_workers (int) is the number of Ray nodes to be initialized cpu_core_per_worker (int) is the number of CPU cores in each Ray node ram_gb_per_worker (float) is the memory size in GB for each Ray node n_gpus (int) is the number of Nvidia GPUs in each Ray node (if n_gpus > 0 , cpu_core_per_worker and ram_gb_per_worker are ignored) Choosing a Ray cluster config \u00b6 Shakudo Platform currently comes with nine pre-configured worker pools: 'hyperplane-xs-high-mem' (POOL_4_32) with 3.5 allocatable cores, and 27.0 allocatable ram 'hyperplane-small' (POOL_8_8) with 7.0 allocatable cores, and 5.0 allocatable ram 'hyperplane-small-mid-mem' (POOL_8_16) with 7.5 allocatable cores, and 12.0 allocatable ram 'hyperplane-small-high-mem' (POOL_8_64) with 7.5 allocatable cores, and 58.0 allocatable ram 'hyperplane-med' (POOL_16_16) with 15.0 allocatable cores, and 12.0 allocatable ram 'hyperplane-med-mid-mem' (POOL_16_32) with 15.0 allocatable cores, and 27.0 allocatable ram 'hyperplane-med-high-mem' (POOL_16_128) with 15.0 allocatable cores, and 110.0 allocatable ram 'hyperplane-large' (POOL_32_32) with 28.0 allocatable cores, and 27.0 allocatable ram 'hyperplane-xxl-high-mem' (POOL_96_768) with 94.0 allocatable cores, and 675.0 allocatable ram Once you specify your number of cores and memory, Shakudo Platform will automatically choose the most appropriate pool from the above. You can also choose one of the above with the quickstart_ray function. If you are aiming for a specific pool, ensure your cpu_core_per_worker == the number of allocatable cores and ram_gb_per_worker == the allocatable ram. For example, if you would like to use a POOL_16_16 worker, you may want to use the following cluster initalization ray_cluster = initialize_ray_cluster ( num_workers = 4 , cpu_core_per_worker = 15 , ram_gb_per_worker = 12 ) Shutdown a Ray cluster \u00b6 After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Jupyeter session or job is finished. You can add this line to the end of your code to shutdown the Ray nodes. stop_ray_cluster ( ray_cluster ) Reconnecting to a Ray cluster \u00b6 If you've already spun up a Ray cluster and want to connect to the same cluster (for example: in another notebook on the same session), simply run the get_ray_cluster function. This function will connect to an existing cluster. The extra_workers parameter specifies whether you want to add nodes to your existing cluster ( extra_workers = 0 by default). The nodes that are added to the cluster will be of the same specification as the original cluster. If you want to clean up the Ray nodes and re-initialize, simply run the initialize_ray_cluster function, as detailed above. So, now, if we wanted to connect to the cluster we initialized above in a new notebook, we can do so in one of two ways. The simpler and recommended way is the following: get_ray_cluster () However, you can also use the initialize_ray_cluster to accomplish the same. Note, the arguments for cpu_core_per_worker and ram_gb_per_worker must be the same as when you initialized the cluster originally. initialize_ray_cluster ( num_workers = 0 , cpu_core_per_worker = 15 , ram_gb_per_worker = 12 , use_existing = True ) In order to connect to an existing Ray cluster and add 2 workers to it, we can do that as follows: get_ray_cluster ( extra_workers = 2 ) Checkout this example of using Ray to train lightGBM on 250G of data on Shakudo Platform. Ray Datasets efficiently loaded 250G of data into the distributed nodes and Ray Tune seamlessly performed distributed hyperparameter optimization for lightGBM.","title":"Ray on Shakudo Platform"},{"location":"ray_hyperplane.html#working-with-ray","text":"Shakudo Platform provides simple APIs to start and shutdown distributed Ray clusters.","title":"Working with Ray"},{"location":"ray_hyperplane.html#initializing-a-distributed-ray-cluster","text":"Quickstart a distributed Ray cluster using the following. from hyperplane.ray_common import quickstart_ray ray_cluster = quickstart_ray ( num_workers = 4 , size = 'hyperplane-med-high-mem' ) Initialize a distributed Ray cluster with ease and more customizability using the following: from hyperplane.ray_common import initialize_ray_cluster ray_cluster = initialize_ray_cluster ( num_workers = 4 , cpu_core_per_worker = 4 , ram_gb_per_worker = 4 , n_gpus = 0 ) num_workers (int) is the number of Ray nodes to be initialized cpu_core_per_worker (int) is the number of CPU cores in each Ray node ram_gb_per_worker (float) is the memory size in GB for each Ray node n_gpus (int) is the number of Nvidia GPUs in each Ray node (if n_gpus > 0 , cpu_core_per_worker and ram_gb_per_worker are ignored)","title":"Initializing a distributed Ray cluster"},{"location":"ray_hyperplane.html#choosing-a-ray-cluster-config","text":"Shakudo Platform currently comes with nine pre-configured worker pools: 'hyperplane-xs-high-mem' (POOL_4_32) with 3.5 allocatable cores, and 27.0 allocatable ram 'hyperplane-small' (POOL_8_8) with 7.0 allocatable cores, and 5.0 allocatable ram 'hyperplane-small-mid-mem' (POOL_8_16) with 7.5 allocatable cores, and 12.0 allocatable ram 'hyperplane-small-high-mem' (POOL_8_64) with 7.5 allocatable cores, and 58.0 allocatable ram 'hyperplane-med' (POOL_16_16) with 15.0 allocatable cores, and 12.0 allocatable ram 'hyperplane-med-mid-mem' (POOL_16_32) with 15.0 allocatable cores, and 27.0 allocatable ram 'hyperplane-med-high-mem' (POOL_16_128) with 15.0 allocatable cores, and 110.0 allocatable ram 'hyperplane-large' (POOL_32_32) with 28.0 allocatable cores, and 27.0 allocatable ram 'hyperplane-xxl-high-mem' (POOL_96_768) with 94.0 allocatable cores, and 675.0 allocatable ram Once you specify your number of cores and memory, Shakudo Platform will automatically choose the most appropriate pool from the above. You can also choose one of the above with the quickstart_ray function. If you are aiming for a specific pool, ensure your cpu_core_per_worker == the number of allocatable cores and ram_gb_per_worker == the allocatable ram. For example, if you would like to use a POOL_16_16 worker, you may want to use the following cluster initalization ray_cluster = initialize_ray_cluster ( num_workers = 4 , cpu_core_per_worker = 15 , ram_gb_per_worker = 12 )","title":"Choosing a Ray cluster config"},{"location":"ray_hyperplane.html#shutdown-a-ray-cluster","text":"After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Jupyeter session or job is finished. You can add this line to the end of your code to shutdown the Ray nodes. stop_ray_cluster ( ray_cluster )","title":"Shutdown a Ray cluster"},{"location":"ray_hyperplane.html#reconnecting-to-a-ray-cluster","text":"If you've already spun up a Ray cluster and want to connect to the same cluster (for example: in another notebook on the same session), simply run the get_ray_cluster function. This function will connect to an existing cluster. The extra_workers parameter specifies whether you want to add nodes to your existing cluster ( extra_workers = 0 by default). The nodes that are added to the cluster will be of the same specification as the original cluster. If you want to clean up the Ray nodes and re-initialize, simply run the initialize_ray_cluster function, as detailed above. So, now, if we wanted to connect to the cluster we initialized above in a new notebook, we can do so in one of two ways. The simpler and recommended way is the following: get_ray_cluster () However, you can also use the initialize_ray_cluster to accomplish the same. Note, the arguments for cpu_core_per_worker and ram_gb_per_worker must be the same as when you initialized the cluster originally. initialize_ray_cluster ( num_workers = 0 , cpu_core_per_worker = 15 , ram_gb_per_worker = 12 , use_existing = True ) In order to connect to an existing Ray cluster and add 2 workers to it, we can do that as follows: get_ray_cluster ( extra_workers = 2 ) Checkout this example of using Ray to train lightGBM on 250G of data on Shakudo Platform. Ray Datasets efficiently loaded 250G of data into the distributed nodes and Ray Tune seamlessly performed distributed hyperparameter optimization for lightGBM.","title":"Reconnecting to a Ray cluster"},{"location":"sandbox.html","text":"Demo/ sandbox walkthrough \u00b6 Shakudo Platform enables you to build pipelines from VSCode notebooks, python files, or Jupyter notebooks. In this tutorial, we will walk you through the sandbox/demo environment, where you can start up a Jupyter instance, scale up a Dask cluster, and run an existing notebook in a pipeline. You can find the full example here 1. Start your jhub instance \u00b6 Start a session by clicking on JuperServer or going on the Sessions Page. For more details on the pre-configed environment, please checkout Sessions . You can now start using your Jupyter instance. 2. Open a notebook \u00b6 Open a notebook with the Launcher. Choose the Python [conda env:root]* kernel. 3. Process data in Pandas \u00b6 The example we are going to use will use be a group-by task on a public flight dataset in AWS S3 bucket. The dataset has 22 CSV files and is 11GB in total. import pandas as pd df_pd = pd . read_csv ( \"s3://dask-data/airline-data/1990.csv\" , usecols = [ 'DepTime' , 'FlightNum' , 'DepDelay' , 'Origin' , 'Dest' , 'Distance' ]) df_sort_pd = df_pd . groupby ( 'Origin' ) . apply ( lambda x : x . nlargest ( n = 10 , columns = 'Distance' )) The 1990.csv data shape is (5270893, 6) , the processing time of the above operation is 21s on a 16 CPU 16G RAM machine. Now we want to scale up the above computation for 10 CSV files for 1990 to 1999. On one 16 CPU 16G RAM machine it will cause an out of memory error and crash the kernel. To process the files one by one, in total the operation is estimated to take 3.5 minutes . 4. Spin up a Dask cluster \u00b6 Now let's spin up a distributed Dask cluster and speed up the computation! The easiest way to get started is by using the notebook_common function to spin up a pre-configured Dask cluster. You can specify the number of workers with argument num_workers or specify more specs to better fit your computation. Shakudo Platform will automatically choose a cluster configuration for you and provides a Dask dashboard link to monitor progress. from hyperplane import notebook_common as nc client , cluster = nc . initialize_cluster ( nprocs = 5 , nthreads = 3 , ram_gb_per_proc = 2.4 , cores_per_worker = 15 , scheduler_deploy_mode = \"remote\" , num_workers = 3 ) You will be able to see the spinning up logs of the Dask cluster and the link to the Dask dashboard. \ud83d\udc49 Shakudo Platform: selecting worker node pool \ud83d\udc49 Shakudo Platform: selecting scheduler node pool Creating scheduler pod on cluster. This may take some time. \ud83d\udc49 Shakudo Platform: spinning up a dask cluster with a scheduler as a standalone container. \ud83d\udc49 Shakudo Platform: In a few minutes you'll be able to access the dashboard at https://ds.hyperplane.dev/dask-cluster-e002f3d0-b18d-4027-81c5-bed613eb63a4/status \ud83d\udc49 Shakudo Platform: to get logs from all workers, do `cluster.get_logs()` 5. Process data in Dask \u00b6 To run the above code on a Dask cluster, we just need to swap the Pandas API to Dask API, which is very similar. Dask does lazy computation, the last line df_sort.compute> triggers the computation. You can find information on the Dask concepts and Dask best practices page. For full Dask documentation, check out https://docs.dask.org/en/ df = dd . read_csv ( data_url , storage_options = { 'anon' : True }, usecols = [ 'DepTime' , 'FlightNum' , 'DepDelay' , 'Origin' , 'Dest' , 'Distance' ], dtype = { 'Distance' : 'float64' , 'DepTime' : 'float64' , 'FlightNum' : 'int64' , 'DepDelay' : 'float64' , 'Dest' : 'object' , }, encoding = \"ISO-8859-1\" ) df_sort = df . groupby ( 'Origin' ) . apply ( lambda x : x . nlargest ( n = 10 , columns = 'Distance' )) df_sort . compute () The above Dask operation took 18.8 seconds using 3 remote 16 CPU 16G RAM Dask nodes. The animation below shows the Dask dashboard in action for the above computation: 6. Automate the job with Shakudo Platform jobs \u00b6 Now the data processing notebook is developed and tested, to automatically run this notebook on a schedule as in most production setups, we can simply add a pipeline.yaml file to build a pipeline. To read more on pipeline YAML files please visit the create a pipeline job page . The yaml file will look like this: pipeline: name: \"data prep pipeline\" tasks: - name: \"Dask groupby data\" type: \"jupyter notebook\" notebook_path: \"pipeline_job/dask_group_sort.ipynb\" notebook_output_path: \"dask_group_sort_output.ipynb\" 7. Productionize the notebook by Launching a job on Shakudo Platform \u00b6 Now we are one step away to put the job in production! To launch a pipeline job or a scheduled job, we can go to the Shakudo Platform Dashboard or use GraphQL queries in the GrafhQL playground. This video shows you how to launch a job in action on the dashboard: Once the job is created, we are officially alive in production! Additional Functionality \u00b6 Shakudo Platform offers a variety of other functionalities for more advanced workflows. Some additional uses include the following: Run pipelines on a schedule Submitting pipeline jobs/trigger pipelines within Jupyter notebooks in jhub Storing pipeline runs for easy debugging","title":"Demo/sandbox walkthrough"},{"location":"sandbox.html#demo-sandbox-walkthrough","text":"Shakudo Platform enables you to build pipelines from VSCode notebooks, python files, or Jupyter notebooks. In this tutorial, we will walk you through the sandbox/demo environment, where you can start up a Jupyter instance, scale up a Dask cluster, and run an existing notebook in a pipeline. You can find the full example here","title":"Demo/ sandbox walkthrough"},{"location":"sandbox.html#1-start-your-jhub-instance","text":"Start a session by clicking on JuperServer or going on the Sessions Page. For more details on the pre-configed environment, please checkout Sessions . You can now start using your Jupyter instance.","title":"1. Start your jhub instance"},{"location":"sandbox.html#2-open-a-notebook","text":"Open a notebook with the Launcher. Choose the Python [conda env:root]* kernel.","title":"2. Open a notebook"},{"location":"sandbox.html#3-process-data-in-pandas","text":"The example we are going to use will use be a group-by task on a public flight dataset in AWS S3 bucket. The dataset has 22 CSV files and is 11GB in total. import pandas as pd df_pd = pd . read_csv ( \"s3://dask-data/airline-data/1990.csv\" , usecols = [ 'DepTime' , 'FlightNum' , 'DepDelay' , 'Origin' , 'Dest' , 'Distance' ]) df_sort_pd = df_pd . groupby ( 'Origin' ) . apply ( lambda x : x . nlargest ( n = 10 , columns = 'Distance' )) The 1990.csv data shape is (5270893, 6) , the processing time of the above operation is 21s on a 16 CPU 16G RAM machine. Now we want to scale up the above computation for 10 CSV files for 1990 to 1999. On one 16 CPU 16G RAM machine it will cause an out of memory error and crash the kernel. To process the files one by one, in total the operation is estimated to take 3.5 minutes .","title":"3. Process data in Pandas"},{"location":"sandbox.html#4-spin-up-a-dask-cluster","text":"Now let's spin up a distributed Dask cluster and speed up the computation! The easiest way to get started is by using the notebook_common function to spin up a pre-configured Dask cluster. You can specify the number of workers with argument num_workers or specify more specs to better fit your computation. Shakudo Platform will automatically choose a cluster configuration for you and provides a Dask dashboard link to monitor progress. from hyperplane import notebook_common as nc client , cluster = nc . initialize_cluster ( nprocs = 5 , nthreads = 3 , ram_gb_per_proc = 2.4 , cores_per_worker = 15 , scheduler_deploy_mode = \"remote\" , num_workers = 3 ) You will be able to see the spinning up logs of the Dask cluster and the link to the Dask dashboard. \ud83d\udc49 Shakudo Platform: selecting worker node pool \ud83d\udc49 Shakudo Platform: selecting scheduler node pool Creating scheduler pod on cluster. This may take some time. \ud83d\udc49 Shakudo Platform: spinning up a dask cluster with a scheduler as a standalone container. \ud83d\udc49 Shakudo Platform: In a few minutes you'll be able to access the dashboard at https://ds.hyperplane.dev/dask-cluster-e002f3d0-b18d-4027-81c5-bed613eb63a4/status \ud83d\udc49 Shakudo Platform: to get logs from all workers, do `cluster.get_logs()`","title":"4. Spin up a Dask cluster"},{"location":"sandbox.html#5-process-data-in-dask","text":"To run the above code on a Dask cluster, we just need to swap the Pandas API to Dask API, which is very similar. Dask does lazy computation, the last line df_sort.compute> triggers the computation. You can find information on the Dask concepts and Dask best practices page. For full Dask documentation, check out https://docs.dask.org/en/ df = dd . read_csv ( data_url , storage_options = { 'anon' : True }, usecols = [ 'DepTime' , 'FlightNum' , 'DepDelay' , 'Origin' , 'Dest' , 'Distance' ], dtype = { 'Distance' : 'float64' , 'DepTime' : 'float64' , 'FlightNum' : 'int64' , 'DepDelay' : 'float64' , 'Dest' : 'object' , }, encoding = \"ISO-8859-1\" ) df_sort = df . groupby ( 'Origin' ) . apply ( lambda x : x . nlargest ( n = 10 , columns = 'Distance' )) df_sort . compute () The above Dask operation took 18.8 seconds using 3 remote 16 CPU 16G RAM Dask nodes. The animation below shows the Dask dashboard in action for the above computation:","title":"5. Process data in Dask"},{"location":"sandbox.html#6-automate-the-job-with-shakudo-platform-jobs","text":"Now the data processing notebook is developed and tested, to automatically run this notebook on a schedule as in most production setups, we can simply add a pipeline.yaml file to build a pipeline. To read more on pipeline YAML files please visit the create a pipeline job page . The yaml file will look like this: pipeline: name: \"data prep pipeline\" tasks: - name: \"Dask groupby data\" type: \"jupyter notebook\" notebook_path: \"pipeline_job/dask_group_sort.ipynb\" notebook_output_path: \"dask_group_sort_output.ipynb\"","title":"6. Automate the job with Shakudo Platform jobs"},{"location":"sandbox.html#7-productionize-the-notebook-by-launching-a-job-on-shakudo-platform","text":"Now we are one step away to put the job in production! To launch a pipeline job or a scheduled job, we can go to the Shakudo Platform Dashboard or use GraphQL queries in the GrafhQL playground. This video shows you how to launch a job in action on the dashboard: Once the job is created, we are officially alive in production!","title":"7. Productionize the notebook by Launching a job on Shakudo Platform"},{"location":"sandbox.html#additional-functionality","text":"Shakudo Platform offers a variety of other functionalities for more advanced workflows. Some additional uses include the following: Run pipelines on a schedule Submitting pipeline jobs/trigger pipelines within Jupyter notebooks in jhub Storing pipeline runs for easy debugging","title":"Additional Functionality"},{"location":"sas_on_hyperplane.html","text":"SAS on Shakudo Platform \u00b6 Shakudo Platform offers various ways to run your SAS code on Shakudo Platform. Set up \u00b6 Start by choosing the SAS image Ensure you have access to a SAS server. Once you are in your SAS image, create an .authinfo file under /root containing the following information: oda user YOUR_USERNAME password YOUR_PASSWORD SAS Kernel \u00b6 The SAS kernel can be used to trigger code to run on your SAS server. You may copy and paste code that runs on your SAS server directly in the notebook, while taking advantage of jupyter notebook cells to split up your code. Jupyter magics \u00b6 User jupyter magics %%SAS to run your copied SAS code directly in the notebook. SASPy \u00b6 SASPy is a python library developed by SAS, designed to help you connect python and SAS. SASPy submit \u00b6 Copy your SAS code and run it as is with saspy submitLST. convert to and from dataframes \u00b6 Read your data into Shakudo Platform, with pandas. Use df2sd to convert from dataframe to SAS data. Read data with SAS, then convert your tables to dataframes for preprocessing or other downstream tasks with sd2df . SASPy also comes with additional functions for visualization and simple preprocessing. SAS pipeline jobs \u00b6 To run a job with SASPy, you must specify your job_type: \"sas\" in your yaml. You will also need to choose the \"SAS\" jobType and include your credentials in the job submission parameters with sas_username and sas_password .","title":"SAS"},{"location":"sas_on_hyperplane.html#sas-on-shakudo-platform","text":"Shakudo Platform offers various ways to run your SAS code on Shakudo Platform.","title":"SAS on Shakudo Platform"},{"location":"sas_on_hyperplane.html#set-up","text":"Start by choosing the SAS image Ensure you have access to a SAS server. Once you are in your SAS image, create an .authinfo file under /root containing the following information: oda user YOUR_USERNAME password YOUR_PASSWORD","title":"Set up"},{"location":"sas_on_hyperplane.html#sas-kernel","text":"The SAS kernel can be used to trigger code to run on your SAS server. You may copy and paste code that runs on your SAS server directly in the notebook, while taking advantage of jupyter notebook cells to split up your code.","title":"SAS Kernel"},{"location":"sas_on_hyperplane.html#jupyter-magics","text":"User jupyter magics %%SAS to run your copied SAS code directly in the notebook.","title":"Jupyter magics"},{"location":"sas_on_hyperplane.html#saspy","text":"SASPy is a python library developed by SAS, designed to help you connect python and SAS.","title":"SASPy"},{"location":"sas_on_hyperplane.html#saspy-submit","text":"Copy your SAS code and run it as is with saspy submitLST.","title":"SASPy submit"},{"location":"sas_on_hyperplane.html#convert-to-and-from-dataframes","text":"Read your data into Shakudo Platform, with pandas. Use df2sd to convert from dataframe to SAS data. Read data with SAS, then convert your tables to dataframes for preprocessing or other downstream tasks with sd2df . SASPy also comes with additional functions for visualization and simple preprocessing.","title":"convert to and from dataframes"},{"location":"sas_on_hyperplane.html#sas-pipeline-jobs","text":"To run a job with SASPy, you must specify your job_type: \"sas\" in your yaml. You will also need to choose the \"SAS\" jobType and include your credentials in the job submission parameters with sas_username and sas_password .","title":"SAS pipeline jobs"},{"location":"sessions.html","text":"Sessions \u00b6 Sessions has a variety of fully managed Jupiter environments for developing VSCode notebooks, python files, and Jupyter notebooks. To spin up a Sessions instance, go to the Sessions tab on the dashboard and start a new session. Below is a list of pre-build images for a quick start. Display Name Image Name Best For Key Packages Complete Packages List Basic basic basic data science numpy scipy scikit-learn basic Deep learning dl deep learning TensorFlow Pytorch MXNet transformers dl GPU gpu machine learning with GPU Rapids TensorFlow Pytorch gpu Spark on Ray rayspark data engineering with spark on ray Ray raydp Spark Spark on Ray Ray ray distributed training, hyperparameter tuning Ray Ray Triton Model Serving triton model serving with triton tritonclient Triton Crypto crypto crypto trading and DApps ccxt web3 Crypto Climate climate large scale climate data processing xclim Pangeo xarray Climate SAS on Jupyter sas run sas code in jupyter and integrate with python saspy SAS R on Jupyter rstat run R code in jupyter and integrate with python rstat R","title":"Start a Jupyter instance"},{"location":"sessions.html#sessions","text":"Sessions has a variety of fully managed Jupiter environments for developing VSCode notebooks, python files, and Jupyter notebooks. To spin up a Sessions instance, go to the Sessions tab on the dashboard and start a new session. Below is a list of pre-build images for a quick start. Display Name Image Name Best For Key Packages Complete Packages List Basic basic basic data science numpy scipy scikit-learn basic Deep learning dl deep learning TensorFlow Pytorch MXNet transformers dl GPU gpu machine learning with GPU Rapids TensorFlow Pytorch gpu Spark on Ray rayspark data engineering with spark on ray Ray raydp Spark Spark on Ray Ray ray distributed training, hyperparameter tuning Ray Ray Triton Model Serving triton model serving with triton tritonclient Triton Crypto crypto crypto trading and DApps ccxt web3 Crypto Climate climate large scale climate data processing xclim Pangeo xarray Climate SAS on Jupyter sas run sas code in jupyter and integrate with python saspy SAS R on Jupyter rstat run R code in jupyter and integrate with python rstat R","title":"Sessions"},{"location":"spark_hyperplane.html","text":"Working with Spark \u00b6 Shakudo Platform provides simple APIs to use Spark on distributed Ray clusters using RayDP. RayDP combines your Spark and Ray clusters, making it easy to do large scale data processing using the PySpark API and seemlessly use that data to train your models using TensorFlow and PyTorch. Initializing a distributed Ray cluster for Spark \u00b6 Initialize a distributed Ray cluster as usual using the following: from hyperplane.ray_common import initialize_ray_cluster ray_cluster = initialize_ray_cluster ( num_workers = 4 , cpu_core_per_worker = 15 , ram_gb_per_worker = 12 ) num_workers (int) is the number of Ray nodes to be initialized cpu_core_per_worker (int) is the number of CPU cores in each Ray node ram_gb_per_worker (float) is the memory size in GB for each Ray node Read more about Ray and Ray on Shakudo Platform . Start a Spark session \u00b6 spark = raydp . init_spark ( 'example' , num_executors = 2 , executor_cores = 4 , executor_memory = '4G' ) Use PySpark \u00b6 Once the Spark session is initialized, you can use pyspark as ususal from here on. The latest RayDP supports PySpark 3.2.0+, which provides simple Pandas-like APIs. import pyspark.pandas as pd df = pd . read_csv ( \"data.csv\" ) Shutdown a Ray cluster \u00b6 After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Jupyeter session or job is finished. You can add this line to the end of your code to shutdown the Ray nodes. stop_ray_cluster ( ray_cluster ) More examples: - Spark on Ray example uses Spark on Ray to process data on Shakudo Platform. - Spark3.2.0 example uses Pandas-like APIs to process data.","title":"Spark on Shakudo Platform"},{"location":"spark_hyperplane.html#working-with-spark","text":"Shakudo Platform provides simple APIs to use Spark on distributed Ray clusters using RayDP. RayDP combines your Spark and Ray clusters, making it easy to do large scale data processing using the PySpark API and seemlessly use that data to train your models using TensorFlow and PyTorch.","title":"Working with Spark"},{"location":"spark_hyperplane.html#initializing-a-distributed-ray-cluster-for-spark","text":"Initialize a distributed Ray cluster as usual using the following: from hyperplane.ray_common import initialize_ray_cluster ray_cluster = initialize_ray_cluster ( num_workers = 4 , cpu_core_per_worker = 15 , ram_gb_per_worker = 12 ) num_workers (int) is the number of Ray nodes to be initialized cpu_core_per_worker (int) is the number of CPU cores in each Ray node ram_gb_per_worker (float) is the memory size in GB for each Ray node Read more about Ray and Ray on Shakudo Platform .","title":"Initializing a distributed Ray cluster for Spark"},{"location":"spark_hyperplane.html#start-a-spark-session","text":"spark = raydp . init_spark ( 'example' , num_executors = 2 , executor_cores = 4 , executor_memory = '4G' )","title":"Start a Spark session"},{"location":"spark_hyperplane.html#use-pyspark","text":"Once the Spark session is initialized, you can use pyspark as ususal from here on. The latest RayDP supports PySpark 3.2.0+, which provides simple Pandas-like APIs. import pyspark.pandas as pd df = pd . read_csv ( \"data.csv\" )","title":"Use PySpark"},{"location":"spark_hyperplane.html#shutdown-a-ray-cluster","text":"After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Jupyeter session or job is finished. You can add this line to the end of your code to shutdown the Ray nodes. stop_ray_cluster ( ray_cluster ) More examples: - Spark on Ray example uses Spark on Ray to process data on Shakudo Platform. - Spark3.2.0 example uses Pandas-like APIs to process data.","title":"Shutdown a Ray cluster"},{"location":"ssh_connection.html","text":"Users can now connect to Sessions with VSCode Remote. For steps on connecting, please see below. Connecting \u00b6 First time connecting \u00b6 Install VSCode Remote - ssh extension to your VSCode Register your keys: Generate a ssh key pair for your local machine ( ssh-keygen or skip if you already have ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub files) Copy your public key ( ~/.ssh/id_rsa.pub ) into your Sessions\u2019 ~/.ssh/authorized_keys file i. in your local: cat ~/.ssh/id_rsa.pub ii. in your Session: vim ~/.ssh/authorized_keys and copy the output from i. (ssh-rsa) into this file (press \"Esc\", the type \":wq\" to write and quit vim editor) Continue to Normal Connection step 2 Copy the ssh host to your root folder in the Session: cp /etc/ssh/ssh_host_* ~/.ssh/ Normal connection \u00b6 Start a Shakudo Platform Session if you haven't already Note your \"ssh connection command\" and \"Notebook URI\" from the Sessions table Connect For SSH from terminal: - Copy the ssh command from your Sessions dashboard by clicking on \"<>\" for your the specific Session you created - Paste the ssh command into a terminal on your local machine to connect to Sessions via ssh - Check that you are in the conda \"base\" environment with conda activate base For VSCode Remote - Add an SSH target in your VSCode Remote Explorer. Paste the printed ssh command from above. Check that there are no other entries with the same IP as the above in your ssh config To connect to the Jupyter server, click \"Jupyter Server: Remote\" in the bottom bar on your VSCode, choose \"Existing\" URI, copy and paste the \"Notebook URI\" for your session Troubleshooting \u00b6 Ensure your local key file has 700 permissions ( chmod 700 ~/.ssh/id_rsa ). Also ensure your Sessions ~/.ssh folder has 700 permissions with chmod 700 ~/.ssh chmod 700 ~/.ssh/* Ensure your ~/.ssh/config does not have another entry with the same IP or Hostname as the command from above If you are still having issues, consider deleting your ~/.ssh/authorized_keys file from your Sessions and restart from \"First time connecting\".","title":"Connect to VSCode Remote"},{"location":"ssh_connection.html#connecting","text":"","title":"Connecting"},{"location":"ssh_connection.html#first-time-connecting","text":"Install VSCode Remote - ssh extension to your VSCode Register your keys: Generate a ssh key pair for your local machine ( ssh-keygen or skip if you already have ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub files) Copy your public key ( ~/.ssh/id_rsa.pub ) into your Sessions\u2019 ~/.ssh/authorized_keys file i. in your local: cat ~/.ssh/id_rsa.pub ii. in your Session: vim ~/.ssh/authorized_keys and copy the output from i. (ssh-rsa) into this file (press \"Esc\", the type \":wq\" to write and quit vim editor) Continue to Normal Connection step 2 Copy the ssh host to your root folder in the Session: cp /etc/ssh/ssh_host_* ~/.ssh/","title":"First time connecting"},{"location":"ssh_connection.html#normal-connection","text":"Start a Shakudo Platform Session if you haven't already Note your \"ssh connection command\" and \"Notebook URI\" from the Sessions table Connect For SSH from terminal: - Copy the ssh command from your Sessions dashboard by clicking on \"<>\" for your the specific Session you created - Paste the ssh command into a terminal on your local machine to connect to Sessions via ssh - Check that you are in the conda \"base\" environment with conda activate base For VSCode Remote - Add an SSH target in your VSCode Remote Explorer. Paste the printed ssh command from above. Check that there are no other entries with the same IP as the above in your ssh config To connect to the Jupyter server, click \"Jupyter Server: Remote\" in the bottom bar on your VSCode, choose \"Existing\" URI, copy and paste the \"Notebook URI\" for your session","title":"Normal connection"},{"location":"ssh_connection.html#troubleshooting","text":"Ensure your local key file has 700 permissions ( chmod 700 ~/.ssh/id_rsa ). Also ensure your Sessions ~/.ssh folder has 700 permissions with chmod 700 ~/.ssh chmod 700 ~/.ssh/* Ensure your ~/.ssh/config does not have another entry with the same IP or Hostname as the command from above If you are still having issues, consider deleting your ~/.ssh/authorized_keys file from your Sessions and restart from \"First time connecting\".","title":"Troubleshooting"},{"location":"triton_serving.html","text":"What is Triton Inference Server \u00b6 Shakudo Platform comes with a built-in NVIDIA Triton Inference Server that simplifies the deployment of AI models at scale in production. Triton is an open-source inference serving software, it lets teams deploy trained AI models from any framework (TensorFlow, NVIDIA\u00ae TensorRT\u00ae, PyTorch, ONNX Runtime, or custom) from local storage or cloud platform on any GPU- or CPU-based infrastructure (cloud, data center, or edge). Serve a model \u00b6 To serve your model with the Triton server Upload your model checkpoint to the triton server model repository and write a client file. The default path of the Triton model repository is {your_cloud_bucket}/triton-server/model-repository/ . Follow the structure of the Triton model repostiory to add models. For certain types of model, such as Tensorflow, you can skip the config.pbtxt, Triton can create the file for you based on the model checkpoint. Triton will automatically detect the new model and load the model onto the server. If the model is loaded successfully, you'll see the new model on the left side of the Triton Tab . Write a client that contains an inference function using the model. The official Triton client examples have a collection of examples of on client files for popular machine learning tasks such as image recognition and NLP. Wrap your client function and dependencies with FastAPI or Flask App. Example code in this repo for a simple model . Start a service for your client App on the right side of your Triton Tab . Fill in the following fields: Triton client name : any unique name you want the client to be called, for example a_simple_model Custom URL endpoint : the URL your model endpoint will be served on, for example a_simple_model_endpoint Shakudo Platform YAML: the path of your yaml file that contains the bash file which starts the server. In the simple model example the path is hyperplane-triton-api/hyperplane_triton_api.yaml After the service is up and running, your endpoint is called infer in your App.py, your model endpoint can be reached at https://{your_domain}/hyperplane.dev/a_simple_model_endpoint/infer/ Checkout the full example code . Serve multiple models with one endpoint \u00b6 With NVIDIA Triton on Shakudo Platform, serving multiple models with one endpoint is as easy as serving one model with one endpoint. Add models to your Triton model repository, models will be loaded by Triton server automatically. If the models are loaded successfully, the newly added model will show up in the available model list on the left side of your Triton tab . Add a model client as usual, but parameterize the inference function with model_name. The client code will handle the logic of which model to use among the multiple models. Simple example code: def run_inference(request_body: str, model_name: str) -> str: ## input output code as usual response = client.infer(model_name, inputs, outputs=outputs) ## other post infer code Serve ensemble models \u00b6 With the Triton server, ensemble models can be served through building a pipeline of models. The official Triton documentation provides more details on ensemble models . On Shakudo Platform ensemble models can be served following these steps: Step 1. Add models to your Triton model repository, models will be loaded by Triton server automatically. If the models are loaded successfully, the newly added model will show up in the available model list on the left side of your Triton tab . Step 2. Create a new model in the Triton model repository on the bucket {your_cloud_bucket}/triton-server/model-repository/ and add an config.pbtxt file for the ensemble model. Make sure the platform is set to ensemble . Here is an example of ensemble config.pbtxt file: name: \"ensemble_model\" platform: \"ensemble\" input [ { name: \"IMAGE\" data_type: TYPE_UINT8 dims: [256, 256, 3] } ] output [ { name: \"MODEL1_OUTPUT\" data_type: TYPE_INT64 dims: [ 2 ] }, { name: \"MODEL2_OUTPUT\" data_type: TYPE_INT64 dims: [ 2 ] } ] ensemble_scheduling { step [ { model_name: \"model1\" model_version: -1 input_map { key: \"INPUT__0\" value: \"IMAGE\" } output_map { key: \"OUTPUT__1\" value: \"MODEL1_OUTPUT\" } }, { model_name: \"model2\" model_version: -1 input_map { key: \"INPUT__0\" value: \"IMAGE\" } output_map { key: \"OUTPUT__1\" value: \"MODEL2_OUTPUT\" } } ] } Step 3. Add a model.py in the model repository that uses python backend. It should take INPUT__0 as input and OUTPUT__1 and OUTPUT__2 as model outputs. Use asyncio to gather inference results from both models concurrently, such as the following: in_0 = pb_utils.get_input_tensor_by_name(request, \"INPUT__0\") inference_response_awaits = [] for model_name in ['model1', 'model2']: infer_request = pb_utils.InferenceRequest( model_name=model_name, requested_output_names=[\"OUTPUT__1\"], inputs=[in_0]) inference_response_awaits.append(infer_request.async_exec()) inference_responses = await asyncio.gather(*inference_response_awaits) To read more about the Triton python backend . Examples \u00b6 Examples code that serves an image recognition model: a simple Triton service on Shakudo Platform in the Shakudo Platform example repository","title":"Triton"},{"location":"triton_serving.html#what-is-triton-inference-server","text":"Shakudo Platform comes with a built-in NVIDIA Triton Inference Server that simplifies the deployment of AI models at scale in production. Triton is an open-source inference serving software, it lets teams deploy trained AI models from any framework (TensorFlow, NVIDIA\u00ae TensorRT\u00ae, PyTorch, ONNX Runtime, or custom) from local storage or cloud platform on any GPU- or CPU-based infrastructure (cloud, data center, or edge).","title":"What is Triton Inference Server"},{"location":"triton_serving.html#serve-a-model","text":"To serve your model with the Triton server Upload your model checkpoint to the triton server model repository and write a client file. The default path of the Triton model repository is {your_cloud_bucket}/triton-server/model-repository/ . Follow the structure of the Triton model repostiory to add models. For certain types of model, such as Tensorflow, you can skip the config.pbtxt, Triton can create the file for you based on the model checkpoint. Triton will automatically detect the new model and load the model onto the server. If the model is loaded successfully, you'll see the new model on the left side of the Triton Tab . Write a client that contains an inference function using the model. The official Triton client examples have a collection of examples of on client files for popular machine learning tasks such as image recognition and NLP. Wrap your client function and dependencies with FastAPI or Flask App. Example code in this repo for a simple model . Start a service for your client App on the right side of your Triton Tab . Fill in the following fields: Triton client name : any unique name you want the client to be called, for example a_simple_model Custom URL endpoint : the URL your model endpoint will be served on, for example a_simple_model_endpoint Shakudo Platform YAML: the path of your yaml file that contains the bash file which starts the server. In the simple model example the path is hyperplane-triton-api/hyperplane_triton_api.yaml After the service is up and running, your endpoint is called infer in your App.py, your model endpoint can be reached at https://{your_domain}/hyperplane.dev/a_simple_model_endpoint/infer/ Checkout the full example code .","title":"Serve a model"},{"location":"triton_serving.html#serve-multiple-models-with-one-endpoint","text":"With NVIDIA Triton on Shakudo Platform, serving multiple models with one endpoint is as easy as serving one model with one endpoint. Add models to your Triton model repository, models will be loaded by Triton server automatically. If the models are loaded successfully, the newly added model will show up in the available model list on the left side of your Triton tab . Add a model client as usual, but parameterize the inference function with model_name. The client code will handle the logic of which model to use among the multiple models. Simple example code: def run_inference(request_body: str, model_name: str) -> str: ## input output code as usual response = client.infer(model_name, inputs, outputs=outputs) ## other post infer code","title":"Serve multiple models with one endpoint"},{"location":"triton_serving.html#serve-ensemble-models","text":"With the Triton server, ensemble models can be served through building a pipeline of models. The official Triton documentation provides more details on ensemble models . On Shakudo Platform ensemble models can be served following these steps: Step 1. Add models to your Triton model repository, models will be loaded by Triton server automatically. If the models are loaded successfully, the newly added model will show up in the available model list on the left side of your Triton tab . Step 2. Create a new model in the Triton model repository on the bucket {your_cloud_bucket}/triton-server/model-repository/ and add an config.pbtxt file for the ensemble model. Make sure the platform is set to ensemble . Here is an example of ensemble config.pbtxt file: name: \"ensemble_model\" platform: \"ensemble\" input [ { name: \"IMAGE\" data_type: TYPE_UINT8 dims: [256, 256, 3] } ] output [ { name: \"MODEL1_OUTPUT\" data_type: TYPE_INT64 dims: [ 2 ] }, { name: \"MODEL2_OUTPUT\" data_type: TYPE_INT64 dims: [ 2 ] } ] ensemble_scheduling { step [ { model_name: \"model1\" model_version: -1 input_map { key: \"INPUT__0\" value: \"IMAGE\" } output_map { key: \"OUTPUT__1\" value: \"MODEL1_OUTPUT\" } }, { model_name: \"model2\" model_version: -1 input_map { key: \"INPUT__0\" value: \"IMAGE\" } output_map { key: \"OUTPUT__1\" value: \"MODEL2_OUTPUT\" } } ] } Step 3. Add a model.py in the model repository that uses python backend. It should take INPUT__0 as input and OUTPUT__1 and OUTPUT__2 as model outputs. Use asyncio to gather inference results from both models concurrently, such as the following: in_0 = pb_utils.get_input_tensor_by_name(request, \"INPUT__0\") inference_response_awaits = [] for model_name in ['model1', 'model2']: infer_request = pb_utils.InferenceRequest( model_name=model_name, requested_output_names=[\"OUTPUT__1\"], inputs=[in_0]) inference_response_awaits.append(infer_request.async_exec()) inference_responses = await asyncio.gather(*inference_response_awaits) To read more about the Triton python backend .","title":"Serve ensemble models"},{"location":"triton_serving.html#examples","text":"Examples code that serves an image recognition model: a simple Triton service on Shakudo Platform in the Shakudo Platform example repository","title":"Examples"},{"location":"tutorials.html","text":"Tutorials \u00b6 You can find here a list of short vidoe tutorials provided by Shakudo Platform. Sign into Shakudo Platform Scale up computation with Dask Deploy a jupyter notebook based pipeline Run a pipeline through the Shakudo Platform dashboard Reproduce and resolve issue in production Run a pipeline job on a pre-defined schedule Sign into Shakudo Platform \u00b6 Scale up computation with Dask \u00b6 Deploy a jupyter notebook based pipeline \u00b6 Run a pipeline through the Shakudo Platform dashboard \u00b6 Reproduce and resolve issue in production \u00b6 Run a pipeline job on a pre-defined schedule \u00b6","title":"Tutorials"},{"location":"tutorials.html#tutorials","text":"You can find here a list of short vidoe tutorials provided by Shakudo Platform. Sign into Shakudo Platform Scale up computation with Dask Deploy a jupyter notebook based pipeline Run a pipeline through the Shakudo Platform dashboard Reproduce and resolve issue in production Run a pipeline job on a pre-defined schedule","title":"Tutorials"},{"location":"tutorials.html#sign-into-shakudo-platform","text":"","title":"Sign into Shakudo Platform"},{"location":"tutorials.html#scale-up-computation-with-dask","text":"","title":"Scale up computation with Dask"},{"location":"tutorials.html#deploy-a-jupyter-notebook-based-pipeline","text":"","title":"Deploy a jupyter notebook based pipeline"},{"location":"tutorials.html#run-a-pipeline-through-the-shakudo-platform-dashboard","text":"","title":"Run a pipeline through the Shakudo Platform dashboard"},{"location":"tutorials.html#reproduce-and-resolve-issue-in-production","text":"","title":"Reproduce and resolve issue in production"},{"location":"tutorials.html#run-a-pipeline-job-on-a-pre-defined-schedule","text":"","title":"Run a pipeline job on a pre-defined schedule"}]}